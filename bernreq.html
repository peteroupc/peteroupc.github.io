<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Open Questions on the Bernoulli Factory Problem</title><meta name="citation_title" content="Open Questions on the Bernoulli Factory Problem"><meta name="citation_pdf_url" content="https://peteroupc.github.io/bernreq.pdf"><meta name="citation_url" content="https://peteroupc.github.io/bernreq.html"><meta name="citation_date" content="2022/06/14"><meta name="citation_online_date" content="2022/06/14"><meta name="og:title" content="Open Questions on the Bernoulli Factory Problem"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/bernreq.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="Open Questions on the Bernoulli Factory Problem"><meta name="author" content="Peter Occil"/><meta name="citation_author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css">
            <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, preferredFont: "TeX" },
                    tex2jax: { displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], processEscapes: true } });
            </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>Open Questions on the Bernoulli Factory Problem</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Background></a></p>

<h2>Background</h2>

<p>We&#39;re given a coin that shows heads with an unknown probability, $\lambda$. The goal is to use that coin (and possibly also a fair coin) to build a &quot;new&quot; coin that shows heads with a probability that depends on $\lambda$, call it $f(\lambda)$. This is the <em>Bernoulli factory problem</em>, and it can be solved only for certain functions $f$. (For example, flipping the coin twice and taking heads only if exactly one coin shows heads, we can simulate the probability $2\lambda(1-\lambda)$.)</p>

<p>Specifically, the only functions that can be simulated this way <strong>are continuous and polynomially bounded on their domain, and map $[0, 1]$ or a subset thereof to $[0, 1]$</strong>, as well as $f=0$ and $f=1$. These functions are called <em>factory functions</em> in this page. (A function $f(x)$ is <em>polynomially bounded</em> if both $f$ and $1-f$ are bounded below by min($x^n$, $(1-x)^n$) for some integer $n$ (Keane and O&#39;Brien 1994). This implies that $f$ admits no roots on (0, 1) and can&#39;t take on the value 0 or 1 except possibly at 0 and/or 1.)</p>

<p>This page contains several questions about the <a href="https://peteroupc.github.io/bernoulli.html"><strong>Bernoulli factory</strong></a> problem.  Answers to them will greatly improve my pages on this site about Bernoulli factories.  If you can answer any of them, post an issue in the <a href="https://github.com/peteroupc/peteroupc.github.io/issues"><strong>GitHub issues page</strong></a>.</p>

<p><a id=Contents></a></p>

<h2>Contents</h2>

<ul>
<li><a href="#Background"><strong>Background</strong></a></li>
<li><a href="#Contents"><strong>Contents</strong></a></li>
<li><a href="#Polynomials_that_approach_a_factory_function_fast"><strong>Polynomials that approach a factory function &quot;fast&quot;</strong></a>

<ul>
<li><a href="#Formal_Statement"><strong>Formal Statement</strong></a></li>
<li><a href="#A_Matter_of_Efficiency"><strong>A Matter of Efficiency</strong></a></li>
<li><a href="#Questions"><strong>Questions</strong></a></li>
</ul></li>
<li><a href="#New_coins_from_old_smoothly"><strong>New coins from old, smoothly</strong></a>

<ul>
<li><a href="#Questions_2"><strong>Questions</strong></a></li>
</ul></li>
<li><a href="#Reverse_time_martingales"><strong>Reverse-time martingales</strong></a></li>
<li><a href="#Tossing_Heads_According_to_a_Concave_Function"><strong>Tossing Heads According to a Concave Function</strong></a>

<ul>
<li><a href="#Using_Two_Polynomial_Sequences"><strong>Using Two Polynomial Sequences</strong></a></li>
<li><a href="#Using_a_Series_Expansion"><strong>Using a Series Expansion</strong></a></li>
<li><a href="#Questions_3"><strong>Questions</strong></a></li>
</ul></li>
<li><a href="#Simulable_and_strongly_simulable_functions"><strong>Simulable and strongly simulable functions</strong></a></li>
<li><a href="#Multiple_Output_Bernoulli_Factories"><strong>Multiple-Output Bernoulli Factories</strong></a>

<ul>
<li><a href="#Questions_4"><strong>Questions</strong></a></li>
<li><a href="#Functions_with_Optimal_Factories"><strong>Functions with Optimal Factories</strong></a></li>
</ul></li>
<li><a href="#From_coin_flips_to_algebraic_functions_via_pushdown_automata"><strong>From coin flips to algebraic functions via pushdown automata</strong></a>

<ul>
<li><a href="#Pushdown_Automata"><strong>Pushdown Automata</strong></a></li>
<li><a href="#Algebraic_Functions"><strong>Algebraic Functions</strong></a></li>
<li><a href="#Questions_5"><strong>Questions</strong></a></li>
</ul></li>
<li><a href="#Other_Questions"><strong>Other Questions</strong></a></li>
<li><a href="#Remarks"><strong>Remarks</strong></a></li>
<li><a href="#My_Attempt"><strong>My Attempt</strong></a></li>
<li><a href="#References"><strong>References</strong></a></li>
</ul>

<p><a id=Polynomials_that_approach_a_factory_function_fast></a></p>

<h2>Polynomials that approach a factory function &quot;fast&quot;</h2>

<p><a href="https://math.stackexchange.com/questions/3904732/what-are-ways-to-compute-polynomials-that-converge-from-above-and-below-to-a-con"><strong>https://math.stackexchange.com/questions/3904732/what-are-ways-to-compute-polynomials-that-converge-from-above-and-below-to-a-con</strong></a></p>

<p><a href="https://mathoverflow.net/questions/424272/explicit-and-fast-error-bounds-for-polynomial-approximation"><strong>https://mathoverflow.net/questions/424272/explicit-and-fast-error-bounds-for-polynomial-approximation</strong></a></p>

<p>A polynomial $f(x)$ is written in <em>Bernstein form of degree $n$</em> if it is written as&mdash; $$f(x)=\sum_{k=0}^n a_k {n \choose k} x^k (1-x)^{n-k},$$ where $a_0, ..., a_k$ are the polynomial&#39;s <em>Bernstein coefficients</em>.</p>

<p>An <a href="https://peteroupc.github.io/bernoulli.html#General_Factory_Functions"><strong>algorithm</strong></a> simulates a factory function via two sequences of polynomials that converge from above and below to that function. Roughly speaking, the algorithm works as follows:</p>

<ol>
<li>Generate U, a uniform random variate in $[0, 1]$.</li>
<li>Flip the input coin (with a probability of heads of $\lambda$), then build an upper and lower bound for $f(\lambda)$, based on the outcomes of the flips so far. In this case, these bounds come from two degree-$n$ polynomials that approach $f$ as $n$ gets large, where $n$ is the number of coin flips so far in the algorithm.</li>
<li>If U is less than or equal to the lower bound, return 1. If U is greater than the upper bound, return 0. Otherwise, go to step 2.</li>
</ol>

<p>The result of the algorithm is 1 with probability <em>exactly</em> equal to $f(\lambda)$, or 0 otherwise.</p>

<p>However, the algorithm requires the polynomial sequences to meet certain requirements; among them, the sequences must be of Bernstein-form polynomials that converge from above and below to a factory function.  See the formal statement, next.</p>

<p><a id=Formal_Statement></a></p>

<h3>Formal Statement</h3>

<p>More formally, for the approximation schemes I am looking for, there exist two sequences of polynomials, namely—</p>

<ul>
<li>$g_{n}(\lambda): =\sum_{k=0}^{n}a(n, k){n \choose k}\lambda^{k}(1-\lambda)^{n-k}$, and</li>
<li>$h_{n}(\lambda): =\sum_{k=0}^{n}b(n, k){n \choose k}\lambda^{k}(1-\lambda)^{n-k}$,</li>
</ul>

<p>for every integer $n\ge1$, such that—</p>

<ol>
<li>$0\le a(n, k)\le b(n, k)\le1$,</li>
<li>$\lim_{n}g_{n}(\lambda)=\lim_{n}h_{n}(\lambda)=f(\lambda)$ for every $\lambda\in[0,1]$, and</li>
<li>For every $m&lt;n$, both $(g_{n} - g_{m})$ and $(h_{m} - h_{n})$ have non-negative coefficients once $g_{n}$, $g_{m}$, $h_{n}$, and $h_{m}$ are rewritten as degree-$n$ polynomials in Bernstein form,</li>
</ol>

<p>where $f(\lambda)$ is continuous on $[0, 1]$ (Nacu and Peres 2005; Holtz et al. 2011), and the goal is to find the appropriate values for $a(n, k)$ and $b(n, k)$.</p>

<p>It is allowed for $a(n, k)\lt 0$ for a given $n$ and some $k$, in which case all $a(n, k)$ for that $n$ are taken to be 0 instead. It is allowed for $b(n, k)\gt 1$ for a given $n$ and some $k$, in which case all $b(n, k)$ for that $n$ are taken to be 1 instead.</p>

<p>Alternatively, find a way to rewrite $f(\lambda)$ as&mdash; $$f(\lambda) = \sum_{n\ge 1} P_n(\lambda) = 1 - \sum_{n\ge 1} Q_n(\lambda),$$ where $P_n$ and $Q_n$ are polynomials of degree $n$ with non-negative Bernstein coefficients.</p>

<p><a id=A_Matter_of_Efficiency></a></p>

<h3>A Matter of Efficiency</h3>

<p>However, ordinary Bernstein polynomials can&#39;t in general converge to a function faster than $O(1/n)$, a result known since Voronovskaya (1932) and a rate that will lead to an <strong>infinite expected number of coin flips in general</strong>.  (See also the answer below and my <a href="https://peteroupc.github.io/bernsupp.html"><strong>supplemental notes</strong></a>.)</p>

<p>But Lorentz (1966) showed that if the function is positive and $C^k$ continuous, there are polynomials with non-negative Bernstein coefficients that converge at the rate $O(1/n^{k/2})$ (and thus can enable a <strong>finite expected number of coin flips</strong> if the function is &quot;smooth&quot; enough).</p>

<p>Thus, people have developed alternatives, including iterated Bernstein polynomials, to improve the convergence rate. These include Micchelli (1973), Guan (2009), Güntürk and Li (2021), the &quot;Lorentz operator&quot; in Holtz et al. (2011) (see also &quot;<a href="#New_coins_from_old_smoothly"><strong>New coins from old, smoothly</strong></a>&quot;), and Draganov (2014).</p>

<p>These alternative polynomials usually include results where the error bound is the desired $O(1/n^{k/2})$, but all those results (e.g., Theorem 4.4 in Micchelli; Theorem 5 in Güntürk and Li) have hidden constants with no upper bounds given, making them unimplementable (that is, it can&#39;t be known beforehand whether a given polynomial will come close to the target function within a user-specified error tolerance).</p>

<p><a id=Questions></a></p>

<h3>Questions</h3>

<p>Thus the questions are:</p>

<ol>
<li><p>Are there practical formulas to compute polynomials that&mdash;</p>

<ul>
<li>meet the formal statement above, and</li>
<li>meet the following error bound? $|f(x) - P_n(f)(x)| \le \epsilon(f,n,x) = O(1/n^{k/2})$ where $P_n(f)(x)$ is an approximating polynomial of degree $n$, $k$ is the number of continuous derivatives, and $\epsilon(f,n,x)$ is a fully determined formula with all constants in the formula having a <strong>known exact value or upper bound</strong>.</li>
</ul></li>
<li><p>Are there other practical formulas to approximate specific factory functions with polynomials that meet the formal statement above?</p></li>
<li><p>Are there practical formulas to compute polynomials that meet the error bound given in question 1 and can be readily rewritten to Bernstein form with non-negative Bernstein coefficients?</p></li>
</ol>

<p><a id=New_coins_from_old_smoothly></a></p>

<h2>New coins from old, smoothly</h2>

<p><a href="https://mathoverflow.net/questions/407179/using-the-holtz-method-to-build-polynomials-that-converge-to-a-continuous-functi"><strong>https://mathoverflow.net/questions/407179/using-the-holtz-method-to-build-polynomials-that-converge-to-a-continuous-functi</strong></a></p>

<p>Now, we focus on a <strong>specific approximation scheme</strong>, the one <strong>presented by <a href="https://link.springer.com/content/pdf/10.1007/s00365-010-9108-5.pdf"><strong>Holtz et al. 2011</strong></a>, in the paper &quot;New coins from old, smoothly&quot;</strong>.</p>

<p>The scheme involves building polynomials that are shifted upward and downward to approximate $f$ from above and below so that the polynomials meet the <a href="#Formal_Statement"><strong>Formal Statement</strong></a> given earlier.</p>

<p>The scheme achieves a convergence rate that generally depends on the smoothness of $f$; in fact, it can achieve the highest convergence rate possible for functions with that smoothness.</p>

<p>Specifically, Holtz et al. proved the following results:</p>

<ol>
<li><p>A function $f(\lambda):[0,1]\to(0,1)$ can be approximated, in a manner that solves the Bernoulli factory problem, at the rate $O((\Delta_n(\lambda))^\beta)$ if and only if $f$ is $\lfloor\beta\rfloor$ times differentiable and has a ($\beta-\lfloor\beta\rfloor$)-Hölder continuous $\lfloor\beta\rfloor$-th derivative, where $\beta&gt;0$ is a non-integer and $\Delta_n(\lambda) = \max((\lambda(1-\lambda)/n)^{1/2}, 1/n)$.  (Roughly speaking, the rate is $O((1/n)^{\beta})$ when $\lambda$ is close to 0 or 1, and $O((1/n)^{\beta/2})$ elsewhere.)</p></li>
<li><p>A function $f(\lambda):[0,1]\to(0,1)$ can be approximated, in a manner that solves the Bernoulli factory problem, at the rate $O((\Delta_n(\lambda))^{r+1})$ only if the $r$th derivative of $f$ is in the Zygmund class, where $r\ge 0$ is an integer.</p></li>
</ol>

<p>The scheme is as follows:</p>

<p>Let $f$ be a function&mdash;</p>

<ul>
<li>that maps [0, 1] to the open interval (0, 1), and</li>
<li>whose $r$th derivative is $\beta$-Hölder continuous, where $\beta$ is in (0, 1).</li>
</ul>

<p>Let $\alpha = r+\beta$, let $b = 2^s$, and let $s\ge0$ be an integer. Let $Q_{n, r}f$ be a degree $n+r$ approximating polynomial called a <em>Lorentz operator</em> (see the paper for details on the Lorentz operator). Let $n_0$ be the smallest $n$ such that $Q_{n_0, r}f$ has coefficients within [0, 1]. Define the following for every integer $n \ge n_0$ divisible by $n_{0}b$:</p>

<ul>
<li>$f_{n_0} = Q_{n_0, r}f$.</li>
<li><p>$f_{n} = f_{n/b} + Q_{n, r}(f-f_{n/b})$ for each integer $n &gt; n_0$.</p></li>
<li><p>$\phi(n, \alpha, \lambda) = \frac{\theta_{\alpha}}{n^{\alpha}}+(\frac{\lambda(1-\lambda)}{n})^{\alpha/2}$.</p></li>
</ul>

<p>Let $B_{n, k, F}$ be the $k$th coefficient of the degree-$n$ Bernstein polynomial of $F$.</p>

<p>Let $C(\lambda)$ be a polynomial as follows: Find the degree-$n$ Bernstein polynomial of $\phi(n, r+\beta, \lambda)$, then elevate it to a degree-$n+r$ Bernstein polynomial.</p>

<p>Then the coefficients for the degree $n+r$ polynomial that approximates $f$ are&mdash;</p>

<ul>
<li>$g(n, r, k) = B_{n+r,k,f_{n}} - D * B_{n+r,k,C}$, and</li>
<li>$h(n, r, k) = B_{n+r,k,f_{n}} + D * B_{n+r,k,C}$.</li>
</ul>

<p>However, the Holtz method is not yet implementable, for the following reasons among others:</p>

<ul>
<li>The paper doesn&#39;t give values or upper bounds for important constants, notably the three constants $s$, $\theta_{\alpha}$, and $D$. For example, the paper says only that $D$ should be chosen &quot;large enough&quot;.</li>
<li>The method&#39;s results are only asymptotic.</li>
<li>The paper has no examples of how the scheme works for a selection of functions $f$.</li>
</ul>

<p>And I seek ways to make this solution implementable.</p>

<p><a id=Questions_2></a></p>

<h3>Questions</h3>

<ol>
<li>What are practical upper bounds for $s$, $\theta_{\alpha}$, and $D$ for the &quot;New coins from old, smoothly&quot; method, given a factory function $f$, with or without additional assumptions on $f$ (such as smoothness and/or concavity requirements on $f$ and/or its derivatives)?</li>
<li><p>Given a continuous function $f$ that maps $[0,1]$ to $(0,1)$, is the &quot;New coins from old, smoothly&quot; method valid in the following cases?  (Note that the method as written doesn&#39;t apply to non-integer $\alpha$; see also Conjecture 34 of Holtz et al., 2011, which claims the converse of the second result given above.)</p>

<ul>
<li>With $\alpha=1, r=0$, when $f$ is Lipschitz continuous and/or differentiable.</li>
<li>With $\alpha=2, r=1$, when $f$ has a Lipschitz continuous first derivative.</li>
<li>With $\alpha=2, r=2$, when $f$ is twice differentiable.</li>
<li>With $\alpha=4, r=3$, when $f$ has a Lipschitz continuous third derivative.</li>
<li>With $\alpha=4, r=4$, when $f$ is four times differentiable.</li>
<li>With $\alpha=5, r=4$, when $f$ has a Lipschitz continuous fourth derivative.</li>
</ul></li>
</ol>

<p><a id=Reverse_time_martingales></a></p>

<h2>Reverse-time martingales</h2>

<p><a href="https://mathoverflow.net/questions/385244/reverse-time-martingale-for-non-polynomial-approximating-functions"><strong>https://mathoverflow.net/questions/385244/reverse-time-martingale-for-non-polynomial-approximating-functions</strong></a></p>

<p>One way to toss heads with probability $f(\lambda)$ given a coin that shows heads with probability $\lambda$ is to build randomized upper and lower bounds that converge to $f$ on average. These bounds serve as an unbiased estimator of $f(\lambda)$; the algorithm returns 1 with probability equal to the estimate, and 0 otherwise.</p>

<p>Part of the <em>reverse-time martingale algorithm</em> of Łatuszyński et al. (2009/2011) (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#General_Factory_Functions"><strong>General Factory Functions</strong></a>&quot;) to simulate a factory function $f(\lambda)$ is as follows. For each <em>n</em> starting with 1:</p>

<ol>
<li><p>Flip the input coin, and compute the <em>n</em><sup>th</sup> upper and lower bounds of <em>f</em> given the number of heads so far, call them <em>L</em> and <em>U</em>.</p></li>
<li><p>Compute the $(n-1)$th upper and lower bounds of <em>f</em> given the number of heads so far, call them <em>L*</em> and <em>U*</em>. (These bounds must be the same regardless of the outcomes of future coin flips, and the interval [<em>L*</em>, <em>U*</em>] must equal or entirely contain the interval [<em>L</em>, <em>U</em>].)</p></li>
</ol>

<p>More technically (Algorithm 4):</p>

<ol>
<li>Obtain $L_n$ and $U_n$ given $\mathcal{F}_{0, n-1}$,</li>
<li>Compute $L^{\star}<em>{n}$ = $\mathbb{E}(L</em>{n-1} | \mathcal{F}<em>{n})$ and $U^{\star}</em>{n}$ = $\mathbb{E}(U_{n-1} | \mathcal{F}_{n})$,</li>
</ol>

<p>where $\mathcal{F}_n$ is a filtration that depends on $L_n$ and $U_n$.</p>

<p>Though the paper as well as the section on general factory functions that I linked to above shows how this algorithm can be implemented for polynomials, these parts of the algorithm appear to work for any two sequences of functions that converge to $f$, where $L$ or $L^\star$ and $U$ or $U^\star$ are their lower and upper bound approximations. An example for <a href="https://math.stackexchange.com/questions/3904732/what-are-ways-to-compute-polynomials-that-converge-from-above-and-below-to-a-con"><strong>polynomials</strong></a> follows:</p>

<ol>
<li>Given the number of heads $H_n$, $L_n$ is the $H_n$th Bernstein coefficient of the $n$th lower approximating polynomial, and $U_n$ is the $H_n$th Bernstein coefficient of the $n$th upper approximating polynomial.</li>
<li>$L^\star_n$ is the $H_n$th Bernstein coefficient of the $(n-1)$th lower approximating polynomial, and $U^\star_n$ is the $H_n$th Bernstein coefficient of the $(n-1)$th upper approximating polynomial, after elevating both polynomials to degree $n$.</li>
</ol>

<p>But how do these steps work when the <strong>approximating functions (the functions that converge to <em>f</em>) are other than polynomials?</strong> Specifically, what if the approximating functions are rational functions with integer coefficients? rational functions with rational coefficients? arbitrary approximating functions?</p>

<p><a id=Tossing_Heads_According_to_a_Concave_Function></a></p>

<h2>Tossing Heads According to a Concave Function</h2>

<p><a href="https://mathoverflow.net/questions/409174/concave-functions-series-representation-and-converging-polynomials"><strong>https://mathoverflow.net/questions/409174/concave-functions-series-representation-and-converging-polynomials</strong></a></p>

<p><a id=Using_Two_Polynomial_Sequences></a></p>

<h3>Using Two Polynomial Sequences</h3>

<p>If $f$ is a concave factory function, there is a simple way to approximate that function from below with polynomials in Bernstein form.  Define&mdash;</p>

<p>$$a(n,k) = f(k/n).$$</p>

<p>Then the lower polynomials $g_{n}(\lambda)$ will meet all the requirements of the <a href="#Formal_Statement"><strong>formal statement</strong></a> above. Indeed:</p>

<p>$$g_{n}(\lambda)=\sum_{k=0}^{n}f(k/n){n \choose k}\lambda^{k}(1-\lambda)^{n-k}.$$</p>

<p>The trickier part is thus to approximate $f(\lambda)$ from above (that is, to find the upper polynomials $h_n$).  In this sense I am aware of attempts to do so for specific functions: Nacu &amp; Peres (2005) for linear functions, and Flegal &amp; Herbei (2012) for a twice differentiable elbow function.  I am also aware of a result by Holtz et al. 2011, given in the section &quot;New coins from old, smoothly&quot;.</p>

<p>I believe a more general solution is to somehow find the maximum difference between $g_{n}(\lambda)$ and $f(\lambda)$, for each $n$, and to shift the lower polynomial $g_n$ upward by that difference to get the upper polynomial $h_{n}(\lambda)$.  I believe this solution works for the function $\min(\lambda,1-\lambda)$ (by getting the maximum difference at $\lambda=1/2$), but I don&#39;t know if it works for more general concave functions.</p>

<p><a id=Using_a_Series_Expansion></a></p>

<h3>Using a Series Expansion</h3>

<p>There is another way to simulate a concave $f$.  This involves rewriting the concave function as a series expansion as follows:</p>

<p>$$f(\lambda)=\sum_{a\ge 0} \gamma_a(\lambda) = \sum_{a\ge 0} \frac{\gamma_a(\lambda)}{\pi(a, p)} \pi(a, p), \tag{1}$$</p>

<p>where&mdash;</p>

<ul>
<li>$\gamma_a(\lambda) = g_{n_{a}}(\lambda) - g_{n_{a-1}}(\lambda)$,</li>
<li>$\pi(a, p) = p (1-p)^a$ is the probability of getting a non-negative integer $a$ in step 1 of the following algorithm,</li>
<li>$g_n$ is the Bernstein polynomial for $f$ of degree $n$, with $g_{0} := 0$,</li>
<li>$(n_a)$ is an increasing sequence of positive integers, with $n_{-1} := 0$,</li>
<li>$p$ is a rational number in $(0, 1)$, and</li>
<li>$\frac{\gamma_a(\lambda)}{\pi(a, p)}$, which will be a polynomial, must map $[0, 1]$ to $[0, 1]$, and must not equal 0 or 1 anywhere on $(0, 1)$ unless it&#39;s a constant.  In the case of concave functions, this polynomial will always be non-negative.</li>
</ul>

<p>Then an algorithm to toss heads with probability equal to $f$ would be:</p>

<ol>
<li>Flip a coin that shows heads with probability $p$ until that coin shows heads.  Set $a$ to the number of tails.</li>
<li>Write $\frac{\gamma_a(\lambda)}{\pi(a, p)}$ as a polynomial in Bernstein form of degree $n_{a}$ (or a higher degree such that the Bernstein coefficients are all in [0, 1]). Flip the biased coin (with probability of heads $\lambda$) $n$ times, where $n$ is the polynomial&#39;s degree, and let $j$ be the number of heads.</li>
<li>Return 1 with probability equal to the polynomial&#39;s $j$th Bernstein coefficient, or 0 otherwise (see also Goyal and Sigman 2012 for an algorithm to simulate polynomials).</li>
</ol>

<p>(This algorithm has similarities to the one used in the proof in Keane and O&#39;Brien 1994.)</p>

<p>However, using this technique for a given concave $f$ requires finding the appropriate sequence for $n_a$ and the appropriate value of $p$ so that the series expansion can be formed.  Here is an example for $\min(\lambda, 1-\lambda)$ which <em>appears</em> to be correct:</p>

<ul>
<li>$n_a = 2 * 2^a$.</li>
<li>$p = 0.27$.</li>
</ul>

<p>Finding these parameters was far from rigorous, though, and the process will have to be repeated for each concave function.</p>

<p>It&#39;s also possible for $a$ to be generated in step 1 differently, perhaps with a Poisson distribution.  In that case, $\pi(a, p)$ will have to be changed to the corresponding probability of getting $a$ under the new distribution.</p>

<p>Note: Some concave functions can be rewritten as&mdash;</p>

<p>$$f(\lambda)=g_{n_k}(\lambda) + \sum_{a\gt k} \frac{\gamma_a(\lambda)}{\pi(a, p)} \pi(a, p), \tag{2}$$</p>

<p>for some integer $k\ge 0$, if they satisfy the series expansion $(1)$ except that $\frac{\gamma_a(\lambda)}{\pi(a, p)}$ is allowed to equal 1 or greater for some $p$ in $(0, 1)$ and some $a\le k$.  This way of writing $f$ is acceptable for my purposes.</p>

<p><a id=Questions_3></a></p>

<h3>Questions</h3>

<ol>
<li>Given that a factory function $f(\lambda)$ is concave and $C^\alpha$ continuous, is there a formula to find the amount by which to shift the lower polynomials $g_n$ upward so that the upper polynomials $h_n$ meet the formal statement above (or to otherwise convert the lower polynomials to upper polynomials that meet that statement)?  By Holtz&#39;s results, this formula would have to behave asymptotically like $O((\Delta_n(\lambda))^\alpha)$, but I am looking for nonasymptotic results that achieve this rate of convergence.</li>
<li>Given that a factory function $f(\lambda):[0, 1] \to (0, 1)$ is concave and continuous, is it enough to shift $g_{n}(\lambda)$ upward by the maximum difference between $g_{n}(\lambda)$ and $f(\lambda)$, for each $n$, to get the corresponding upper polynomial $h_{n}(\lambda)$?  If not, for which concave functions does this work?</li>
<li>Given that a factory function $f(\lambda):[0, 1] \to [0, 1)$ is concave and continuous, what values of $n_a$ and $p$ will allow that function to have the series expansion $(1)$ or $(2)$?  I suspect that a formula for this question will depend on the smoothness of $f$, due to Holtz&#39;s results.</li>
</ol>

<p><a id=Simulable_and_strongly_simulable_functions></a></p>

<h2>Simulable and strongly simulable functions</h2>

<p><a href="https://mathoverflow.net/questions/404961/from-biased-coins-and-nothing-else-to-biased-coins"><strong>https://mathoverflow.net/questions/404961/from-biased-coins-and-nothing-else-to-biased-coins</strong></a></p>

<p>There are two kinds of factory functions:</p>

<ul>
<li>A function $f(\lambda)$ is <em>simulable</em> if an algorithm exists to toss heads with probability $f(\lambda)$ given a coin with probability of heads $\lambda$ (the &quot;biased coin&quot;) as well as a fair coin.</li>
<li>A function $f(\lambda)$ is <em>strongly simulable</em> if an algorithm exists to toss heads with probability $f(\lambda)$ given <strong>only</strong> a coin with probability of heads $\lambda$.</li>
</ul>

<p>Every strongly simulable function is simulable, but not vice versa.</p>

<p>In fact, Keane and O&#39;Brien (1994) showed already that $f(\lambda)$ is strongly simulable if $f$ is simulable and neither 0 nor 1 is in $f$&#39;s domain (that is, if the biased coin doesn&#39;t show heads every time or tails every time).  And it&#39;s also easy to show that if $f$ is strongly simulable, then $f(0)$ and $f(1)$ must each be 0, 1, or undefined.</p>

<p>However, it&#39;s not so trivial to find the exact class of strongly simulable functions when $f$&#39;s domain includes 0, 1, or both.</p>

<p>As one illustration of this, the proof of Keane and O&#39;Brien relies on generating a geometric random variate and using that variate to control which &quot;part&quot; of the target function $f(\lambda)$ to simulate.   This obviously works on all of [0, 1] if the algorithm uses both the biased coin and a separate fair coin.  However, if only the biased coin is used in the algorithm, the geometric random variate is generated using fair bits via the von Neumann method, which however will never terminate if $\lambda$ is either 0 or 1.</p>

<p>As another illustration, I managed to find the following <a href="https://peteroupc.github.io/bernsupp.html#Which_functions_don_t_require_outside_randomness_to_simulate"><strong>result</strong></a>:</p>

<ul>
<li><p>If $f(\lambda)$ is a factory function that meets the following conditions, then $f$ is strongly simulable.</p>

<ol>
<li>$f(0)$ and $f(1)$ must each be 0, 1, or undefined.</li>
<li>If $f(0) = 0$ or $f(1) = 0$ or both, then either $f$ must be constant on its domain or there must be a polynomial $g(\lambda)$ in Bernstein form whose coefficients are computable and in the interval [0, 1], such that $g(0) = f(0)$ and $g(1) = f(1)$ whenever 0 or 1, respectively, is in the domain of f, and such that $g(\lambda)$ &gt; $f(\lambda)$ for every $\lambda$ in the domain of $f$, except at 0 and 1.</li>
<li>If $f(0) = 1$ or $f(1) = 1$ or both, then either $f$ must be constant on its domain or there must be a polynomial $h(\lambda)$ in Bernstein form whose coefficients are computable and in the interval [0, 1], such that $h(0) = f(0)$ and $h(1) = f(1)$ whenever 0 or 1, respectively, is in the domain of f, and such that $h(\lambda)$ &lt; $f(\lambda)$ for every $\lambda$ in the domain of $f$, except at 0 and 1.</li>
</ol></li>
</ul>

<p>And the proof proceeds by showing, among other things, that the Bernoulli factory for $f$ must flip the input coin and get 0 and 1 before it simulates any fair coin flips via the von Neumann trick.</p>

<p>Question: <strong>Does the result just given describe all the functions that are strongly simulable (using nothing but the biased coin) when the biased coin can show heads every time and/or tails every time?  If not, what is the exact class of strongly simulable functions?</strong>  Examples of functions to ponder are those that are continuous but not Lipschitz continuous at 0 (and $\lambda$ can equal 0), such as $\lambda^\alpha$ where $\alpha \in (0, 1)$, or $\lim_{z\to\lambda} z-z \ln(z)$, or $\lim_{z\to\lambda} -1/(2 \ln(z/2))$.</p>

<p><a id=Multiple_Output_Bernoulli_Factories></a></p>

<h2>Multiple-Output Bernoulli Factories</h2>

<p><a href="https://mathoverflow.net/questions/412772/from-biased-coins-to-biased-coins-as-efficiently-as-possible"><strong>https://mathoverflow.net/questions/412772/from-biased-coins-to-biased-coins-as-efficiently-as-possible</strong></a></p>

<p>Let $J$ be a closed interval on $(0, 1)$, and let $f(\lambda):J \to (0, 1)$ be continuous.</p>

<p>Then by Keane and O&#39;Brien, $f$ admits an algorithm that solves the Bernoulli factory problem for $f$ (using only the biased coin, in fact). A related problem is a Bernoulli factory that takes a coin with unknown probability of heads $\lambda \in J$ and produces <em>one or more</em> samples, at a time, of the probability $f(\lambda)$. This question calls it a <em>multiple-output Bernoulli factory</em>.</p>

<p>Obviously, any single-output Bernoulli factory can produce multiple outputs by running itself multiple times. But for some functions $f$, it may be that producing multiple outputs at a time may use fewer coin flips than producing one output multiple times.</p>

<p>Define the entropy bound as&mdash; $$h(f(\lambda))/h(\lambda),$$ where&mdash; $$h(x)=-x \ln(x)-(1-x) \ln(1-x),$$ is related to the Shannon entropy function.</p>

<p><a id=Questions_4></a></p>

<h3>Questions</h3>

<ol>
<li>Given that a function $f(\lambda)$ is continuous and maps a closed interval in (0, 1) to (0, 1), is there a multiple-output Bernoulli factory algorithm for $f$ with an expected number of coin flips per sample that is arbitrarily close to the entropy bound, uniformly for every $\lambda$ in $f$&#39;s domain? Call such a Bernoulli factory an <em>optimal factory</em>.  (See Nacu and Peres 2005, Question 1.)</li>
<li>Does the answer to question 1 change if the algorithm can also use a fair coin in addition to the biased coin?</li>
</ol>

<p><a id=Functions_with_Optimal_Factories></a></p>

<h3>Functions with Optimal Factories</h3>

<p>So far, the following functions do admit an optimal factory:</p>

<ul>
<li>The functions $\lambda$ and $1-\lambda$.</li>
<li>Constants in [0, 1]. As Nacu and Peres (2005) already showed, any such constant $c$ admits an optimal factory: generate unbiased random bits using Peres&#39;s iterated von Neumann extractor (Peres 1992), then build a binary tree that generates 1 with probability $c$ and 0 otherwise (Knuth and Yao 1976).</li>
</ul>

<p>It is easy to see that if an optimal factory exists for $f(\lambda)$, then one also exists for $1-f(\lambda)$: simply change all ones returned by the $f(\lambda)$ factory into zeros and vice versa.</p>

<p>Also, as Yuval Peres (Jun. 24, 2021) told me, there is an efficient multiple-output Bernoulli factory for $f(\lambda) = \lambda/2$: the key is to flip the input coin enough times to produce unbiased random bits using his extractor (Peres 1992), then multiply each unbiased bit with another input coin flip to get a sample from $\lambda/2$. Given that the sample is equal to 0, there are three possibilities that can &quot;be extracted to produce more fair bits&quot;: either the unbiased bit is 0, or the coin flip is 0, or both are 0.  This algorithm, though, might not count as an <em>optimal factory</em>, and Peres described this algorithm only incompletely.  Indeed, the correctness might depend on how the three possibilities are &quot;extracted to produce more fair bits&quot;; after all, the number of coin flips per sample, for every $\lambda$, must not surpass the entropy bound.</p>

<p>In any case, I believe that not all factory functions admit an optimal factory described here; especially because&mdash;</p>

<ul>
<li>the question may depend on $f$&#39;s range, and</li>
<li>the efficiency of even a <em>single-output</em> Bernoulli factory depends on $f$&#39;s smoothness (e.g., $O(1/n^{(r+\alpha)/2})$ only if $f$&#39;s $r$th derivative is $\alpha$-Hölder continuous for some $\alpha \in (0, 1)$; Holtz et al. 2011).</li>
</ul>

<p>See an <a href="https://peteroupc.github.io/bernsupp.html#Multiple_Output_Bernoulli_Factory"><strong>appendix in one of my articles</strong></a> for more information on my progress on the problem.</p>

<p><a id=From_coin_flips_to_algebraic_functions_via_pushdown_automata></a></p>

<h2>From coin flips to algebraic functions via pushdown automata</h2>

<p><a href="https://cstheory.stackexchange.com/questions/50853/from-coin-flips-to-algebraic-functions-via-pushdown-automata"><strong>https://cstheory.stackexchange.com/questions/50853/from-coin-flips-to-algebraic-functions-via-pushdown-automata</strong></a></p>

<p>This section is about solving the Bernoulli factory problem on a restricted computing model, namely the model of <em>pushdown automata</em> (finite-state machines with a stack) that are driven by flips of a coin and produce new probabilities.</p>

<p><a id=Pushdown_Automata></a></p>

<h3>Pushdown Automata</h3>

<p>A <em>pushdown automaton</em> has a finite set of <em>states</em> and a finite set of <em>stack symbols</em>, one of which is called EMPTY and takes a biased coin with an unknown probability of heads. It starts with a given state and its stack starts with EMPTY. On each iteration:</p>

<ul>
<li>The automaton flips the coin.</li>
<li>Based on the coin flip (HEADS or TAILS), the current state, and the top stack symbol, it moves to a new state (or keeps it unchanged) and replaces the top stack symbol with zero, one, or two symbols. Thus, there are three kinds of <em>transition rules</em>:

<ul>
<li>(<em>state</em>, <em>flip</em>, <em>symbol</em>) &rarr; (<em>state2</em>, {<em>symbol2</em>}): move to <em>state2</em>, replace top stack symbol with same or different one.</li>
<li>(<em>state</em>, <em>flip</em>, <em>symbol</em>) &rarr; (<em>state2</em>, {<em>symbol2</em>, <em>new</em>}): move to <em>state2</em>, replace top stack symbol with <em>symbol2</em>, then <em>push</em> a new symbol (<em>new</em>) onto the stack.</li>
<li>(<em>state</em>, <em>flip</em>, <em>symbol</em>) &rarr; (<em>state2</em>, {}): move to <em>state2</em>, <em>pop</em> the top symbol from the stack.</li>
</ul></li>
</ul>

<p>When the stack is empty, the machine stops and returns either 0 or 1 depending on the state it ends up at. (For the questions below, let <em>flip</em> be HEADS, TAILS, or a rational number in [0, 1]; this likewise reduces to the definition above.  The rest of this question assumes the pushdown automaton terminates with probability 1.)</p>

<p><a id=Algebraic_Functions></a></p>

<h3>Algebraic Functions</h3>

<p>Let $f: (0, 1) \to (0, 1)$ be continuous.  Mossel and Peres (2005) showed that a pushdown automaton can simulate $f$ only if $f$ is <em>algebraic over the rational numbers</em> (there is a nonzero polynomial $P(x, y)$ in two variables and whose coefficients are rational numbers, such that $P(x, f(x)) = 0$ for every $x$ in the domain of $f$).  The algebraic function generated by pushdown automata corresponds to a system of polynomial equations, as described by Mossel and Peres (2005) and Esparza et al. 2004.</p>

<p>Let $\mathcal{C}$ be the class of continuous functions that map (0, 1) to (0, 1) and are algebraic over rationals.  The constants 0 and 1 are also in $\mathcal{C}$.</p>

<p>Let $\mathcal{D} \subseteq \mathcal{C}$ be the class of functions that a pushdown automaton can simulate.</p>

<p>I don&#39;t yet know whether $\mathcal{D}=\mathcal{C}$ (and that was also a question of Mossel and Peres).</p>

<p>The following section of my open-source page, <a href="https://peteroupc.github.io/morealg.html#Pushdown_Automata_and_Algebraic_Functions,"><strong>https://peteroupc.github.io/morealg.html#Pushdown_Automata_and_Algebraic_Functions,</strong></a> contains information on the question. As that section shows, I could establish the following about the class $\mathcal{D}$:</p>

<ul>
<li>$\sqrt{\lambda}$ is in $\mathcal{D}$, and so is every rational function in $\mathcal{C}$.</li>
<li>If $f(\lambda)$ and $g(\lambda)$ are in $\mathcal{D}$, then so are their product and composition.</li>
<li>If $f(\lambda)$ is in $\mathcal{D}$, then so is every Bernstein-form polynomial in the variable $f(\lambda)$ with coefficients in $\mathcal{D}$.</li>
<li>If a pushdown automaton can generate a discrete distribution of <em>n</em>-letter words, then that distribution&#39;s probability generating function is in $\mathcal{D}$ (cf. Dughmi et al. 2021).</li>
<li>If a pushdown automaton can generate a discrete distribution of <em>n</em>-letter words of the same letter, it can generate that distribution conditioned on a finite set of word lengths, or a periodic infinite set of word lengths (e.g., odd word lengths only).</li>
<li>Every quadratic irrational in (0, 1) is in $\mathcal{D}$.</li>
</ul>

<p><a id=Questions_5></a></p>

<h3>Questions</h3>

<ol>
<li>For every function in class $\mathcal{C}$, is there a pushdown automaton that can simulate that function? (In other words, is $\mathcal{D}=\mathcal{C}$?).</li>
<li>In particular, is min($\lambda$, $1-\lambda$) in class $\mathcal{D}$? What about $\lambda^{1/p}$ for some prime $p\ge 3$?</li>
</ol>

<p><a id=Other_Questions></a></p>

<h2>Other Questions</h2>

<p><a href="https://stats.stackexchange.com/questions/541402/what-are-relatively-simple-simulations-that-succeed-with-an-irrational-probabili"><strong>Simple simulation algorithms</strong></a>:</p>

<ul>
<li><p>What simulations exist that are &quot;relatively simple&quot; and succeed with an irrational probability between 0 and 1? What about &quot;relatively simple&quot; Bernoulli factory algorithms for factory functions?  Here, &quot;relatively simple&quot; means that the algorithm:</p>

<ul>
<li>Should use only uniform random integers (or bits) and integer arithmetic.</li>
<li>Does not use floating-point arithmetic or make direct use of square root or transcendental functions.</li>
<li>Should not use rational arithmetic or increasingly complex approximations, except as a last resort.</li>
</ul>

<p>See also Flajolet et al., &quot;On Buffon machines and numbers&quot;, 2010.  There are many ways to describe the irrational probability or factory function. I seek references to papers or books that describe irrational constants or factory functions in any of the following ways:</p>

<ul>
<li>For irrational constants:

<ul>
<li>Simple <a href="https://peteroupc.github.io/bernoulli.html#Continued_Fractions"><strong>continued fraction</strong></a> expansions.</li>
<li>Closed shapes inside the unit square whose area is an irrational number.  (Includes algorithms that tell whether a box lies inside, outside, or partly inside or outside the shape.)    <a href="https://peteroupc.github.io/morealg.html#pi___4"><strong>Example.</strong></a></li>
<li>Generate a uniform (<em>x</em>, <em>y</em>) point inside a closed shape, then return 1 with probability <em>x</em>.  For what shapes is the expected value of <em>x</em> an irrational number?  <a href="https://peteroupc.github.io/morealg.html#4_3___pi"><strong>Example.</strong></a></li>
<li>Functions that map [0, 1] to [0, 1] whose integral (area under curve) is an irrational number.</li>
</ul></li>
<li>Bernoulli factory functions with any of the following series expansions, using rational arithmetic only:

<ul>
<li>Series with non-negative terms where <em>f</em>(0) is 0 and <em>f</em>(1) is rational or vice versa (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#Certain_Power_Series"><strong>Certain Power Series</strong></a>&quot;).</li>
<li>Series with non-negative terms that can be &quot;tucked&quot; under a discrete probability mass function (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#Convex_Combinations"><strong>Convex Combinations</strong></a>&quot;).</li>
<li>Alternating power series (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#Certain_Power_Series"><strong>Certain Power Series</strong></a>&quot;).</li>
<li>Series with non-negative terms and bounds on the truncation error (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#Certain_Converging_Series"><strong>Certain Converging Series</strong></a>&quot;).</li>
</ul></li>
</ul></li>
</ul>

<p><a id=Remarks></a></p>

<h2>Remarks</h2>

<ul>
<li>Besides the questions on concave functions given above, there is also the question of whether the solution terminates with a finite expected running time.  In this sense, Nacu &amp; Peres showed that a finite expected time is possible only if $f$ is Lipschitz continuous, and I strongly suspect it&#39;s not possible either unless $f$ has a Hölder continuous fourth derivative, in view of the results by Holtz given earlier.</li>
<li>A practical solution to the questions on concave functions might require more sophisticated methods than simply using $f$&#39;s Bernstein polynomials, since I am aware that the plain Bernstein approximation doesn&#39;t improve in convergence rate if a function has two or more continuous derivatives (as opposed to just one), at least without further knowledge of $f$.</li>
</ul>

<p>The following remarks relate to the questions on pushdown automata:</p>

<ul>
<li>Etessami and Yannakakis (2009) showed that pushdown automata with rational probabilities are equivalent to recursive Markov chains (with rational transition probabilities), and that for every recursive Markov chain, the system of polynomial equations has nonnegative coefficients. But this paper doesn&#39;t deal with the case of recursive Markov chains where the transition probabilities cannot just be rational, but can also be $\lambda$ and $1-\lambda$ where $\lambda$ is an unknown rational or irrational probability of heads.</li>
<li>Banderier and Drmota (2014) showed the asymptotic behavior of power series solutions $f(\lambda)$ of a polynomial system, where both the series and the system have nonnegative real coefficients. Notably, functions of the form $\lambda^{1/p}$ where $p\ge 3$ is not a power of 2, are not possible solutions, because their so-called &quot;critical exponent&quot; is not dyadic. But the result seems not to apply to <em>piecewise</em> power series such as $\min(\lambda,1-\lambda)$, which are likewise algebraic functions.</li>
</ul>

<p><a id=My_Attempt></a></p>

<h2>My Attempt</h2>

<p>The Python script <a href="https://github.com/peteroupc/peteroupc.github.io/blob/master/lorentz.py"><strong>lorentz.py</strong></a> shows my attempt to implement the Holtz approximation scheme. It implements an algorithm to toss heads with probability equal to a C2 or C4 continuous piecewise polynomial factory function.  However, it relies on an unproven conjecture (Conjecture 34) in the Holtz paper.</p>

<p>Based on this attempt, the C4 continuous case is efficient enough for my purposes, but the case of functions with lesser regularity is not so efficient (such as Lipschitz or C2).</p>

<hr>

<p>Here is my current progress for the Lorentz operator for $\alpha=2$, so $r=2$ (which applies to twice-differentiable functions with Hölder continuous second derivative, even though the paper appears not to apply when $\alpha$ is an integer). Is the following work correct?</p>

<p>The Lorentz operator for $r=2$ finds the degree-n Bernstein polynomial for the target function $f$, elevates it to degree $n+r$, then shifts the coefficient at $k+1$ by $-f\prime\prime(k/n) A(n,k)$ (but the coefficients at 0 and $n+r$ are not shifted this way), where:</p>

<p>$$A(n,k) = (1/(4n)) \times  2 \times (n+2-k)/((n+1)\times (n+2)),$$</p>

<p>where $k$ is an integer in $[0,n+r]$. Observing that $A(n,k)$ equals 0 at 0 and at $n+r$, and has a peak at $(n+r)/2$,
the shift will be no greater (in absolute value) than
$A(n,(n+r)/2)*F$, where $F$ is the maximum absolute value
of the second derivative of $f(\lambda)$.  $A(n,(n+r)/2)$ is bounded
above by $(3/16)/n$ for $n\ge 1$, and by $(3/22)/n$ for $n\ge 10$.</p>

<p>Now, find $\theta_\alpha$ for $\alpha=2$, according to Lemma 25:</p>

<p>Let $0&lt;\gamma&lt;2^{\alpha/2}-1$ ($\gamma&lt;1$ if $\alpha=2$).</p>

<p>Solve for $K$: $$(1-(1+\gamma)/2^{\alpha/2} - 4/(4*K)) = 0.$$  The solution for $\alpha=2$ is $K = 2/(1-\gamma)$.</p>

<p>Now find:  $$\theta_a = ((4/4) K^{\alpha/2}/n^\alpha) / ((1-(1+\gamma)/2^\alpha)/n^\alpha)$$ The solution for $\alpha=2$ is $\theta_a = 8/((\gamma-3) (\gamma-1))$.</p>

<p>For $\gamma=1/100$ and $\alpha=2$, $\theta_a = 80000/29601 &lt; 2.703$.</p>

<p>There&#39;s no need to check whether the output polynomials have Bernstein coefficients in $(0, 1)$, since out-of-bounds polynomials will be replaced with 0 or 1 as necessary &mdash; which is more practical and convenient.</p>

<p>Now all that remains is to find $D$ given $\alpha=2$. I believe this will involve the following:</p>

<ol>
<li>Find upper bounds for the constants $C_{19}$, $C_{21}$, and $C_{24}$ (used in Lemmas 19, 21, and 24, respectively) given $\alpha$ and $r$ (and for $C_{19}$ at least, the best I can do is a visual inspection of the plot).</li>
<li>Calculate $t = C_{24} C_{19} (1+2 C_{21}) b^{\alpha/2} M$, where $M$ is the Hölder constant of $f$&#39;s $r$th derivative (see &quot;Step 4&quot; in &quot;The Iterative Construction&quot;).</li>
<li>Find the maximum value of $(t (x+(1-x))^r B_n \Delta_n{\alpha}) / ((x+(1-x))^r B_n\phi_n)$ (the best I can do is a visual inspection), and set $D$ to that value.</li>
</ol>

<p>Moreover, there remains to find the parameters for the Lorentz operator when $r$ is 0, 1, 2, or 4. (When $r=0$ or $r=1$, the Lorentz operator is simply the Bernstein polynomial of degree $n$, elevated $r$ degrees to degree $n+r$.)</p>

<p><a id=References></a></p>

<h2>References</h2>

<ul>
<li>Łatuszyński, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., &quot;<a href="https://arxiv.org/abs/0907.4018v2"><strong>Simulating events of unknown probabilities via reverse time martingales</strong></a>&quot;, arXiv:0907.4018v2 [stat.CO], 2009/2011.</li>
<li>Keane, M. S., and O&#39;Brien, G. L., &quot;A Bernoulli factory&quot;, <em>ACM Transactions on Modeling and Computer Simulation</em> 4(2), 1994.</li>
<li>Holtz, O., Nazarov, F., Peres, Y., &quot;New Coins from Old, Smoothly&quot;, Constructive Approximation 33 (2011).</li>
<li>Nacu, Şerban, and Yuval Peres. &quot;Fast simulation of new coins from old&quot;, The Annals of Applied Probability 15, no. 1A (2005): 93-115.</li>
<li>Knuth, Donald E. and Andrew Chi-Chih Yao. &quot;The complexity of nonuniform random number generation&quot;, in <em>Algorithms and Complexity: New Directions and Recent Results</em>, 1976.</li>
<li>Peres, Y., &quot;<a href="https://projecteuclid.org/euclid.aos/1176348543"><strong>Iterating von Neumann&#39;s procedure for extracting random bits</strong></a>&quot;, Annals of Statistics 1992,20,1, p. 590-597.</li>
<li>Mossel, Elchanan, and Yuval Peres. New coins from old: computing with unknown bias. Combinatorica, 25(6), pp.707-724, 2005.</li>
<li>Icard, Thomas F., &quot;Calibrating generative models: The probabilistic Chomsky–Schützenberger hierarchy.&quot; Journal of Mathematical Psychology 95 (2020): 102308.</li>
<li>Dughmi, Shaddin, Jason Hartline, Robert D. Kleinberg, and Rad Niazadeh. &quot;Bernoulli Factories and Black-box Reductions in Mechanism Design.&quot; Journal of the ACM (JACM) 68, no. 2 (2021): 1-30.</li>
<li>Etessami, K. And Yannakakis, M., &quot;Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations&quot;, Journal of the ACM 56(1), pp.1-66, 2009.</li>
<li>Banderier, C. And Drmota, M., 2015. Formulae and asymptotics for coefficients of algebraic functions. Combinatorics, Probability and Computing, 24(1), pp.1-53.</li>
<li>Esparza, J., Kučera, A. and Mayr, R., 2004, July. Model checking probabilistic pushdown automata. In Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science, 2004. (pp. 12-21). IEEE.</li>
<li>Flajolet, P., Pelletier, M., Soria, M., &quot;<a href="https://arxiv.org/abs/0906.5560v2"><strong>On Buffon machines and numbers</strong></a>&quot;, arXiv:0906.5560v2 [math.PR], 2010.</li>
<li>von Neumann, J., &quot;Various techniques used in connection with random digits&quot;, 1951.</li>
<li>G.G. Lorentz, &quot;The degree of approximation by polynomials with positive coefficients&quot;, 1966.</li>
<li>Micchelli, C. (1973). The saturation class and iterates of the Bernstein polynomials. Journal of Approximation Theory, 8(1), 1-18.</li>
<li>Guan, Zhong. &quot;<a href="https://arxiv.org/pdf/0909.0684"><strong>Iterated Bernstein polynomial approximations</strong></a>.&quot; arXiv preprint arXiv:0909.0684 (2009).</li>
<li>Güntürk, C. Sinan, and Weilin Li. &quot;<a href="https://arxiv.org/pdf/2112.09183"><strong>Approximation with one-bit polynomials in Bernstein form</strong></a>&quot; arXiv preprint arXiv:2112.09183 (2021).</li>
<li>Draganov, Borislav R. &quot;On simultaneous approximation by iterated Boolean sums of Bernstein operators.&quot; Results in Mathematics 66, no. 1 (2014): 21-41.</li>
</ul>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>

<div class="noprint">
<p>
<a href="//twitter.com/intent/tweet">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
if("share" in navigator){
 document.getElementById("sharer").href="javascript:void(null)";
 document.getElementById("sharer").innerHTML="Share This Page";
 navigator.share({title:document.title,url:document.location.href}).then(
   function(){});
} else {
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
}
</script>
</body></html>
