<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>The Sampling Problem</title><meta name="citation_title" content="The Sampling Problem"><meta name="citation_pdf_url" content="https://peteroupc.github.io/sampling.pdf"><meta name="citation_url" content="https://peteroupc.github.io/sampling.html"><meta name="citation_date" content="2023/07/14"><meta name="citation_online_date" content="2023/07/14"><meta name="og:title" content="The Sampling Problem"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/sampling.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="The Sampling Problem"><meta name="author" content="Peter Occil"/><meta name="citation_author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css">
            <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, preferredFont: "TeX" },
                    tex2jax: { displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], processEscapes: true } });
            </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>The Sampling Problem</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p>Suppose $(X_0, X_1, X_2, X_3, ...)$ is an endless stream of random variates, or <em>input values</em>.</p>

<p>Let <code>InDist</code> be the distribution of these input values, and let $\lambda$ be an unknown parameter that determines the distribution <code>InDist</code>, such as its expected value.  Suppose the problem is to <strong>produce a random variate with a distribution</strong> <code>OutDist</code> <strong>that depends on the unknown parameter $\lambda$</strong>.  Then, of the algorithms in the section &quot;<a href="https://peteroupc.github.io/randmisc.md#Sampling_Distributions_Using_Incomplete_Information"><strong>Sampling Distributions Using Incomplete Information</strong></a>&quot;</p>

<ul>
<li>In <strong>Algorithm 1</strong> (Jacob and Thiery 2015)[^1], <code>InDist</code> is arbitrary but must have a known minimum and maximum, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is non-negative and has an expected value of $f(\lambda)$.</li>
<li>In <strong>Algorithm 2</strong> (Duvignau 2015)[^2], <code>InDist</code> is a fair die with an unknown number of faces, $\lambda$ is the number of faces, and <code>OutDist</code> is a specific distribution that depends on the number of faces.</li>
<li>In <strong>Algorithm 3</strong> (Lee et al. 2014)[^3], <code>InDist</code> is arbitrary, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is non-negative and has an expected value equal to the mean of $f(X)$, where $X$ is an input value taken.</li>
<li>In <strong>Algorithm 4</strong> (Jacob and Thiery 2015)[^4], <code>InDist</code> is arbitrary but must have a known minimum, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is non-negative and has an expected value of $f(\lambda)$.</li>
<li>In <strong>Algorithm 5</strong> (Akahira et al. 1992)[^5], <code>InDist</code> is Bernoulli, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> has an expected value of $f(\lambda)$. [^6]</li>
<li>In the <a href="https://peteroupc.github.io/bernoulli.html"><strong>Bernoulli factory problem</strong></a>, <code>InDist</code> is Bernoulli, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is Bernoulli with an expected value of $f(\lambda)$.</li>
</ul>

<p>In all cases given above, each input value is independent of everything else.</p>

<p>There are numerous other cases of interest that are not covered in the algorithms above.  An example is the case of <strong>Algorithm 5</strong> except <code>InDist</code> is any discrete distribution, not just Bernoulli.  An interesting topic is to answer the following: In which cases (and for which functions $f$) can the problem be solved...</p>

<ul>
<li>...when the number of input values taken is finite with probability 1 (a <em>sequential unbiased</em> estimator)?</li>
<li>...when only a fixed number $n$ of input values can be taken (a fixed-sample-size unbiased estimator)?</li>
<li>...using an algorithm that produces outputs whose expected value <em>approaches</em> $f(\lambda)$ as more input values are taken (an <em>asymptotically unbiased</em> estimator)?</li>
</ul>

<p>The answers to these questions will depend on&mdash;</p>

<ul>
<li>the allowed distributions for <code>InDist</code>,</li>
<li>the allowed distributions for <code>OutDist</code>,</li>
<li>which parameter $\lambda$ is unknown,</li>
<li>whether the inputs are independent, and</li>
<li>whether outside randomness is allowed.</li>
</ul>

<p>It should be noted that many of these cases have been studied and resolved in academic papers and books (e.g., Keane and O&#39;Brien (1994)[^7] for the Bernoulli factory problem) &mdash; the problem here is one of bringing all these results together in one place.  An additional question is to find lower bounds on the input/output ratio that an algorithm can achieve as the number of inputs taken increases (e.g., Nacu and Peres (2005, Question 2)[^8]).</p>

<p><a id=Results></a></p>

<h2>Results</h2>

<p>The following is an example of results for this problem.</p>

<ul>
<li>Suppose <code>InDist</code> takes on numbers from a finite set; $\lambda$ is the expected value of <code>InDist</code>; and <code>OutDist</code> has an expected value of $f(\lambda)$.

<ul>
<li>Then a fixed-size unbiased estimator exists only if $f$ is a polynomial of degree $n$ or less, where $n$ is the number of inputs taken ((Lehmann 1983, for coin flips)[^9], Paninski (2003, proof of Proposition 8, more generally)[^10]).</li>
<li>The existence of sequential unbiased estimators is claimed by Singh (1964).  But see Akahira et al. (1992)[^11].</li>
</ul></li>
</ul>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<p>[^1]: Jacob, P.E., Thiery, A.H., &quot;On nonnegative unbiased estimators&quot;, Ann. Statist., Volume 43, Number 2 (2015), 769-784.</p>

<p>[^2]: Duvignau, R., 2015. Maintenance et simulation de graphes aléatoires dynamiques (Doctoral dissertation, Université de Bordeaux).</p>

<p>[^3]: Lee, A., Doucet, A. and Łatuszyński, K., 2014. &quot;<a href="https://arxiv.org/abs/1407.5770v1"><strong>Perfect simulation using atomic regeneration with application to Sequential Monte Carlo</strong></a>&quot;, arXiv:1407.5770v1  [stat.CO].</p>

<p>[^4]: Jacob, P.E., Thiery, A.H., &quot;On nonnegative unbiased estimators&quot;, Ann. Statist., Volume 43, Number 2 (2015), 769-784.</p>

<p>[^5]: AKAHIRA, Masafumi, Kei TAKEUCHI, and Ken-ichi KOIKE. &quot;Unbiased estimation in sequential binomial sampling&quot;,  Rep. Stat. Appl. Res., JUSE 39 1-13, 1992.</p>

<p>[^6]: Singh (1964, &quot;Existence of unbiased estimates&quot;, Sankhyā A 26) claimed that an estimation algorithm with expected value $f(\lambda)$ exists for a more general class of <code>InDist</code> distributions than the Bernoulli distribution, as long as there are polynomials that converge pointwise to $f$, and Bhandari and Bose (1990, &quot;Existence of unbiased estimates in sequential binomial experiments&quot;, Sankhyā A 52) claimed necessary conditions for those algorithms.  However, Akahira et al. (1992) questioned the claims of both papers, and the latter paper underwent a correction, which I haven&#39;t seen (Sankhyā A 55, 1993).</p>

<p>[^7]: Keane,  M.  S.,  and  O&#39;Brien,  G.  L., &quot;A Bernoulli factory&quot;, <em>ACM Transactions on Modeling and Computer Simulation</em> 4(2), 1994.</p>

<p>[^8]:  Nacu, Şerban, and Yuval Peres. &quot;<a href="https://projecteuclid.org/euclid.aoap/1106922322"><strong>Fast simulation of new coins from old</strong></a>&quot;, The Annals of Applied Probability 15, no. 1A (2005): 93-115.</p>

<p>[^9]: Lehmann, E.L., <em>Theory of Point Estimation</em>, 1983.</p>

<p>[^10]: Paninski, Liam. “Estimation of Entropy and Mutual Information.” Neural Computation 15 (2003): 1191-1253.</p>

<p>[^11]: AKAHIRA, Masafumi, Kei TAKEUCHI, and Ken-ichi KOIKE. &quot;Unbiased estimation in sequential binomial sampling&quot;,  Rep. Stat. Appl. Res., JUSE 39 1-13, 1992.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>

<div class="noprint">
<p>
<a href="//twitter.com/intent/tweet">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
<p style='font-size:120%;font-weight:bold'><a href='https://peteroupc.github.io/sampling.pdf'>Download a PDF of this page</a></p></nav></body></html>
