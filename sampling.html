<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>The Sampling Problem</title><meta name="citation_title" content="The Sampling Problem"><meta name="citation_pdf_url" content="https://peteroupc.github.io/sampling.pdf"><meta name="citation_url" content="https://peteroupc.github.io/sampling.html"><meta name="citation_date" content="2023/07/18"><meta name="citation_online_date" content="2023/07/18"><meta name="og:title" content="The Sampling Problem"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/sampling.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="The Sampling Problem"><meta name="author" content="Peter Occil"/><meta name="citation_author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css">
            <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, preferredFont: "TeX" },
                    tex2jax: { displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], processEscapes: true } });
            </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>The Sampling Problem</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p>This page is about a mathematical problem of <strong>sampling a probability distribution with unknown parameters</strong>.  This problem can be described as sampling from a new distribution using an endless stream of random variates from an incompletely known distribution.</p>

<p>Suppose there is an endless stream of numbers, each generated at random and independently from each other, and as many numbers can be sampled from the stream as desired.  Let $(X_0, X_1, X_2, X_3, ...)$ be that endless stream, and call the numbers <em>input values</em>.</p>

<p>Let <code>InDist</code> be the probability distribution of these input values, and let $\lambda$ be an unknown parameter that determines the distribution <code>InDist</code>, such as its expected value (or mean or &quot;long-run average&quot;).  Suppose the problem is to <strong>produce a random variate with a distribution</strong> <code>OutDist</code> <strong>that depends on the unknown parameter $\lambda$</strong>.  Then, of the algorithms in the section &quot;<a href="https://peteroupc.github.io/randmisc.md#Sampling_Distributions_Using_Incomplete_Information"><strong>Sampling Distributions Using Incomplete Information</strong></a>&quot;:</p>

<ul>
<li>In <strong>Algorithm 1</strong> (Jacob and Thiery 2015)[^1], <code>InDist</code> is arbitrary but must have a known minimum and maximum, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is non-negative and has an expected value of $f(\lambda)$.</li>
<li>In <strong>Algorithm 2</strong> (Duvignau 2015)[^2], <code>InDist</code> is a fair die with an unknown number of faces, $\lambda$ is the number of faces, and <code>OutDist</code> is a specific distribution that depends on the number of faces.</li>
<li>In <strong>Algorithm 3</strong> (Lee et al. 2014)[^3], <code>InDist</code> is arbitrary, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is non-negative and has an expected value equal to the mean of $f(X)$, where $X$ is an input value taken.</li>
<li>In <strong>Algorithm 4</strong> (Jacob and Thiery 2015)[^1], <code>InDist</code> is arbitrary but must have a known minimum, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is non-negative and has an expected value of $f(\lambda)$.</li>
<li>In <strong>Algorithm 5</strong> (Akahira et al. 1992)[^4], <code>InDist</code> is Bernoulli, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> has an expected value of $f(\lambda)$.</li>
<li>In the <a href="https://peteroupc.github.io/bernoulli.html"><strong>Bernoulli factory problem</strong></a> (a problem of turning biased coins to biased coins), <code>InDist</code> is Bernoulli, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is Bernoulli with an expected value of $f(\lambda)$.</li>
</ul>

<p>In all cases given above, each input value is independent of everything else.</p>

<p>There are numerous other cases of interest that are not covered in the algorithms above.  An example is the case of <strong>Algorithm 5</strong> except <code>InDist</code> is any discrete distribution, not just Bernoulli. [^5]  An interesting topic is to answer the following: In which cases (and for which functions $f$) can the problem be solved...</p>

<ul>
<li>...when the number of input values taken is random, but finite with probability 1 (a <em>sequential unbiased</em> estimator)?</li>
<li>...when only a fixed number $n$ of input values can be taken (a fixed-sample-size unbiased estimator)?</li>
<li>...using an algorithm that produces outputs whose expected value <em>approaches</em> $f(\lambda)$ as more input values are taken (an <em>asymptotically unbiased</em> estimator)?</li>
</ul>

<p>The answers to these questions will depend on&mdash;</p>

<ul>
<li>the allowed distributions for <code>InDist</code>,</li>
<li>the allowed distributions for <code>OutDist</code>,</li>
<li>which parameter $\lambda$ is unknown,</li>
<li>whether the inputs are independent, and</li>
<li>whether outside randomness is allowed.</li>
</ul>

<p>An additional question is to find lower bounds on the input/output ratio that an algorithm can achieve as the number of inputs taken increases (e.g., Nacu and Peres (2005, Question 2)[^6]).</p>

<p>My interest on the problem is in the existence and construction of simple-to-implement algorithms that solve the <em>sampling problem</em> given here.  In addition, the cases that most interest me are when&mdash;</p>

<ul>
<li>$\lambda$ is <code>InDist</code>&#39;s expected value, and</li>
<li><code>OutDist</code> has an expected value of $f(\mathbb{E}[X])$ or $\mathbb{E}[f(X)]$, where $X$ is an input value taken,</li>
</ul>

<p>with or without other conditions.</p>

<p><a id=Results></a></p>

<h2>Results</h2>

<p>It should be noted that many special cases of the sampling problem have been studied and resolved in academic papers and books.</p>

<p>The problem here is one of bringing all these results together in one place.</p>

<p>The following are examples of results for this problem.</p>

<ul>
<li>Suppose <code>InDist</code> takes on numbers from a finite set; $\lambda$ is the expected value of <code>InDist</code>; and <code>OutDist</code> has an expected value of $f(\lambda)$.

<ul>
<li>A fixed-size unbiased estimator exists only if $f$ is a homogeneous polynomial of degree $n$ or less, where $n$ is the number of inputs taken (Lehmann (1983, for coin flips)[^7], Paninski (2003, proof of Proposition 8, more generally)[^8]).</li>
<li>The existence of sequential unbiased estimators is claimed by Singh (1964)[^9].  But see Akahira et al. (1992)[^4].</li>
</ul></li>
<li>Suppose <code>InDist</code> has a finite mean, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is nonnegative and has an expected value of $f(\lambda)$.

<ul>
<li>There is no sequential unbiased estimator (and thus no fixed-size unbiased estimator) (Jacob and Thiery 2015)[^1].</li>
</ul></li>
<li>Suppose <code>InDist</code> has a finite mean and is supported on $[a, \infty)$, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is nonnegative and has an expected value of $f(\lambda)$.

<ul>
<li>A sequential unbiased estimator exists only if $f$ is nowhere decreasing (Jacob and Thiery 2015)[^1].</li>
</ul></li>
<li>Suppose <code>InDist</code> has a finite mean and is supported on $(\infty, a]$, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is nonnegative and has an expected value of $f(\lambda)$.

<ul>
<li>A sequential unbiased estimator exists only if $f$ is nowhere increasing (Jacob and Thiery 2015)[^1].</li>
</ul></li>
<li>Suppose <code>InDist</code> is Bernoulli, $\lambda$ is the expected value of <code>InDist</code>, and <code>OutDist</code> is Bernoulli with an expected value of $f(\lambda)$.

<ul>
<li>Let $D$ be the set of allowed values for $\lambda$.  Thus, $D$ is either the closed unit interval or a subset thereof.</li>
<li>A sequential unbiased estimator exists if and only if $f$ is everywhere 0, everywhere 1, or continuous and polynomially bounded on $D$ (Keane and O&#39;Brien 1994)[^10].</li>
<li>A fixed-size unbiased estimator exists if and only if $f$ is a polynomial of degree $n$ with $n+1$ Bernstein coefficients in the closed unit interval, where $n$ is the number of inputs taken (Goyal and Sigman 2012)[^11].</li>
<li>Perhaps it is true that an asymptotically unbiased estimator exists if and only if there are polynomials $p_1, p_2, ...$ that converge pointwise to $f$ on $D$ (that is, for each $\lambda$ in $D$, $p_n(\lambda)$ approaches $f(\lambda)$ as $n$ increases), and the polynomials&#39; Bernstein coefficients lie in the closed unit interval (see also Singh (1964)[^9]).</li>
</ul></li>
</ul>

<p>There are also two other results on the existence of fixed-sample and asymptotically unbiased estimators, but they are relatively hard to translate to this problem in a simple way: Liu and Brown (1993)[^12], Hirano and Porter (2012)[^13].</p>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<p>[^1]: Jacob, P.E., Thiery, A.H., &quot;On nonnegative unbiased estimators&quot;, Ann. Statist., Volume 43, Number 2 (2015), 769-784.</p>

<p>[^2]: Duvignau, R., &quot;Maintenance et simulation de graphes aléatoires dynamiques&quot;, Doctoral dissertation, Université de Bordeaux, 2015.</p>

<p>[^3]: Lee, A., Doucet, A. and Łatuszyński, K., 2014. &quot;<a href="https://arxiv.org/abs/1407.5770v1"><strong>Perfect simulation using atomic regeneration with application to Sequential Monte Carlo</strong></a>&quot;, arXiv:1407.5770v1  [stat.CO].</p>

<p>[^4]: AKAHIRA, Masafumi, Kei TAKEUCHI, and Ken-ichi KOIKE. &quot;Unbiased estimation in sequential binomial sampling&quot;,  Rep. Stat. Appl. Res., JUSE 39 1-13, 1992.</p>

<p>[^5]: Singh (1964, &quot;Existence of unbiased estimates&quot;, Sankhyā A 26) claimed that an estimation algorithm with expected value $f(\lambda)$ exists for a more general class of <code>InDist</code> distributions than the Bernoulli distribution, as long as there are polynomials that converge pointwise to $f$, and Bhandari and Bose (1990, &quot;Existence of unbiased estimates in sequential binomial experiments&quot;, Sankhyā A 52) claimed necessary conditions for those algorithms.  However, Akahira et al. (1992) questioned the claims of both papers, and the latter paper underwent a correction, which I haven&#39;t seen (Sankhyā A 55, 1993).</p>

<p>[^6]: Nacu, Şerban, and Yuval Peres. &quot;<a href="https://projecteuclid.org/euclid.aoap/1106922322"><strong>Fast simulation of new coins from old</strong></a>&quot;, The Annals of Applied Probability 15, no. 1A (2005): 93-115.</p>

<p>[^7]: Lehmann, E.L., <em>Theory of Point Estimation</em>, 1983.</p>

<p>[^8]: Paninski, Liam. “Estimation of Entropy and Mutual Information.” Neural Computation 15 (2003): 1191-1253.</p>

<p>[^9]: R. Singh, &quot;Existence of unbiased estimates&quot;, Sankhyā A 26, 1964.</p>

<p>[^10]: Keane,  M.  S.,  and  O&#39;Brien,  G.  L., &quot;A Bernoulli factory&quot;, <em>ACM Transactions on Modeling and Computer Simulation</em> 4(2), 1994.</p>

<p>[^11]: Goyal, V. and Sigman, K., 2012. On simulating a class of Bernstein polynomials. ACM Transactions on Modeling and Computer Simulation (TOMACS), 22(2), pp.1-5.</p>

<p>[^12]: Liu., R.C., Brown, L.D., &quot;Nonexistence of informative unbiased estimators in singular problems&quot;, Annals of Statistics 21(1), 1993.</p>

<p>[^13]: Hirano, Keisuke, and Jack R. Porter. &quot;Impossibility results for nondifferentiable functionals.&quot; Econometrica 80, no. 4 (2012): 1769-1790.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>

<div class="noprint">
<p>
<a href="//twitter.com/intent/tweet">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
<p style='font-size:120%;font-weight:bold'><a href='https://peteroupc.github.io/sampling.pdf'>Download a PDF of this page</a></p></nav></body></html>
