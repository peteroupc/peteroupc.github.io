<!DOCTYPE html><html xmlns:dc="http://purl.org/dc/terms/" itemscope itemtype="http://schema.org/Article"><head><meta http-equiv=Content-Type content="text/html; charset=utf-8"><title>Supplemental Notes for Bernoulli Factory Algorithms</title><meta name="citation_title" content="Supplemental Notes for Bernoulli Factory Algorithms"><meta name="citation_pdf_url" content="https://peteroupc.github.io/bernsupp.pdf"><meta name="citation_url" content="https://peteroupc.github.io/bernsupp.html"><meta name="citation_date" content="2023/03/10"><meta name="citation_online_date" content="2023/03/10"><meta name="og:title" content="Supplemental Notes for Bernoulli Factory Algorithms"><meta name="og:type" content="article"><meta name="og:url" content="https://peteroupc.github.io/bernsupp.html"><meta name="og:site_name" content="peteroupc.github.io"><meta name="twitter:title" content="Supplemental Notes for Bernoulli Factory Algorithms"><meta name="author" content="Peter Occil"/><meta name="citation_author" content="Peter Occil"/><meta name="viewport" content="width=device-width"><link rel=stylesheet type="text/css" href="/style.css">
            <script type="text/x-mathjax-config"> MathJax.Hub.Config({"HTML-CSS": { availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, preferredFont: "TeX" },
                    tex2jax: { displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], inlineMath: [ ["$", "$"], ["\\\\(","\\\\)"] ], processEscapes: true } });
            </script><script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_HTML-full"></script></head><body>  <div class="header">
<nav><p><a href="#navigation">Menu</a> - <a href="#top">Top</a> - <a href="/">Home</a></nav></div>
<div class="mainarea" id="top">
<h1>Supplemental Notes for Bernoulli Factory Algorithms</h1>

<p><a href="mailto:poccil14@gmail.com"><strong>Peter Occil</strong></a></p>

<p><a id=Contents></a></p>

<h2>Contents</h2>

<ul>
<li><a href="#Contents"><strong>Contents</strong></a></li>
<li><a href="#About_This_Document"><strong>About This Document</strong></a></li>
<li><a href="#Definitions"><strong>Definitions</strong></a></li>
<li><a href="#General_Factory_Functions"><strong>General Factory Functions</strong></a>

<ul>
<li><a href="#Building_the_Lower_and_Upper_Polynomials"><strong>Building the Lower and Upper Polynomials</strong></a></li>
</ul></li>
<li><a href="#Approximate_Bernoulli_Factories"><strong>Approximate Bernoulli Factories</strong></a>

<ul>
<li><a href="#Approximate_Bernoulli_Factories_for_Certain_Functions"><strong>Approximate Bernoulli Factories for Certain Functions</strong></a></li>
<li><a href="#Approximate_Bernoulli_Factories_for_Power_Series"><strong>Approximate Bernoulli Factories for Power Series</strong></a></li>
<li><a href="#Approximate_Bernoulli_Factories_for_Linear_Functions"><strong>Approximate Bernoulli Factories for Linear Functions</strong></a></li>
</ul></li>
<li><a href="#Achievable_Simulation_Rates"><strong>Achievable Simulation Rates</strong></a></li>
<li><a href="#Complexity"><strong>Complexity</strong></a></li>
<li><a href="#Examples_of_Bernoulli_Factory_Polynomial_Building_Schemes"><strong>Examples of Bernoulli Factory Polynomial-Building Schemes</strong></a></li>
<li><a href="#Miscellaneous_Bernoulli_Factories"><strong>Miscellaneous Bernoulli Factories</strong></a>

<ul>
<li><a href="#Certain_Piecewise_Linear_Functions"><strong>Certain Piecewise Linear Functions</strong></a></li>
<li><a href="#Pushdown_Automata_for_Square_Root_Like_Functions"><strong>Pushdown Automata for Square-Root-Like Functions</strong></a></li>
<li><a href="#Ratio_of_Lower_Gamma_Functions_gamma__m___x__gamma__m__1"><strong>Ratio of Lower Gamma Functions (&gamma;(<em>m</em>, <em>x</em>)/&gamma;(<em>m</em>, 1)).</strong></a></li>
<li><a href="#4_3___pi"><strong>4/(3*<em>&pi;</em>)</strong></a></li>
<li><a href="#1_exp__k__1_exp__k__1"><strong>(1 + exp(<em>k</em>)) / (1 + exp(<em>k</em> + 1))</strong></a></li>
</ul></li>
<li><a href="#Notes"><strong>Notes</strong></a></li>
<li><a href="#Appendix"><strong>Appendix</strong></a>

<ul>
<li><a href="#Proofs_on_Cutting_Off_a_Power_Series"><strong>Proofs on Cutting Off a Power Series</strong></a></li>
<li><a href="#Results_Used_in_Approximate_Bernoulli_Factories"><strong>Results Used in Approximate Bernoulli Factories</strong></a></li>
<li><a href="#Failures_of_the_Consistency_Requirement"><strong>Failures of the Consistency Requirement</strong></a></li>
<li><a href="#Which_functions_admit_a_Bernoulli_factory"><strong>Which functions admit a Bernoulli factory?</strong></a></li>
<li><a href="#Which_functions_don_t_require_outside_randomness_to_simulate"><strong>Which functions don&#39;t require outside randomness to simulate?</strong></a></li>
<li><a href="#Multiple_Output_Bernoulli_Factory"><strong>Multiple-Output Bernoulli Factory</strong></a></li>
<li><a href="#Proofs_for_Polynomial_Building_Schemes"><strong>Proofs for Polynomial-Building Schemes</strong></a>

<ul>
<li><a href="#A_Conjecture_on_Polynomial_Approximation"><strong>A Conjecture on Polynomial Approximation</strong></a></li>
</ul></li>
<li><a href="#Example_of_Polynomial_Building_Scheme"><strong>Example of Polynomial-Building Scheme</strong></a></li>
<li><a href="#Pushdown_Automata_and_Algebraic_Functions"><strong>Pushdown Automata and Algebraic Functions</strong></a>

<ul>
<li><a href="#Finite_State_and_Pushdown_Generators"><strong>Finite-State and Pushdown Generators</strong></a></li>
</ul></li>
</ul></li>
<li><a href="#License"><strong>License</strong></a></li>
</ul>

<p><a id=About_This_Document></a></p>

<h2>About This Document</h2>

<p><strong>This is an open-source document; for an updated version, see the</strong> <a href="https://github.com/peteroupc/peteroupc.github.io/raw/master/bernsupp.md"><strong>source code</strong></a> <strong>or its</strong> <a href="https://github.com/peteroupc/peteroupc.github.io/blob/master/bernsupp.md"><strong>rendering on GitHub</strong></a><strong>.  You can send comments on this document on the</strong> <a href="https://github.com/peteroupc/peteroupc.github.io/issues"><strong>GitHub issues page</strong></a><strong>.  See</strong> &quot;<a href="https://peteroupc.github.io/bernreq.html"><strong>Open Questions on the Bernoulli Factory Problem</strong></a>&quot; <strong>for a list of things about this document that I seek answers to.</strong></p>

<p>My audience for this article is <strong>computer programmers with mathematics knowledge, but little or no familiarity with calculus</strong>.  It should be read in conjunction with the article &quot;<a href="https://peteroupc.github.io/bernoulli.html"><strong>Bernoulli Factory Algorithms</strong></a>&quot;.</p>

<p>I encourage readers to implement any of the algorithms given in this page, and report their implementation experiences.  In particular, <a href="https://github.com/peteroupc/peteroupc.github.io/issues/18"><strong>I seek comments on the following aspects</strong></a>:</p>

<ul>
<li>Are the algorithms in this article (in conjunction with &quot;Bernoulli Factory Algorithms&quot;) easy to implement? Is each algorithm written so that someone could write code for that algorithm after reading the article?</li>
<li>Does this article have errors that should be corrected?</li>
<li>Are there ways to make this article more useful to the target audience?</li>
</ul>

<p>Comments on other aspects of this document are welcome.</p>

<p><a id=Definitions></a></p>

<h2>Definitions</h2>

<p>This section describes certain math terms used on this page for programmers to understand.</p>

<p>The <em>closed unit interval</em> (written as [0, 1]) means the set consisting of 0, 1, and every real number in between.</p>

<p>The following terms can describe a function $f(x)$, specifically how &quot;well-behaved&quot; $f$ is &mdash; which can be important when designing Bernoulli factory algorithms.  This page mostly cares how $f$ behaves when its domain is the closed unit interval, that is, when $0 \le x \le 1$.</p>

<ul>
<li>If $f$ is continuous, its <em>derivative</em> is, roughly speaking, its &quot;slope&quot; or &quot;velocity&quot; or &quot;instantaneous-rate-of-change&quot; function.  The derivative (or <em>first derivative</em>) is denoted as $f&#39;$.  The <em>second derivative</em> (&quot;slope-of-slope&quot;) of $f$, denoted $f&#39;&#39;$, is the derivative of $f&#39;$; the <em>third derivative</em> is the derivative of $f&#39;&#39;$; and so on.</li>
<li>A <a href="https://en.wikipedia.org/wiki/H%C3%B6lder_condition"><strong><em>Hölder continuous</em></strong></a> function  (with <em>M</em> being the <em>Hölder constant</em> and <em>&alpha;</em> being the <em>Hölder exponent</em>) is a continuous function <em>f</em> such that <em>f</em>(<em>x</em>) and <em>f</em>(<em>y</em>) are no more than <em>M</em>*<em>&delta;</em><sup><em>&alpha;</em></sup> apart whenever <em>x</em> and <em>y</em> are in the function&#39;s domain and no more than <em>&delta;</em> apart.<br>Roughly speaking, the function&#39;s &quot;steepness&quot; is no greater than that of <em>M</em>*<em>x</em><sup><em>&alpha;</em></sup>.</li>
<li>A <em>Lipschitz continuous</em> function with constant <em>L</em> (the <em>Lipschitz constant</em>) is Hölder continuous with Hölder exponent 1 and Hölder constant <em>L</em>.<br>Roughly speaking, the function&#39;s &quot;steepness&quot; is no greater than that of <em>L</em>*<em>x</em>.<br>If the function has a derivative on its domain, <em>L</em> can be the maximum absolute value of that derivative.</li>
<li>A <em>convex</em> function $f$ has the property that $f((x+y)/2) \le (f(x)+f(y))/2$ whenever $x$, $y$, and $(x+y)/2$ are in the function&#39;s domain.<br>Roughly speaking, if the function&#39;s &quot;slope&quot; never goes down, then it&#39;s convex.</li>
<li>A <em>concave</em> function $f$ has the property that $f((x+y)/2) \ge (f(x)+f(y))/2$ whenever $x$, $y$, and $(x+y)/2$ are in the function&#39;s domain.<br>Roughly speaking, if the function&#39;s &quot;slope&quot; never goes up, then it&#39;s concave.</li>
</ul>

<p><a id=General_Factory_Functions></a></p>

<h2>General Factory Functions</h2>

<p>As a reminder, the <em>Bernoulli factory problem</em> is: We&#39;re given a coin that shows heads with an unknown probability, <em>&lambda;</em>, and the goal is to use that coin (and possibly also a fair coin) to build a &quot;new&quot; coin that shows heads with a probability that depends on <em>&lambda;</em>, call it <em>f</em>(<em>&lambda;</em>).</p>

<p>The algorithm for <a href="https://peteroupc.github.io/bernoulli.html#General_Factory_Functions"><strong>general factory functions</strong></a>, described in my main article on Bernoulli factory algorithms, works by building randomized upper and lower bounds for a function <em>f</em>(<em>&lambda;</em>), based on flips of the input coin.  Roughly speaking, the algorithm works as follows:</p>

<ol>
<li>Generate a random variate, <em>U</em>, uniformly distributed, greater than 0 and less than 1.</li>
<li>Flip the input coin, then build an upper and lower bound for <em>f</em>(<em>&lambda;</em>), based on the outcomes of the flips so far.</li>
<li>If <em>U</em> is less than or equal to the lower bound, return 1. If <em>U</em> is greater than the upper bound, return 0.  Otherwise, go to step 2.</li>
</ol>

<p>These randomized upper and lower bounds come from two sequences of polynomials as follows:</p>

<ol>
<li>One sequence approaches the function <em>f</em>(<em>&lambda;</em>) from above, the other from below, and both sequences must converge to <em>f</em>.</li>
<li>For each sequence, the first polynomial has degree 1 (so is a linear function), and each other polynomial&#39;s degree is 1 higher than the previous.</li>
<li><p>The <em>consistency requirement</em> must be met: The difference&mdash;</p>

<ul>
<li>between the degree-(<em>n</em>&minus;1) upper polynomial and the degree-<em>n</em> upper polynomial, and</li>
<li>between the degree-<em>n</em> lower polynomial and the degree-(<em>n</em>&minus;1) lower polynomial,</li>
</ul>

<p>must have nonnegative coefficients, once the polynomials are rewritten in Bernstein form and elevated to degree <em>n</em>.</p></li>
</ol>

<p>The consistency requirement ensures that the upper polynomials &quot;decrease&quot; and the lower polynomials &quot;increase&quot;.  Unfortunately, the reverse is not true in general; even if the upper polynomials &quot;decrease&quot; and the lower polynomials &quot;increase&quot; to <em>f</em>, this does not ensure the consistency requirement by itself.  Examples of this fact are shown in the section &quot;<a href="#Failures_of_the_Consistency_Requirement"><strong>Failures of the Consistency Requirement</strong></a>&quot; in the appendix.</p>

<p>In this document, <strong>fbelow</strong>(<em>n</em>, <em>k</em>) and <strong>fabove</strong>(<em>n</em>, <em>k</em>) mean the <em>k</em><sup>th</sup> coefficient for the lower or upper degree-<em>n</em> polynomial in Bernstein form, respectively, where 0 &le; <em>k</em> &le; <em>n</em> is an integer.</p>

<p><a id=Building_the_Lower_and_Upper_Polynomials></a></p>

<h3>Building the Lower and Upper Polynomials</h3>

<p>A <em>factory function</em> <em>f</em>(<em>&lambda;</em>) is a function for which the Bernoulli factory problem can be solved (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#About_Bernoulli_Factories"><strong>About Bernoulli Factories</strong></a>&quot;). The following are ways to build sequences of polynomials that appropriately converge to a factory function if that function meets certain conditions.  It would be helpful to plot that factory function using a computer algebra system to see if it belongs to any of the classes of functions described below.</p>

<p><strong>Concave functions.</strong> If <em>f</em> is concave, then <strong>fbelow</strong>(<em>n</em>, <em>k</em>) can equal <em>f</em>(<em>k</em>/<em>n</em>), thanks to Jensen&#39;s inequality. One example is <em>f</em>(<em>&lambda;</em>) = 1&minus; <em>&lambda;</em><sup>2</sup>.</p>

<p><strong>Convex functions.</strong> If <em>f</em> is convex, then <strong>fabove</strong>(<em>n</em>, <em>k</em>) can equal <em>f</em>(<em>k</em>/<em>n</em>), thanks to Jensen&#39;s inequality.  One example is <em>f</em>(<em>&lambda;</em>) = exp(&minus;<em>&lambda;</em>/4).</p>

<p><strong>Hölder and Lipschitz continuous functions.</strong> I have found a way to extend the results of Nacu and Peres (2005)[^1] to certain functions with a slope that tends to a vertical slope.  The following scheme, proved in the appendix, implements <strong>fabove</strong> and <strong>fbelow</strong> if <em>f</em>(<em>&lambda;</em>)&mdash;</p>

<ul>
<li>is <a href="https://en.wikipedia.org/wiki/H%C3%B6lder_condition"><strong><em>Hölder continuous</em></strong></a> on the closed unit interval, with Hölder constant <em>m</em> and Hölder exponent <em>&alpha;</em> (see &quot;<a href="#Definitions"><strong>Definitions</strong></a>&quot;), and</li>
<li>on the closed unit interval&mdash;

<ul>
<li>has a minimum of greater than 0 and a maximum of less than 1, or</li>
<li>is convex and has a minimum of greater than 0, or</li>
<li>is concave and has a maximum of less than 1.</li>
</ul></li>
</ul>

<p>Finding <em>m</em> and <em>&alpha;</em> is non-trivial in general.  But assuming <em>m</em> and <em>&alpha;</em> are known, then for every integer <em>n</em> that&#39;s a power of 2:</p>

<ul>
<li><em>D</em>(<em>n</em>) = <em>m</em>*(2/7)<sup><em>&alpha;</em>/2</sup>/((2<sup><em>&alpha;</em>/2</sup>&minus;1)*<em>n</em><sup><em>&alpha;</em>/2</sup>).</li>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) if <em>f</em> is concave; otherwise, min(<strong>fbelow</strong>(4,0), <strong>fbelow</strong>(4,1), ..., <strong>fbelow</strong>(4,4)) if <em>n</em> &lt; 4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; <em>D</em>(<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) if <em>f</em> is convex; otherwise, max(<strong>fabove</strong>(4,0), <strong>fabove</strong>(4,1), ..., <strong>fabove</strong>(4,4)) if <em>n</em> &lt; 4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + <em>D</em>(<em>n</em>).</li>
</ul>

<blockquote>
<p><strong>Note:</strong></p>

<ol>
<li>Some factory functions are not Hölder continuous for any Hölder exponent greater than 0.  These functions have a slope that&#39;s steeper than every &quot;nth&quot; root, and can&#39;t be handled by this method.  One example is <em>f</em>(<em>&lambda;</em>) = 1/10 if <em>&lambda;</em> is 0 and &minus;1/(2*ln(<em>&lambda;</em>/2)) + 1/10 otherwise, which has a slope near 0 that&#39;s steeper than every &quot;nth&quot; root.</li>
<li>If the factory function has a Hölder exponent of 1 (and so is Lipschitz continuous), <em>D</em>(<em>n</em>) can be <em>m</em>*322613/(250000*sqrt(<em>n</em>)), which is an upper bound.</li>
<li>If the factory function&#39;s Hölder exponent is 1/2 or greater, <em>D</em>(<em>n</em>) can be <em>m</em>*154563/(40000*<em>n</em><sup>1/4</sup>), which is an upper bound.</li>
</ol>
</blockquote>

<p><strong>Functions with a Lipschitz continuous derivative.</strong> The following method, proved in the appendix, implements <strong>fabove</strong> and <strong>fbelow</strong> if <em>f</em>(<em>&lambda;</em>)&mdash;</p>

<ul>
<li>has a Lipschitz continuous derivative (see &quot;<a href="#Definitions"><strong>Definitions</strong></a>&quot;), and</li>
<li>in the closed unit interval&mdash;

<ul>
<li>has a minimum of greater than 0 and a maximum of less than 1, or</li>
<li>is convex and has a minimum of greater than 0, or</li>
<li>is concave and has a maximum of less than 1.</li>
</ul></li>
</ul>

<p>Let <em>m</em> be the Lipschitz constant of <em>f</em>&#39;s derivative, or a greater number than that constant (if <em>f</em> has a second derivative on its domain, then <em>m</em> can be the maximum absolute value of that second derivative).  Then for every integer <em>n</em> that&#39;s a power of 2:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) if <em>f</em> is concave; otherwise, min(<strong>fbelow</strong>(4,0), <strong>fbelow</strong>(4,1), ..., <strong>fbelow</strong>(4,4)) if <em>n</em> &lt; 4; otherwise,  <em>f</em>(<em>k</em>/<em>n</em>) &minus; <em>m</em>/(7*<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) if <em>f</em> is convex; otherwise, max(<strong>fabove</strong>(4,0), <strong>fabove</strong>(4,1), ..., <strong>fabove</strong>(4,4)) if <em>n</em> &lt; 4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + <em>m</em>/(7*<em>n</em>).</li>
</ul>

<p>My <a href="https://github.com/peteroupc/peteroupc.github.io/blob/master/approxscheme.py"><strong>GitHub repository</strong></a> includes SymPy code for a method, <code>c2params</code>, to calculate the necessary values for <em>m</em> and the bounds of these polynomials, given a factory function.</p>

<blockquote>
<p><strong>Examples:</strong></p>

<ol>
<li><p>Take <em>f</em>(<em>&lambda;</em>) = exp(&minus;<em>&lambda;</em>).  This is a convex function, and its derivative is Lipschitz continuous with Lipschitz constant 1.  Then it can be shown that the following scheme for <em>f</em> is valid (the value 3321/10000 is slightly less than <em>M</em> &minus; 1/(7*4), where <em>M</em> is the minimum of <em>f</em> on its domain):</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 3321/10000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1/(7*n). (Observe that <em>f</em>(<em>k</em>/4) &minus; 1/(7*4) &ge; 3321/10000.)</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) (because <em>f</em> is convex).</li>
</ul></li>
<li><p>Take <em>f</em>(<em>&lambda;</em>) = <em>&lambda;</em>/2 if <em>&lambda;</em> &le; 1/2; (4*<em>&lambda;</em> &minus; 1)/(8*<em>&lambda;</em>) otherwise.  This function is concave, and its derivative is Lipschitz continuous with Lipschitz constant 2.  Then it can be shown that the following scheme for <em>f</em> is valid (the value 893/2000 is slightly greater than <em>M</em> + 2/(7*4), where <em>M</em> is the maximum of <em>f</em> on its domain):</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 893/2000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 2/(7*<em>n</em>).</li>
</ul></li>
</ol>
</blockquote>

<p><strong>Certain functions that equal 0 at 0.</strong> This approach involves transforming the function <em>f</em> so that it no longer equals 0 at the point 0.  This can be done by dividing <em>f</em> by a function (<code>High</code>(<em>&lambda;</em>)) that &quot;dominates&quot; <em>f</em> everywhere on the closed unit interval.  Unlike for the original function, there might be a polynomial-building scheme described earlier in this section for the transformed function.</p>

<p>More specifically, <code>High</code>(<em>&lambda;</em>) must meet the following requirements:</p>

<ul>
<li><code>High</code>(<em>&lambda;</em>) is continuous on the closed unit interval.</li>
<li><code>High</code>(0) = 0. (This is required to ensure correctness in case <em>&lambda;</em> is 0.)</li>
<li>1 &ge; <code>High</code>(1) &ge; <em>f</em>(1) &ge; 0.</li>
<li>1 &gt; <code>High</code>(<em>&lambda;</em>) &gt; <em>f</em>(<em>&lambda;</em>) &gt; 0 whenever 0 &lt; <em>&lambda;</em> &lt; 1.</li>
<li>If <em>f</em>(1) = 0, then <code>High</code>(1) = 0. (This is required to ensure correctness in case <em>&lambda;</em> is 1.)</li>
</ul>

<p>Also, <code>High</code> should be a function with a simple Bernoulli factory algorithm.  For example, <code>High</code> can be a polynomial in Bernstein form of degree <em>n</em> whose <em>n</em> plus one coefficients are [0, 1, 1, ..., 1].  This polynomial is easy to simulate using the algorithms from the section &quot;<a href="https://peteroupc.github.io/bernoulli.html#Certain_Polynomials"><strong>Certain Polynomials</strong></a>&quot;.[^2]</p>

<p>The algorithm is now described.</p>

<p>Let <em>g</em>(<em>&lambda;</em>) = lim<sub><em>&nu;</em>&rarr;<em>&lambda;</em></sub> <em>f</em>(<em>&nu;</em>)/<code>High</code>(<em>&nu;</em>) (roughly speaking, the value that <em>f</em>(<em>&nu;</em>)/<code>High</code>(<em>&nu;</em>) approaches as <em>&nu;</em> approaches <em>&lambda;</em>.) If&mdash;</p>

<ul>
<li><em>f</em>(0) = 0 and <em>f</em>(1) &lt; 1, and</li>
<li><em>g</em>(<em>&lambda;</em>) is continuous on the closed unit interval and belongs in one of the classes of functions given earlier,</li>
</ul>

<p>then <em>f</em> can be simulated using the following algorithm:</p>

<ol>
<li>Run a Bernoulli factory algorithm for <code>High</code>.  If the call returns 0, return 0. (For example, if <code>High</code>(<em>&lambda;</em>) = <em>&lambda;</em>, then this step amounts to the following: &quot;Flip the input coin.  If it returns 0, return 0.&quot;)</li>
<li>Run a Bernoulli factory algorithm for <em>g</em>(.) and return the result of that algorithm.  This can be one of the <a href="https://peteroupc.github.io/bernoulli.html#General_Factory_Functions"><strong>general factory function algorithms</strong></a> if there is a way to calculate polynomials that converge to <em>g</em>(.) in a manner needed for that algorithm (for example, if <em>g</em> is described earlier in this section).</li>
</ol>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>It may happen that <em>g</em>(0) = 0.  In this case, step 2 of this algorithm can involve running this algorithm again, but with new <em>g</em> and <code>High</code> functions that are found based on the current <em>g</em> function.  See the second example below.</li>
<li><code>High</code>(<em>&lambda;</em>) can also equal 1 instead of be described in this section.  That leads to the original Bernoulli factory algorithm for <em>f</em>(<em>&lambda;</em>).</li>
</ol>

<p><strong>Examples:</strong></p>

<ol>
<li>If <em>f</em>(<em>&lambda;</em>) = (sinh(<em>&lambda;</em>)+cosh(<em>&lambda;</em>)&minus;1)/4, then <em>f</em> is less than or equal to <code>High</code>(<em>&lambda;</em>) = <em>&lambda;</em>, so <em>g</em>(<em>&lambda;</em>) is 1/4 if <em>&lambda;</em> = 0, and (exp(<em>&lambda;</em>) &minus; 1)/(4*<em>&lambda;</em>) otherwise.  The following code in Python that uses the SymPy computer algebra library computes this example: <code>fx = (sinh(x)+cosh(x)-1)/4; h = x; pprint(Piecewise((limit(fx/h,x,0), Eq(x,0)), ((fx/h).simplify(), True)))</code>.</li>
<li><p>If <em>f</em>(<em>&lambda;</em>) = cosh(<em>&lambda;</em>) &minus; 1, then <em>f</em> is less than or equal to <code>High</code>(<em>&lambda;</em>) = <em>&lambda;</em>, so <em>g</em>(<em>&lambda;</em>) is 0 if <em>&lambda;</em> = 0, and (cosh(<em>&lambda;</em>)&minus;1)/<em>&lambda;</em> otherwise.  Now, since <em>g</em>(0) = 0, find new functions <em>g</em> and <code>High</code> based on the current <em>g</em>.  The current <em>g</em> is less than or equal to <code>High</code>(<em>&lambda;</em>) = <em>&lambda;</em>*3*(2&minus;<em>&lambda;</em>)/5 (a degree-2 polynomial that in Bernstein form has coefficients [0, 6/10, 6/10]), so <em>G</em>(<em>&lambda;</em>) = 5/12 if <em>&lambda;</em> = 0, and &minus;(5*cosh(<em>&lambda;</em>) &minus; 5)/(3*<em>&lambda;</em><sup>2</sup>*(<em>&lambda;</em>&minus;2)) otherwise. <em>G</em> is bounded away from 0 and 1, resulting in the following algorithm:</p>

<ol>
<li>(Simulate <code>High</code>.) Flip the input coin.  If it returns 0, return 0.</li>
<li>(Simulate <code>High</code>.) Flip the input coin twice.  If both flips return 0, return 0.  Otherwise, with probability 4/10 (that is, 1 minus 6/10), return 0.</li>
<li>Run a Bernoulli factory algorithm for <em>G</em> (which might involve building polynomials that converge to <em>G</em>, noticing that <em>G</em>&#39;s derivative is Lipschitz continuous) and return the result of that algorithm.</li>
</ol></li>
</ol>
</blockquote>

<p><strong>Certain functions that equal 0 at 0 and 1 at 1.</strong>  Let <em>f</em>, <em>g</em>, and <code>High</code> be functions as defined earlier, except that <em>f</em>(0) = 0 and <em>f</em>(1) = 1.  Define the following additional functions:</p>

<ul>
<li><code>Low</code>(<em>&lambda;</em>) is a function that meets the following requirements:

<ul>
<li><code>Low</code>(<em>&lambda;</em>) is continuous on the closed unit interval.</li>
<li><code>Low</code>(0) = 0 and <code>Low</code>(1) = 1.</li>
<li>1 &gt; <em>f</em>(<em>&lambda;</em>) &gt; <code>Low</code>(<em>&lambda;</em>) &gt; 0 whenever 0 &lt; <em>&lambda;</em> &lt; 1.</li>
</ul></li>
<li><em>q</em>(<em>&lambda;</em>) = lim<sub><em>&nu;</em>&rarr;<em>&lambda;</em></sub> <code>Low</code>(<em>&nu;</em>)/<code>High</code>(<em>&nu;</em>).</li>
<li><em>r</em>(<em>&lambda;</em>) = lim<sub><em>&nu;</em>&rarr;<em>&lambda;</em></sub> (1&minus;<em>g</em>(<em>&nu;</em>))/(1&minus;<em>q</em>(<em>&nu;</em>)).</li>
</ul>

<p>Roughly speaking, <code>Low</code> is a function that bounds <em>f</em> from below, just as <code>High</code> bounds <em>f</em> from above. <code>Low</code> should be a function with a simple Bernoulli factory algorithm, such as a polynomial in Bernstein form.  If both <code>Low</code> and <code>High</code> are polynomials of the same degree, <em>q</em> will be a ratio of polynomials with a relatively simple Bernoulli factory algorithm (see &quot;<a href="https://peteroupc.github.io/bernoulli.html#Certain_Rational_Functions"><strong>Certain Rational Functions</strong></a>&quot;).</p>

<p>Now, if <em>r</em>(<em>&lambda;</em>) is continuous on the closed unit interval, then <em>f</em> can be simulated using the following algorithm:</p>

<ol>
<li>Run a Bernoulli factory algorithm for <code>High</code>.  If the call returns 0, return 0. (For example, if <code>High</code>(<em>&lambda;</em>) = <em>&lambda;</em>, then this step amounts to the following: &quot;Flip the input coin.  If it returns 0, return 0.&quot;)</li>
<li>Run a Bernoulli factory algorithm for <em>q</em>(.).  If the call returns 1, return 1.</li>
<li>Run a Bernoulli factory algorithm for <em>r</em>(.), and return 1 minus the result of that call.  The Bernoulli factory algorithm can be one of the <a href="https://peteroupc.github.io/bernoulli.html#General_Factory_Functions"><strong>general factory function algorithms</strong></a> if there is a way to calculate polynomials that converge to <em>r</em>(.) in a manner needed for that algorithm (for example, if <em>r</em> is described earlier in this section).</li>
</ol>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>Quick proof: Rewrite $f=\text{High}\cdot(q\cdot1+(1-q)\cdot(1-r))+(1-\text{High})\cdot0$.</li>
<li><code>High</code>(<em>&lambda;</em>) is allowed to equal 1 if the <em>r</em>(.) in step 3 is allowed to equal 0 at 0.</li>
</ol>

<p><strong>Example:</strong> If <em>f</em>(<em>&lambda;</em>) = (1&minus;exp(<em>&lambda;</em>))/(1&minus;exp(1)), then <em>f</em> is less than or equal to <code>High</code>(<em>&lambda;</em>) = <em>&lambda;</em>, and greater than or equal to <code>Low</code>(<em>&lambda;</em>) = <em>&lambda;</em><sup>2</sup>.  As a result, <em>q</em>(<em>&lambda;</em>) = <em>&lambda;</em>, and <em>r</em>(<em>&lambda;</em>) = (2 &minus; exp(1))/(1 &minus; exp(1)) if <em>&lambda;</em> = 0; 1/(exp(1)&minus;1) if <em>&lambda;</em> = 1; and (&minus;<em>&lambda;</em>*(1 &minus; exp(1)) &minus; exp(<em>&lambda;</em>) + 1)/(<em>&lambda;</em>*(1 &minus; exp(1))*(<em>&lambda;</em> &minus; 1)) otherwise.  This can be computed using the following code in Python that uses the SymPy computer algebra library: <code>fx=(1-exp(x))/(1-exp(1)); high=x; low=x**2; q=(low/high); r=(1-fx/high)/(1-q); r=Piecewise((limit(r, x, 0), Eq(x,0)), (limit(r,x,1),Eq(x,1)), (r,True)).simplify(); pprint(r)</code>.</p>
</blockquote>

<p><strong>Other functions that equal 0 or 1 at the endpoints 0 and/or 1.</strong> If <em>f</em> does not fully admit a polynomial-building scheme under the convex, concave, Lipschitz derivative, and Hölder classes:</p>

<table><thead>
<tr>
<th>If <em>f</em>(0) =</th>
<th>And <em>f</em>(1) =</th>
<th>Method</th>
</tr>
</thead><tbody>
<tr>
<td>&gt; 0 and &lt; 1</td>
<td>1</td>
<td>Use the algorithm for <strong>certain functions that equal 0 at 0</strong>, but with <em>f</em>(<em>&lambda;</em>) = 1 &minus; <em>f</em>(1&minus;<em>&lambda;</em>).<br/><em>Inverted coin</em>: Instead of the usual input coin, use a coin that does the following: &quot;Flip the input coin and return 1 minus the result.&quot;<br/><em>Inverted result:</em> If the overall algorithm would return 0, it returns 1 instead, and vice versa.</td>
</tr>
<tr>
<td>&gt; 0 and &lt; 1</td>
<td>0</td>
<td>Algorithm for <strong>certain functions that equal 0 at 0</strong>, but with <em>f</em>(<em>&lambda;</em>) = <em>f</em>(1&minus;<em>&lambda;</em>).  (For example, cosh(<em>&lambda;</em>)&minus;1 becomes cosh(1&minus;<em>&lambda;</em>)&minus;1.)<br/>Inverted coin.</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>Algorithm for <strong>certain functions that equal 0 at 0 and 1 at 1</strong>, but with <em>f</em>(<em>&lambda;</em>) = 1&minus;<em>f</em>(<em>&lambda;</em>).<br/>Inverted result.</td>
</tr>
<tr>
<td>1</td>
<td>&gt; 0 and &le; 1</td>
<td>Algorithm for <strong>certain functions that equal 0 at 0</strong>, but with <em>f</em>(<em>&lambda;</em>) = 1&minus;<em>f</em>(<em>&lambda;</em>).<br/>Inverted result.</td>
</tr>
</tbody></table>

<p><strong>Specific functions.</strong> My <a href="https://github.com/peteroupc/peteroupc.github.io/blob/master/approxscheme.py"><strong>GitHub repository</strong></a> includes SymPy code for a method, <code>approxscheme2</code>, to build a polynomial-building scheme for certain factory functions.</p>

<p><a id=Approximate_Bernoulli_Factories></a></p>

<h2>Approximate Bernoulli Factories</h2>

<p>An <strong>approximate Bernoulli factory</strong> for a function <em>f</em>(<em>&lambda;</em>) is a Bernoulli factory algorithm that simulates another function, <em>g</em>(<em>&lambda;</em>), that approximates <em>f</em> in some sense.</p>

<p>Usually <em>g</em> is a polynomial, but can also be a rational function (ratio of polynomials) or another function with an easy-to-implement Bernoulli factory algorithm.</p>

<p>Meanwhile, <em>f</em>(<em>&lambda;</em>) can be any function that maps the closed unit interval to itself, even if it isn&#39;t continuous or a factory function (examples include the &quot;step function&quot; 0 if <em>&lambda;</em> &lt; 1/2 and 1 otherwise, or the function 2*min(<em>&lambda;</em>, 1 &minus; <em>&lambda;</em>)).  If the function is continuous, it can be approximated arbitrarily well by an approximate Bernoulli factory (as a result of the so-called &quot;Weierstrass approximation theorem&quot;), but generally not if the function is discontinuous.</p>

<p>To build an approximate Bernoulli factory with a polynomial:</p>

<ol>
<li><p>First, find a polynomial in Bernstein form of degree <em>n</em> that is close to the desired function <em>f</em>(<em>&lambda;</em>).</p>

<p>The simplest choice for this polynomial, known simply as a <em>Bernstein polynomial</em>, has <em>n</em>+1 coefficients and its <em>j</em><sup>th</sup> coefficient (starting at 0) is found as <em>f</em>(<em>j</em>/<em>n</em>).  For this choice, if <em>f</em> is continuous, the polynomial can be brought arbitrarily close to <em>f</em> by choosing <em>n</em> high enough.</p>

<p>Whatever polynomial is used, the polynomial&#39;s coefficients must all lie in [0, 1].</p></li>
<li><p>Then, use one of the algorithms in the section &quot;<a href="https://peteroupc.github.io/bernoulli.html"><strong>Certain Polynomials</strong></a>&quot; to toss heads with probability equal to that polynomial, given its coefficients.</p></li>
</ol>

<blockquote>
<p><strong>Note:</strong> Bias and variance are the two sources of error in a randomized estimation algorithm.  Let <em>g</em>(<em>&lambda;</em>) be an approximation of <em>f</em>(<em>&lambda;</em>). The original Bernoulli factory for <em>f</em>, if it exists, has bias 0 and variance <em>f</em>(<em>&lambda;</em>)*(1&minus;<em>f</em>(<em>&lambda;</em>)), but the approximate Bernoulli factory has bias <em>g</em>(<em>&lambda;</em>) &minus; <em>f</em>(<em>&lambda;</em>) and variance <em>g</em>(<em>&lambda;</em>)*(1&minus;<em>g</em>(<em>&lambda;</em>)). (&quot;Variance reduction&quot; methods are outside the scope of this document.)  An estimation algorithm&#39;s <em>mean squared error</em> equals variance plus square of bias.</p>
</blockquote>

<p><a id=Approximate_Bernoulli_Factories_for_Certain_Functions></a></p>

<h3>Approximate Bernoulli Factories for Certain Functions</h3>

<p>This section first discusses approximating $f$ with a <em>Bernstein polynomial</em> (a degree-$n$ polynomial in Bernstein form with coefficients $f(k/n)$ with $0\le k\le n$).  The advantage is only one Bernstein coefficient has to be found per run; the disadvantage is that Bernstein polynomials approach $f$ slowly in general, in the order of $1/n$ (Voronovskaya 1932)[^3].</p>

<p>There are results that give an upper bound on the error on approximating <em>f</em> with a degree-<em>n</em> Bernstein polynomial.  To find a degree <em>n</em> such that <em>f</em> is approximated with a maximum error of <em>&epsilon;</em>, solve the error bound&#39;s equation for <em>n</em>, then take <em>n</em> = ceil(<em>n</em>) to get the solution if it&#39;s an integer, or the nearest integer that&#39;s bigger than the solution.</p>

<p>For example:</p>

<table><thead>
<tr>
<th>If <em>f</em>(<em>&lambda;</em>):</th>
<th>Then the degree-<em>n</em> Bernstein polynomial is close to $f$ with the following error bound:</th>
<th>Where <em>n</em> is:</th>
<th>Notes</th>
</tr>
</thead><tbody>
<tr>
<td>Has Lipschitz continuous derivative (see &quot;Definitions&quot;).</td>
<td><em>&epsilon;</em> = <em>M</em>/(8*<em>n</em>).</td>
<td><em>n</em> = ceil(<em>M</em>/(8*<em>&epsilon;</em>)).</td>
<td>Lorentz (1966)[^4]. <em>M</em> is the derivative&#39;s Lipschitz constant.</td>
</tr>
<tr>
<td>Hölder continuous with constant <em>M</em> and exponent <em>&alpha;</em>.</td>
<td><em>&epsilon;</em> = <em>M</em>*(1/(4*<em>n</em>))<sup><em>&alpha;</em>/2</sup>.</td>
<td><em>n</em> = ceil(1/(4<sup><em>&alpha;</em></sup>*<em>&epsilon;</em><sup>2</sup>/<em>M</em><sup>2</sup>)<sup>1/<em>&alpha;</em></sup>).</td>
<td>Mathé (1999)[^5]. 0 &lt; <em>&alpha;</em> &le; 1.</td>
</tr>
<tr>
<td>Lipschitz continuous with constant <em>L</em>.</td>
<td><em>&epsilon;</em> = <em>L</em>*sqrt(1/(4*<em>n</em>)).</td>
<td><em>n</em> = ceil(<em>L</em><sup>2</sup>/(4*<em>&epsilon;</em><sup>2</sup>)).</td>
<td>Special case of previous entry.</td>
</tr>
</tbody></table>

<p>Now, if <em>f</em> belongs to any of the classes given above, the following algorithm (adapted from &quot;Certain Polynomials&quot;) simulates a polynomial that approximates <em>f</em> with a maximum error of <em>&epsilon;</em>:</p>

<ol>
<li>Calculate <em>n</em> as described in the table above for the given class.</li>
<li>Flip the input coin <em>n</em> times, and let <em>j</em> be the number of times the coin returned 1 this way.</li>
<li>With probability <em>f</em>(<em>j</em>/<em>n</em>), return 1.  Otherwise, return 0. (If <em>f</em>(<em>j</em>/<em>n</em>) can be an irrational number, see &quot;<a href="https://peteroupc.github.io/bernoulli.html#Algorithms_for_General_Irrational_Constants"><strong>Algorithms for General Irrational Constants</strong></a>&quot; for ways to sample this irrational probability exactly.)</li>
</ol>

<hr>

<p>Alternatively, polynomials other than Bernstein polynomials, but written in Bernstein form, can be used to approximate $f$ with an error no more than $\epsilon$, as long as an explicit upper bound on the approximation error is available.  A ratio of two such polynomials can also approximate $f$ this way.  See my <a href="https://mathoverflow.net/questions/424272"><strong>question on MathOverflow</strong></a>.</p>

<p>An example is given by the iterated Bernstein polynomial construction discussed in Micchelli (1973)[^6] and Guan (2009)[^7]. Let $B_n(f(\lambda))$ be the ordinary Bernstein polynomial for $f(\lambda)$.  Then&mdash;</p>

<ul>
<li>the order-2 iterated Bernstein polynomial of degree $n$ is $U_{n,2} = B_n(W_{n,2})$, where $W_{n,2} = 2 f(\lambda) - B_n(f(\lambda))$, and</li>
<li>the order-3 iterated Bernstein polynomial of degree $n$ is $U_{n,3} = B_n(W_{n,3})$, where $W_{n,3} = B_n(B_n(f(\lambda))) + 3 (f(\lambda) - B_n(f(\lambda)))$</li>
</ul>

<p>(Güntürk and Li 2021, sec. 3.3)[^8]. The goal is now to find a degree $n$ such that&mdash;</p>

<ol>
<li>the iterated polynomial is within $\epsilon$ of $f(\lambda)$, and</li>
<li>the polynomial $W_{n,i}$ is not less than 0 or greater than 1.</li>
</ol>

<p>&nbsp;</p>

<p>By analyzing the proof of Theorem 3.3 of the paper just cited, the following error bounds <em>appear</em> to be true.  In the table below, <em>M</em><sub><em>n</em></sub> is not less than the so-called $C^n$ norm.  Unfortunately, the $C^n$ norm is defined differently in different academic works, and the bounds are sensitive to how that norm is defined.[^9]</p>

<table><thead>
<tr>
<th>If <em>f</em>(<em>&lambda;</em>):</th>
<th>Then the following polynomial:</th>
<th>Is close to <em>f</em> with the following error bound:</th>
<th>Where <em>n</em> is:</th>
</tr>
</thead><tbody>
<tr>
<td>Has continuous third derivative.</td>
<td>$U_{n,2}$</td>
<td><em>&epsilon;</em> = 0.3489*<em>M</em><sub>3</sub>/<em>n</em><sup>3/2</sup>.</td>
<td><em>n</em>=ceil((0.3489)<sup>2/3</sup>*(<em>M</em><sub>4</sub>/<em>&epsilon;</em>)<sup>2/3</sup>) &lt; ceil((49561/100000)*(<em>M</em>/<em>&epsilon;</em>)<sup>2/3</sup>).</td>
</tr>
<tr>
<td>Has continuous fourth derivative.</td>
<td>$U_{n,2}$</td>
<td><em>&epsilon;</em> = 0.275*<em>M</em><sub>4</sub>/<em>n</em><sup>2</sup>.</td>
<td><em>n</em>=ceil(sqrt(0.275)*sqrt(<em>M</em><sub>4</sub>/<em>&epsilon;</em>)) &lt; ceil((52441/100000)*sqrt(<em>M</em>/<em>&epsilon;</em>)).</td>
</tr>
<tr>
<td>Has continuous fifth derivative.</td>
<td>$U_{n,3}$</td>
<td><em>&epsilon;</em> = 0.7284*<em>M</em><sub>5</sub>/<em>n</em><sup>5/2</sup>.</td>
<td><em>n</em>=ceil((0.7284)<sup>2/5</sup>*(<em>M</em><sub>5</sub>/<em>&epsilon;</em>)<sup>2/5</sup>) &lt; ceil((88095/100000)*(<em>M</em>/<em>&epsilon;</em>)<sup>2/5</sup>).</td>
</tr>
<tr>
<td>Has continuous sixth derivative.</td>
<td>$U_{n,3}$</td>
<td><em>&epsilon;</em> = 0.9961*<em>M</em><sub>6</sub>/<em>n</em><sup>3</sup>.</td>
<td><em>n</em>=ceil((0.9961)<sup>1/3</sup>*(<em>M</em><sub>6</sub>/<em>&epsilon;</em>)<sup>1/3</sup>) &lt; ceil((99870/100000)*(<em>M</em>/<em>&epsilon;</em>)<sup>1/3</sup>).</td>
</tr>
</tbody></table>

<p>However, unlike with ordinary Bernstein polynomials, the polynomial $W$ (and thus $U$) is not necessarily bounded by 0 and 1.  The following process can be used to calculate the required degree $n$, given an error tolerance of $\epsilon$.</p>

<ol>
<li>Determine whether $f$ is described in the table above.  Let <em>A</em> be the minimum of $f$ on the closed unit interval and let <em>B</em> be the maximum of $f$ there.</li>
<li>If 0 &lt; <em>A</em> &le; <em>B</em> &lt; 1, calculate $n$ as given in the table above, but with $\epsilon=\min(\epsilon, A, 1-B)$, and stop.</li>
<li>Propositions B1, B2, and B3 in the <a href="#Appendix"><strong>appendix</strong></a> give conditions on $f$ so that $W_{n,2}$ or $W_{n,3}$ (as the case may be) will be nonnegative.  If <em>B</em> is less than 1 and any of those conditions is met, calculate $n$ as given in the table above, but with $\epsilon=\min(\epsilon, 1-B)$. (For B3, set $n$ to max($n$, $m$), where $m$ is given in that proposition.) Then stop; $W$ will now be bounded by 0 and 1.</li>
<li>Calculate $n$ as given in the table above.  Then, if $W_{n,i}(j/n)\lt 0$ or $W_{n,i}(j/n)\gt 1$ for some $0\le j\le n$, double the value of $n$ until this condition is no longer true.</li>
</ol>

<p>Once <em>n</em> is found, simulating the iterated polynomial is as follows:</p>

<ol>
<li>Flip the input coin <em>n</em> times, and let <em>j</em> be the number of times the coin returned 1 this way.</li>
<li>With probability $W_{n,2}(j/n)$ or $W_{n,3}(j/n)$ (as the case may be), return 1.  Otherwise, return 0.</li>
</ol>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>Providing the full proof for the error bounds shown in the table is a bit tedious, so here is a sketch.  The proof was found by analyzing Theorem 3.3 of Güntürk and Li (2021)[^8], finding upper bounds for so-called &quot;central moments&quot; of the binomial distribution (see B4 to B7 in the appendix), then plugging them in to various estimates mentioned in that theorem&#39;s proof.  The most significant estimate in that theorem is denoted $(B_n-I)^{\lceil (r+1)/2 \rceil}(f)$, which in this case is the error when approximating $f$ using an iterated Bernstein polynomial, when $f$ has a continuous $(r+1)$-th derivative.</li>
<li>A polynomial&#39;s Bernstein coefficients can be rounded to multiples of $\delta$ (where $0 \lt\delta\le 1$) by setting $c$=floor($c/\delta$) * $\delta$ for each coefficient $c$.  The new polynomial will differ from the old one by at most $\delta$.  (Thus, to find a polynomial with multiple-of-$\delta$ coefficients that approximates $f$ with error $\epsilon$ [which must be greater than $\delta$], first find a polynomial with error $\epsilon - \delta$, then round that polynomial&#39;s coefficients as given here.)</li>
</ol>
</blockquote>

<p><a id=Approximate_Bernoulli_Factories_for_Power_Series></a></p>

<h3>Approximate Bernoulli Factories for Power Series</h3>

<p>Some functions can be rewritten as a power series, namely: $$f(\lambda) = a_0 \lambda^0 + a_1 \lambda^1 + ... + a_i \lambda^i + ...,$$ where $a_i$, the <em>coefficients</em>, are constant rational numbers[^10].</p>

<p>To simulate an approximation of $f$ that comes within $\epsilon$ of $f$:</p>

<ol>
<li><p>Find the first $n$+1 coefficients such that the polynomial $P(\lambda) = a_0 \lambda^0 + ... + a_n\lambda^n$ is within $\epsilon$ of $f$ wherever $0 \le \lambda \le 1$.</p>

<p>If $f$&#39;s coefficients are each greater than 0, form a nowhere increasing sequence (example: (1/4, 1/8, 1/8, 1/16, ...)), and meet the so-called &quot;ratio test&quot;, the algorithms in Carvalho and Moreira (2022)[^11] can be used here (see also &quot;<a href="#Proofs_on_Cutting_Off_a_Power_Series"><strong>Proofs on Cutting Off a Power Series</strong></a>&quot; in the appendix).</p>

<p>Alternatively, if bounds on the derivatives of $f$ are known, then thanks to Taylor&#39;s theorem, $P(\lambda)$ will be close enough if $M/((n+1)!) \le \epsilon$, where $M$ is equal to or greater than the maximum absolute value of $f$&#39;s ($n$+1)-th derivative on the domain of $f$.</p></li>
<li>Rewrite $P(\lambda)$ as a polynomial in Bernstein form.  (One way to transform a polynomial to Bernstein form, given the &quot;power&quot; coefficients $a_0, ..., a_n$, is the so-called &quot;matrix method&quot; from Ray and Nataraj (2012)[^12].)  Let $b_0, ..., b_n$ be the Bernstein-form polynomial&#39;s coefficients.</li>
<li>Flip the input coin <em>n</em> times, then let <em>j</em> be the number of times the coin returned 1 this way, then return either 1 with probability $b_j$, or 0 otherwise.</li>
</ol>

<p>In fact, if $f(\lambda)$ belongs in <em>Gevrey&#39;s hierarchy</em> (there are $B\ge 1, l\ge 1, \gamma\ge 1$ such that its $n$-th derivative&#39;s absolute value is not greater than $Bl^n n^{\gamma n}$ for every $n$), which includes functions equaling power series as a special case ($\gamma=1$), it&#39;s possible to bound the derivatives and find the appropriate degree for the approximating polynomial (for details, see (Kawamura et al. 2015)[^13]; see also (Gevrey 1918)[^14]).</p>

<p><a id=Approximate_Bernoulli_Factories_for_Linear_Functions></a></p>

<h3>Approximate Bernoulli Factories for Linear Functions</h3>

<p>There are a number of approximate methods to simulate <em>&lambda;</em>*<em>c</em>, where <em>c</em> &gt; 1 and 0 &le; <em>&lambda;</em> &lt; 1/<em>c</em>.  (&quot;Approximate&quot; because this function touches 1 at 1/<em>c</em>, so it can&#39;t be a factory function.) Since the methods use only up to <em>n</em> flips, where <em>n</em> is an integer greater than 0, the approximation will be a polynomial of degree <em>n</em>.</p>

<ul>
<li><p>Henderson and Glynn (2003, Remark 4)[^15] approximates the function <em>&lambda;</em>*2 using a polynomial where the <em>j</em><sup>th</sup> coefficient (starting at 0) is min((<em>j</em>/<em>n</em>)*2, 1&minus;1/<em>n</em>).  If <em>g</em>(<em>&lambda;</em>) is that polynomial, then the error in approximating <em>f</em> is no greater than 1&minus;<em>g</em>(1/2).  <em>g</em> can be computed with the SymPy computer algebra library as follows: <code>from sympy.stats import *; g=2*E( Min(sum(Bernoulli((&quot;B%d&quot; % (i)),z) for i in range(n))/n,(S(1)-S(1)/n)/2))</code>.</p></li>
<li><p>I found the following approximation for <em>&lambda;</em>*<em>c</em>[^16]: &quot;(1.) Set <em>j</em> to 0 and <em>i</em> to 0; (2.) If <em>i</em> &ge; <em>n</em>, return 0; (3.) Flip the input coin, and if it returns 1, add 1 to <em>j</em>; (4.) (Estimate the probability and return 1 if it &#39;went over&#39;.) If (<em>j</em>/(<em>i</em>+1)) &ge; 1/<em>c</em>, return 1; (5.) Add 1 to <em>i</em> and go to step 2.&quot;  Here, <em>&lambda;</em>*<em>c</em> is approximated by a polynomial where the <em>j</em><sup>th</sup> coefficient (starting at 0) is min((<em>j</em>/<em>n</em>)*<em>c</em>, 1).  If <em>g</em>(<em>&lambda;</em>) is that polynomial, then the error in approximating <em>f</em> is no greater than 1&minus;<em>g</em>(1/<em>c</em>).</p></li>
<li><p>The previous approximation generalizes the one given in section 6 of Nacu and Peres (2005)[^1], which approximates <em>&lambda;</em>*2.</p></li>
</ul>

<p><a id=Achievable_Simulation_Rates></a></p>

<h2>Achievable Simulation Rates</h2>

<p>In general, the number of input coin flips needed by any Bernoulli factory algorithm for a factory function <em>f</em>(<em>&lambda;</em>) depends on how &quot;smooth&quot; the function <em>f</em> is.</p>

<p>The following table summarizes the rate of simulation (in terms of the number of input coin flips needed) that can be achieved <em>in theory</em> depending on <em>f</em>(<em>&lambda;</em>), assuming the input coin&#39;s probability of heads is unknown.  In the table below:</p>

<ul>
<li><em>&lambda;</em>, the unknown probability of heads, is <em>&epsilon;</em> or greater and (1&minus;<em>&epsilon;</em>) or less for some <em>&epsilon;</em> &gt; 0.</li>
<li>The simulation makes use of unbiased random bits in addition to input coin flips.</li>
<li><em>&Delta;</em>(<em>n</em>, <em>r</em>, <em>&lambda;</em>) = <em>O</em>(max(sqrt(<em>&lambda;</em>*(1&minus;<em>&lambda;</em>)/<em>n</em>),1/<em>n</em>)<sup><em>r</em></sup>), that is, <em>O</em>((1/<em>n</em>)<sup><em>r</em></sup>) near <em>&lambda;</em> = 0 or 1, and <em>O</em>((1/<em>n</em>)<sup><em>r</em>/2</sup>) elsewhere. (<em>O</em>(<em>h</em>(<em>n</em>)) roughly means &quot;less than or equal to <em>h</em>(<em>n</em>) times a constant, for every <em>n</em> large enough&quot;.)</li>
</ul>

<table><thead>
<tr>
<th>Property of simulation</th>
<th>Property of <em>f</em></th>
</tr>
</thead><tbody>
<tr>
<td>Requires no more than <em>n</em> input coin flips.</td>
<td>If and only if <em>f</em> can be written as a polynomial in Bernstein form of degree <em>n</em> with coefficients in the closed unit interval (Goyal and Sigman 2012)[^17].</td>
</tr>
<tr>
<td>Requires a finite number of flips on average. Also known as &quot;realizable&quot; by Flajolet et al. (2010)[^18].</td>
<td>Only if <em>f</em> is Lipschitz continuous (Nacu and Peres 2005)[^1].<br/>Whenever <em>f</em> admits a fast simulation (Mendo 2019)[^19].</td>
</tr>
<tr>
<td>Number of flips required, raised to power of <em>r</em>, is bounded by a finite number on average and has a tail that drops off uniformly over <em>f</em>&#39;s domain.</td>
<td>Only if <em>f</em> has continuous <em>r</em>-th derivative (Nacu and Peres 2005)[^1].</td>
</tr>
<tr>
<td>Requires more than <em>n</em> flips with probability <em>&Delta;</em>(<em>n</em>, <em>r</em> + 1, <em>&lambda;</em>), for integer <em>r</em> &ge; 0 and every <em>&lambda;</em>. (The greater <em>r</em> is, the faster the simulation.)</td>
<td>Only if <em>f</em> has an <em>r</em>-th derivative that is continuous and in the Zygmund class (see note 3) (Holtz et al. 2011)[^20].</td>
</tr>
<tr>
<td>Requires more than <em>n</em> flips with probability <em>&Delta;</em>(<em>n</em>, <em>&alpha;</em>, <em>&lambda;</em>), for non-integer <em>&alpha;</em> &gt; 0 and every <em>&lambda;</em>. (The greater <em>&alpha;</em> is, the faster the simulation.)</td>
<td>If and only if <em>f</em> has an <em>r</em>-th derivative that is Hölder continuous with exponent (<em>&alpha;</em> &minus; <em>r</em>), where <em>r</em> = floor(<em>&alpha;</em>) (Holtz et al. 2011)[^20]. Assumes <em>f</em> is bounded away from 0 and 1.</td>
</tr>
<tr>
<td>&quot;Fast simulation&quot; (requires more than <em>n</em> flips with a probability that decays exponentially as <em>n</em> gets large).  Also known as &quot;strongly realizable&quot; by Flajolet et al. (2010)[^18].</td>
<td>If and only if <em>f</em> is real analytic (writable as $f(\lambda)=a_0 \lambda^0 + a_1 \lambda^1 + ...$ for real constants $a_i$) (Nacu and Peres 2005)[^1].</td>
</tr>
<tr>
<td>Average number of flips greater than or equal to (<em>f&prime;</em>(<em>&lambda;</em>))<sup>2</sup>*<em>&lambda;</em>*(1&minus;<em>&lambda;</em>)/(<em>f</em>(<em>&lambda;</em>)*(1&minus;<em>f</em>(<em>&lambda;</em>))), where <em>f&prime;</em> is the first derivative of <em>f</em>.</td>
<td>Whenever <em>f</em> admits a fast simulation (Mendo 2019)[^19].</td>
</tr>
</tbody></table>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>By the results of Holtz et al., it is suspected that the target function <em>f</em> can&#39;t be simulated using a finite number of flips on average for every probability of heads unless <em>f</em>&#39;s fourth derivative is Hölder continuous.</li>
<li>A function in the <em>Zygmund class</em>, roughly speaking, has no vertical slope.  The Zygmund class includes the smaller class of Lipschitz continuous functions.</li>
</ol>
</blockquote>

<p><a id=Complexity></a></p>

<h2>Complexity</h2>

<p>The following note shows the complexity of the algorithm for 1/<em>&phi;</em> in the main article, where <em>&phi;</em> is the golden ratio.</p>

<p>Let <strong>E</strong>[<em>N</em>] be the expected (&quot;long-run average&quot;) number of unbiased random bits (fair coin flips) generated by the algorithm.</p>

<p>Then, since each bit is independent, <strong>E</strong>[<em>N</em>] = 2*<em>&phi;</em> as shown below.</p>

<ul>
<li>Each iteration stops the algorithm with probability <em>p</em> = (1/2) + (1&minus;(1/2)) * (1/<em>&phi;</em>) (1/2 for the initial bit and 1/<em>&phi;</em> for the recursive run; (1&minus;(1/2)) because we&#39;re subtracting the (1/2) earlier on the right-hand side from 1).</li>
<li>Thus, the expected number of iterations is <strong>E</strong>[<em>T</em>] = 1/<em>p</em> by a well-known rejection sampling argument, since the algorithm doesn&#39;t depend on iteration counts.</li>
<li>Each iteration uses 1 * (1/2) + (1 + <strong>E</strong>[<em>N</em>]) * (1/2) bits on average, so the whole algorithm uses <strong>E</strong>[<em>N</em>] = (1 * (1/2) + (1 + <strong>E</strong>[<em>N</em>]) * (1/2)) * <strong>E</strong>[<em>T</em>] bits on average (each iteration consumes either 1 bit with probability 1/2, or (1 + <strong>E</strong>[<em>N</em>]) bits with probability 1/2). This equation has the solution <strong>E</strong>[<em>N</em>] = 1 + sqrt(5) = 2*<em>&phi;</em>.</li>
</ul>

<p>Also, on average, half of these flips (<em>&phi;</em>) show 1 and half show 0, since the bits are unbiased (the coin is fair).</p>

<p>A similar analysis to the one above can be used to find the expected (&quot;long-run average&quot;) time complexity of many Bernoulli factory algorithms.</p>

<p><a id=Examples_of_Bernoulli_Factory_Polynomial_Building_Schemes></a></p>

<h2>Examples of Bernoulli Factory Polynomial-Building Schemes</h2>

<p>The following are polynomial-building schemes and hints to simulate a coin of probability <em>f</em>(<em>&lambda;</em>) given an input coin with probability of heads of <em>&lambda;</em>.  The schemes were generated automatically using <code>approxscheme2</code> and have not been rigorously verified for correctness.</p>

<ul>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>cosh(<em>&lambda;</em>) &minus; 3/4</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>The function was detected to be convex and twice differentiable, leading to:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 487/2500 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 154309/(700000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 1043/5000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 462927/(2800000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>3/4 &minus; sqrt(&minus;<em>&lambda;</em>*(<em>&lambda;</em> &minus; 1))</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be convex and (1/2)-Hölder continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1545784563/(400000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 26278337571/(25600000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
</ul></li>
<li>Let <em>f</em>(<em>&lambda;</em>) = <strong>3*sin(sqrt(3)*sqrt(sin(2*<em>&lambda;</em>)))/4 + 1/50</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:

<ul>
<li>Detected to be (1/2)-Hölder continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 709907859/(100000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 709907859/(100000000*n<sup>1/4</sup>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 6389170731/(3200000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 6389170731/(3200000000*n<sup>1/4</sup>).</li>
</ul></li>
</ul></li>
<li>Let <em>f</em>(<em>&lambda;</em>) = <strong>3/4 &minus; sqrt(&minus;<em>&lambda;</em>*(<em>&lambda;</em> &minus; 1))</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:

<ul>
<li>Detected to be convex and (1/2)-Hölder continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1545784563/(400000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 26278337571/(25600000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong><em>&lambda;</em>*sin(7*&pi;*<em>&lambda;</em>)/4 + 1/2</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>Detected to be twice differentiable using numerical methods, which may be inaccurate:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 523/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 11346621/(700000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 1229/1250 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 11346621/(700000*n).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 681/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 34039863/(4480000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 4837/5000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 34039863/(4480000*n).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>sin(4*&pi;*<em>&lambda;</em>)/4 + 1/2</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>The function was detected to be twice differentiable, leading to:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 737/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1973921/(350000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 9263/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 1973921/(350000*n).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 1123/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1973921/(448000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 8877/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 1973921/(448000*n).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>sin(6*&pi;*<em>&lambda;</em>)/4 + 1/2</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>The function was detected to be twice differentiable, leading to:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 517/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 2220661/(175000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 9483/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 2220661/(175000*n).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 1043/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 104371067/(11200000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 8957/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 104371067/(11200000*n).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>sin(4*&pi;*<em>&lambda;</em>)/4 + 1/2</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>The function was detected to be twice differentiable, leading to:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 737/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1973921/(350000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 9263/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 1973921/(350000*n).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 1123/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1973921/(448000*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 8877/10000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 1973921/(448000*n).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong><em>&lambda;</em><sup>2</sup>/2 + 1/10 if <em>&lambda;</em> &le; 1/2; <em>&lambda;</em>/2 &minus; 1/40 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be convex and twice differentiable using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 321/5000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1/(7*n).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>1/2 &minus; sqrt(1 &minus; 2*<em>&lambda;</em>)/2 if <em>&lambda;</em> &lt; 1/2; sqrt(2*<em>&lambda;</em> &minus; 1)/2 + 1/2 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be (1/2)-Hölder continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 1545784563/(400000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 1545784563/(400000000*n<sup>1/4</sup>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 10820491941/(12800000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 10820491941/(12800000000*n<sup>1/4</sup>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>1/2 &minus; sqrt(1 &minus; 2*<em>&lambda;</em>)/4 if <em>&lambda;</em> &lt; 1/2; sqrt(2*<em>&lambda;</em> &minus; 1)/4 + 1/2 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be (1/2)-Hölder continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 772969563/(400000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 772969563/(400000000*n<sup>1/4</sup>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 193/5000 if <em>n</em>&lt;16; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 5410786941/(12800000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 4807/5000 if <em>n</em>&lt;16; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 5410786941/(12800000000*n<sup>1/4</sup>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong><em>&lambda;</em>/2 + (1 &minus; 2*<em>&lambda;</em>)<sup>3/2</sup>/12 &minus; 1/12 if <em>&lambda;</em> &lt; 0; <em>&lambda;</em>/2 + (2*<em>&lambda;</em> &minus; 1)<sup>3/2</sup>/12 &minus; 1/12 if <em>&lambda;</em> &ge; 1/2; <em>&lambda;</em>/2 + (1 &minus; 2*<em>&lambda;</em>)<sup>3/2</sup>/12 &minus; 1/12 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be convex and Lipschitz continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 322613/(500000*sqrt(n)).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 3548743/(32000000*sqrt(n)).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>1/2 &minus; sqrt(1 &minus; 2*<em>&lambda;</em>)/4 if <em>&lambda;</em> &lt; 1/2; sqrt(2*<em>&lambda;</em> &minus; 1)/4 + 1/2 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be (1/2)-Hölder continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; 772969563/(400000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 772969563/(400000000*n<sup>1/4</sup>).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 193/5000 if <em>n</em>&lt;16; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 5410786941/(12800000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 4807/5000 if <em>n</em>&lt;16; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 5410786941/(12800000000*n<sup>1/4</sup>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>1/2 &minus; sqrt(1 &minus; 2*<em>&lambda;</em>)/8 if <em>&lambda;</em> &lt; 1/2; sqrt(2*<em>&lambda;</em> &minus; 1)/8 + 1/2 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>Detected to be (1/2)-Hölder continuous using numerical methods, which may be inaccurate:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 333/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 386562063/(400000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 9667/10000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 386562063/(400000000*n<sup>1/4</sup>).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 451/2000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 2705934441/(12800000000*n<sup>1/4</sup>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 1549/2000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 2705934441/(12800000000*n<sup>1/4</sup>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong><em>&lambda;</em>/4 + (1 &minus; 2*<em>&lambda;</em>)<sup>3/2</sup>/24 + 5/24 if <em>&lambda;</em> &lt; 0; <em>&lambda;</em>/4 + (2*<em>&lambda;</em> &minus; 1)<sup>3/2</sup>/24 + 5/24 if <em>&lambda;</em> &ge; 1/2; <em>&lambda;</em>/4 + (1 &minus; 2*<em>&lambda;</em>)<sup>3/2</sup>/24 + 5/24 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>Detected to be convex and Lipschitz continuous using numerical methods, which may be inaccurate:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 443/5000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 322613/(1000000*sqrt(n)).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = 1111/5000 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) &minus; 3548743/(64000000*sqrt(n)).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>3*<em>&lambda;</em>/2 if <em>&lambda;</em> &le; 1 &minus; <em>&lambda;</em>; 3/2 &minus; 3*<em>&lambda;</em>/2 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>Detected to be concave and Lipschitz continuous using numerical methods, which may be inaccurate:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 124/125 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 967839/(500000*sqrt(n)).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 1863/2000 if <em>n</em>&lt;64; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 2903517/(2000000*sqrt(n)).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>9*<em>&lambda;</em>/5 if <em>&lambda;</em> &le; 1 &minus; <em>&lambda;</em>; 9/5 &minus; 9*<em>&lambda;</em>/5 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li>Detected to be concave and Lipschitz continuous using numerical methods, which may be inaccurate:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 2903517/(1250000*sqrt(n)).</li>
</ul></li>
<li>Generated using tighter bounds than necessarily proven:

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + 8710551/(5000000*sqrt(n)).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>19*<em>&lambda;</em>/20 if <em>&lambda;</em> &le; 1 &minus; <em>&lambda;</em>; 19/20 &minus; 19*<em>&lambda;</em>/20 otherwise</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>Detected to be concave and Lipschitz continuous using numerical methods, which may be inaccurate:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 1817/2000 if <em>n</em>&lt;8; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 6129647/(5000000*sqrt(n)).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 2337/2500 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 18388941/(20000000*sqrt(n)).</li>
</ul></li>
</ul></li>
<li><p>Let <em>f</em>(<em>&lambda;</em>) = <strong>min(1/8, 3*<em>&lambda;</em>)</strong>. Then, for every integer <em>n</em> that&#39;s a power of 2, starting from 1:</p>

<ul>
<li><p>Detected to be concave and Lipschitz continuous using numerical methods, which may be inaccurate:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 4047/5000 if <em>n</em>&lt;32; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 967839/(250000*sqrt(n)).</li>
</ul></li>
<li><p>Generated using tighter bounds than necessarily proven:</p>

<ul>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = 171/400 if <em>n</em>&lt;4; otherwise, <em>f</em>(<em>k</em>/<em>n</em>) + 967839/(1600000*sqrt(n)).</li>
</ul></li>
</ul></li>
</ul>

<p><a id=Miscellaneous_Bernoulli_Factories></a></p>

<h2>Miscellaneous Bernoulli Factories</h2>

<p>&nbsp;</p>

<p>In the methods below, <em>&lambda;</em> is the unknown probability of heads of the coin involved in the Bernoulli factory problem.</p>

<p><a id=Certain_Piecewise_Linear_Functions></a></p>

<h3>Certain Piecewise Linear Functions</h3>

<p>Let <em>f</em>(<em>&lambda;</em>) be a function of the form min(<em>&lambda;</em>*<em>mult</em>, 1&minus;<em>&epsilon;</em>). This is a <em>piecewise linear function</em>, a function made up of two linear pieces (in this case, the pieces are a rising linear part and a constant part).</p>

<p>This section describes how to calculate the Bernstein coefficients for polynomials that converge from above and below to <em>f</em>, based on Thomas and Blanchet (2012)[^21].  These polynomials can then be used to show heads with probability <em>f</em>(<em>&lambda;</em>) using the algorithms given in &quot;<a href="https://peteroupc.github.io/bernoulli.html#General_Factory_Functions"><strong>General Factory Functions</strong></a>&quot;.</p>

<p>In this section, <strong>fbelow(<em>n</em>, <em>k</em>)</strong> and <strong>fabove(<em>n</em>, <em>k</em>)</strong> are the <em>k</em><sup>th</sup> coefficients (with <em>k</em> starting at 0) of the lower and upper polynomials, respectively, in Bernstein form of degree <em>n</em>.</p>

<p>The code at the end of this section uses the computer algebra library SymPy to calculate a list of parameters for a sequence of polynomials converging from above.  The method to do so is called <code>calc_linear_func(eps, mult, count)</code>, where <code>eps</code> is <em>&epsilon;</em>, <code>mult</code> = <em>mult</em>, and <code>count</code> is the number of polynomials to generate.  Each item returned by <code>calc_linear_func</code> is a list of two items: the degree of the polynomial, and a <em>Y parameter</em>.  The procedure to calculate the required polynomials is then logically as follows (as written, it runs very slowly, though):</p>

<ol>
<li>Set <em>i</em> to 1.</li>
<li>Run <code>calc_linear_func(eps, mult, i)</code> and get the degree and <em>Y parameter</em> for the last listed item, call them <em>n</em> and <em>y</em>, respectively.</li>
<li>Set <em>x</em> to &minus;((<em>y</em>&minus;(1&minus;<em>&epsilon;</em>))/<em>&epsilon;</em>)<sup>5</sup>/<em>mult</em> + <em>y</em>/<em>mult</em>.  (This exact formula doesn&#39;t appear in the Thomas and Blanchet paper; rather it comes from the <a href="https://github.com/acthomasca/rberfac/blob/main/rberfac-public-2.R"><strong>supplemental source code</strong></a> uploaded by A. C. Thomas at my request.)</li>
<li>For degree <em>n</em>, <strong>fbelow(<em>n</em>, <em>k</em>)</strong> is min((<em>k</em>/<em>n</em>)*<em>mult</em>, 1&minus;<em>&epsilon;</em>), and <strong>fabove(<em>n</em>, <em>k</em>)</strong> is min((<em>k</em>/<em>n</em>)*<em>y</em>/<em>x</em>,<em>y</em>).  (<strong>fbelow</strong> matches <em>f</em> because <em>f</em> is <em>concave</em> on the interval [0, 1], which roughly means that its rate of growth there never goes up.)</li>
<li>Add 1 to <em>i</em> and go to step 2.</li>
</ol>

<p>It would be interesting to find general formulas to find the appropriate polynomials (degrees and <em>Y parameters</em>) given only the values for <em>mult</em> and <em>&epsilon;</em>, rather than find them &quot;the hard way&quot; via <code>calc_linear_func</code>.  For this procedure, the degrees and <em>Y parameters</em> can be upper bounds, as long as the sequence of degrees is strictly increasing and the sequence of Y parameters is nowhere increasing.</p>

<blockquote>
<p><strong>Note:</strong> In Nacu and Peres (2005)[^1], the following polynomial sequences were suggested to simulate $\min(2\lambda, 1-2\varepsilon)$, provided $\varepsilon \lt 1/8$, where <em>n</em> is a power of 2.  However, with these sequences, an extraordinary number of input coin flips is required to simulate this function each time.</p>

<ul>
<li><strong>fbelow(<em>n</em>, <em>k</em>)</strong> = $\min(2(k/n), 1-2\varepsilon)$.</li>
<li><strong>fabove(<em>n</em>, <em>k</em>)</strong> = $\min(2(k/n), 1-2\varepsilon)+$<br> $
\frac{2\times\max(0, k/n+3\varepsilon - 1/2)}{\varepsilon(2-\sqrt{2})} \sqrt{2/n}+$<br> $\frac{72\times\max(0,k/n-1/9)}{1-\exp(-2\times\varepsilon^2)} \exp(-2n\times\varepsilon^2)$.</li>
</ul>
</blockquote>

<p>SymPy code for piecewise linear functions:</p>

<pre>def bernstein_n(func, x, n, pt=None):
  # Bernstein operator.
  # Create a polynomial that approximates func, which in turn uses
  # the symbol x.  The polynomial&#39;s degree is n and is evaluated
  # at the point pt (or at x if not given).
  if pt==None: pt=x
  ret=0
  v=[binomial(n,j) for j in range(n//2+1)]
  for i in range(0, n+1):
    oldret=ret
    bino=v[i] if i&lt;len(v) else v[n-i]
    ret+=func.subs(x,S(i)/n)*bino*pt**i*(1-pt)**(n-i)
    if pt!=x and ret==oldret and ret&gt;0: break
  return ret

def inflec(y,eps=S(2)/10,mult=2):
  # Calculate the inflection point (x) given y, eps, and mult.
  # The formula is not found in the paper by Thomas and
  # Blanchet 2012, but in
  # the supplemental source code uploaded by
  # A.C. Thomas.
  po=5 # Degree of y-to-x polynomial curve
  eps=S(eps)
  mult=S(mult)
  x=-((y-(1-eps))/eps)**po/mult + y/mult
  return x

def xfunc(y,sym,eps=S(2)/10,mult=2):
  # Calculate Bernstein &quot;control polygon&quot; given y,
  # eps, and mult.
  return Min(sym*y/inflec(y,eps,mult),y)

def calc_linear_func(eps=S(5)/10, mult=1, count=10):
   # Calculates the degrees and Y parameters
   # of a sequence of polynomials that converge
   # from above to min(x*mult, 1-eps).
   # eps must be greater than 0 and less than 1.
   # Default is 10 polynomials.
   polys=[]
   eps=S(eps)
   mult=S(mult)
   count=S(count)
   bs=20
   ypt=1-(eps/4)
   x=symbols(&#39;x&#39;)
   tfunc=Min(x*mult,1-eps)
   tfn=tfunc.subs(x,(1-eps)/mult).n()
   xpt=xfunc(ypt,x,eps=eps,mult=mult)
   bits=5
   i=0
   lastbxn = 1
   diffs=[]
   while i&lt;count:
     bx=bernstein_n(xpt,x,bits,(1-eps)/mult)
     bxn=bx.n()
     if bxn &gt; tfn and bxn &lt; lastbxn:
       # Dominates target function
       #if oldbx!=None:
       #   diffs.append(bx)
       #   diffs.append(oldbx-bx)
       #oldbx=bx
       oldxpt=xpt
       lastbxn = bxn
       polys.append([bits,ypt])
       print(&quot;    [%d,%s],&quot; % (bits,ypt))
       # Find y2 such that y2 &lt; ypt and
       # bernstein_n(oldxpt,x,bits,inflec(y2, ...)) &gt;= y2,
       # so that next Bernstein expansion will go
       # underneath the previous one
       while True:
         ypt-=(ypt-(1-eps))/4
         xpt=inflec(ypt,eps=eps,mult=mult).n()
         bxs=bernstein_n(oldxpt,x,bits,xpt).n()
         if bxs&gt;=ypt.n():
            break
       xpt=xfunc(ypt,x,eps=eps,mult=mult)
       bits+=20
       i+=1
     else:
       bits=int(bits*200/100)
   return polys

calc_linear_func(count=8)
</pre>

<p><a id=Pushdown_Automata_for_Square_Root_Like_Functions></a></p>

<h3>Pushdown Automata for Square-Root-Like Functions</h3>

<p>In this section, ${n \choose m}$ = choose($n$, $m$) is a binomial coefficient.[^60] [^61]</p>

<p>The following algorithm extends the square-root construction of Flajolet et al. (2010)[^22], takes an input coin with probability of heads <em>&lambda;</em> (where 0 &le; <em>&lambda;</em> &lt; 1), and returns 1 with probability&mdash;</p>

<p>$$f(\lambda)=\frac{1-\lambda}{\sqrt{1+4\lambda\mathtt{Coin}(\lambda)(\mathtt{Coin}(\lambda)-1)}} = (1-\lambda)\sum_{n\ge 0} \lambda^n (\mathtt{Coin}(\lambda))^n (1-\mathtt{Coin}(\lambda))^n {2n \choose n}$$  $$= (1-\lambda)\sum_{n\ge 0} (\lambda \mathtt{Coin}(\lambda) (1-\mathtt{Coin}(\lambda)))^n {2n \choose n}$$ $$= \sum_{n\ge 0} (1-\lambda) \lambda^n h_n(\lambda) = \sum_{n\ge 0} g(n, \lambda) h_n(\lambda),$$</p>

<p>and 0 otherwise [^62], where:</p>

<ul>
<li><code>Coin</code>(<em>&lambda;</em>) is a Bernoulli factory function. Although not required, <code>Coin</code> can be a rational function (a ratio of two polynomials) whose coefficients are rational numbers; if so, <em>f</em> will be an <em>algebraic function</em> (a function that can be a solution of a nonzero polynomial equation) and can be simulated by a <em>pushdown automaton</em>, or a state machine with a stack (see the algorithm below and the note that follows it). In the original square-root construction,  <code>Coin</code>(<em>&lambda;</em>) = 1/2.</li>
<li>$g(n, \lambda) = (1-\lambda) \lambda^n$; this is the probability of running the <code>Coin</code> Bernoulli factory $2 \times n$ times.</li>
<li>$h_n(\lambda) = (\mathtt{Coin}(\lambda))^n (1-\mathtt{Coin}(\lambda))^n {2n \choose n}$; this is the probability of getting as many ones as zeros from the <code>Coin</code> Bernoulli factory.</li>
</ul>

<p>Equivalently&mdash; $$f(\lambda)=(1-\lambda) OGF(\lambda \mathtt{Coin}(\lambda) (1-\mathtt{Coin}(\lambda))),$$ where $OGF(x) = \sum_{n\ge 0} x^n {2n \choose n}$ is the algorithm&#39;s ordinary generating function (also known as counting generating function).</p>

<p>The algorithm follows.</p>

<ol>
<li>Set <em>d</em> to 0.</li>
<li>Do the following process repeatedly until this run of the algorithm returns a value:

<ol>
<li>Flip the input coin.  If it returns 1, go to the next substep.  Otherwise, return either 1 if <em>d</em> is 0, or 0 otherwise.</li>
<li>Run a Bernoulli factory algorithm for <code>Coin</code>(<em>&lambda;</em>).  If the run returns 1, add 1 to <em>d</em>.  Otherwise, subtract 1 from <em>d</em>.</li>
<li>Repeat the previous substep.</li>
</ol></li>
</ol>

<blockquote>
<p><strong>Note:</strong> A <em>pushdown automaton</em> is a state machine that keeps a stack of symbols.  In this document, the input for this automaton is a stream of flips of a coin that shows heads with probability <em>&lambda;</em>, and the output is 0 or 1 depending on which state the automaton ends up in when it empties the stack (Mossel and Peres 2005)[^23].  That paper shows that a pushdown automaton, as defined here, can simulate only <em>algebraic functions</em>, that is, functions that can be a solution of a nonzero polynomial equation.  The <a href="#Pushdown_Automata_and_Algebraic_Functions"><strong>appendix</strong></a> defines these machines in more detail and has proofs on which algebraic functions are possible with pushdown automata.</p>

<p>As a pushdown automaton, this algorithm (except the &quot;Repeat the previous substep&quot; part) can be expressed as follows. Let the stack have the single symbol EMPTY, and start at the state POS-S1.  Based on the current state, the last coin flip (HEADS or TAILS), and the symbol on the top of the stack, set the new state and replace the top stack symbol with zero, one, or two symbols.  These <em>transition rules</em> can be written as follows:</p>

<ul>
<li>(POS-S1, HEADS, <em>topsymbol</em>) &rarr; (POS-S2, {<em>topsymbol</em>}) (set state to POS-S2, keep <em>topsymbol</em> on the stack).</li>
<li>(NEG-S1, HEADS, <em>topsymbol</em>) &rarr; (NEG-S2, {<em>topsymbol</em>}).</li>
<li>(POS-S1, TAILS, EMPTY) &rarr; (ONE, {}) (set state to ONE, pop the top symbol from the stack).</li>
<li>(NEG-S1, TAILS, EMPTY) &rarr; (ONE, {}).</li>
<li>(POS-S1, TAILS, X) &rarr; (ZERO, {}).</li>
<li>(NEG-S1, TAILS, X) &rarr; (ZERO, {}).</li>
<li>(ZERO, <em>flip</em>, <em>topsymbol</em>) &rarr; (ZERO, {}).</li>
<li>(POS-S2, <em>flip</em>, <em>topsymbol</em>) &rarr; Add enough transition rules to the automaton to simulate <em>g</em>(<em>&lambda;</em>) by a finite-state machine (only possible if <em>g</em> is rational with rational coefficients (Mossel and Peres 2005)[^23]).  Transition to POS-S2-ZERO if the machine outputs 0, or POS-S2-ONE if the machine outputs 1.</li>
<li>(NEG-S2, <em>flip</em>, <em>topsymbol</em>) &rarr; Same as before, but the transitioning states are NEG-S2-ZERO and NEG-S2-ONE, respectively.</li>
<li>(POS-S2-ONE, <em>flip</em>, <em>topsymbol</em>) &rarr; (POS-S1, {<em>topsymbol</em>, X}) (replace top stack symbol with <em>topsymbol</em>, then push X to the stack).</li>
<li>(POS-S2-ZERO, <em>flip</em>, EMPTY) &rarr; (NEG-S1, {EMPTY, X}).</li>
<li>(POS-S2-ZERO, <em>flip</em>, X) &rarr; (POS-S1, {}).</li>
<li>(NEG-S2-ZERO, <em>flip</em>, <em>topsymbol</em>) &rarr; (NEG-S1, {<em>topsymbol</em>, X}).</li>
<li>(NEG-S2-ONE, <em>flip</em>, EMPTY) &rarr; (POS-S1, {EMPTY, X}).</li>
<li>(NEG-S2-ONE, <em>flip</em>, X) &rarr; (NEG-S1, {}).</li>
</ul>

<p>The machine stops when it removes EMPTY from the stack, and the result is either ZERO (0) or ONE (1).</p>
</blockquote>

<p>For the following algorithm, which extends the end of Note 1 of the Flajolet paper, the probability is&mdash; $$f(\lambda)=(1-\lambda) \sum_{n\ge 0} \lambda^{Hn} \mathtt{Coin}(\lambda)^n (1-\mathtt{Coin}(\lambda))^{Hn-n} {Hn \choose n},$$ where <em>H</em> &ge; 2 is an integer; and <code>Coin</code> has the same meaning as earlier.</p>

<ol>
<li>Set <em>d</em> to 0.</li>
<li>Do the following process repeatedly until this run of the algorithm returns a value:

<ol>
<li>Flip the input coin.  If it returns 1, go to the next substep.  Otherwise, return either 1 if <em>d</em> is 0, or 0 otherwise.</li>
<li>Run a Bernoulli factory algorithm for <code>Coin</code>(<em>&lambda;</em>).  If the run returns 1, add (<em>H</em>&minus;1) to <em>d</em>.  Otherwise, subtract 1 from <em>d</em>.</li>
</ol></li>
</ol>

<p>The following algorithm simulates the probability&mdash; $$
f(\lambda) = (1-\lambda) \sum_{n\ge 0} \lambda^n \left( \sum_{m\ge 0} W(n,m) \mathtt{Coin}(\lambda)^m (1-\mathtt{Coin}(\lambda))^{n-m} {n \choose m}\right)$$ $$= (1-\lambda) \sum_{n\ge 0} \lambda^n \left( \sum_{m\ge 0} V(n,m) \mathtt{Coin}(\lambda)^m (1-\mathtt{Coin}(\lambda))^{n-m}\right),$$ where <code>Coin</code> has the same meaning as earlier; <em>W</em>(<em>n</em>, <em>m</em>) is 1 if <em>m</em>*<em>H</em> equals (<em>n</em>&minus;<em>m</em>)*<em>T</em>, or 0 otherwise; and <em>H</em>&ge;1 and <em>T</em>&ge;1 are integers. (In the first formula, the sum in parentheses is a polynomial in Bernstein form, in the variable <code>Coin</code>(<em>&lambda;</em>) and with only zeros and ones as coefficients.  Because of the <em>&lambda;</em><sup><em>n</em></sup>, the polynomial gets smaller as <em>n</em> gets larger.  <em>V</em>(<em>n</em>, <em>m</em>) is the number of <em>n</em>-letter words that have <em>m</em> heads <em>and</em> describe a walk that ends at the beginning.)</p>

<ol>
<li>Set <em>d</em> to 0.</li>
<li>Do the following process repeatedly until this run of the algorithm returns a value:

<ol>
<li>Flip the input coin.  If it returns 1, go to the next substep.  Otherwise, return either 1 if <em>d</em> is 0, or 0 otherwise.</li>
<li>Run a Bernoulli factory algorithm for <code>Coin</code>(<em>&lambda;</em>).  If the run returns 1 (&quot;heads&quot;), add <em>H</em> to <em>d</em>.  Otherwise (&quot;tails&quot;), subtract <em>T</em> from <em>d</em>.</li>
</ol></li>
</ol>

<p><a id=Ratio_of_Lower_Gamma_Functions_gamma__m___x__gamma__m__1></a></p>

<h3>Ratio of Lower Gamma Functions (&gamma;(<em>m</em>, <em>x</em>)/&gamma;(<em>m</em>, 1)).</h3>

<ol>
<li>Set <em>ret</em> to the result of <strong>kthsmallest</strong> with the two parameters <em>m</em> and <em>m</em>.  (Thus, <em>ret</em> is distributed as <em>u</em><sup>1/<em>m</em></sup> where <em>u</em> is a uniform random variate greater than 0 and less than 1; although <strong>kthsmallest</strong> accepts only integers, this formula works for every <em>m</em> greater than 0.)</li>
<li>Set <em>k</em> to 1, then set <em>u</em> to point to the same value as <em>ret</em>.</li>
<li>Generate a uniform(0, 1) random variate <em>v</em>.</li>
<li>If <em>v</em> is less than <em>u</em>: Set <em>u</em> to <em>v</em>, then add 1 to <em>k</em>, then go to step 3.</li>
<li>If <em>k</em> is odd[^24], return a number that is 1 if <em>ret</em> is less than <em>x</em> and 0 otherwise. (If <em>ret</em> is implemented as a uniform partially-sampled random number (PSRN), this comparison should be done via <strong>URandLessThanReal</strong>.)  If <em>k</em> is even[^25], go to step 1.</li>
</ol>

<p>Derivation:  See Formula 1 in the section &quot;<a href="https://peteroupc.github.io/bernoulli.html#Probabilities_Arising_from_Certain_Permutations"><strong>Probabilities Arising from Certain Permutations</strong></a>&quot;, where:</p>

<ul>
<li><code>ECDF(x)</code>  is the probability that a uniform random variate greater than 0 and less than 1 is <em>x</em> or less, namely <em>x</em> if <em>x</em> is in [0, 1], 0 if <em>x</em> is less than 0, and 1 otherwise.</li>
<li><code>DPDF(x)</code> is the probability density function for the maximum of <em>m</em> uniform random variates in [0, 1], namely <em>m</em>*<em>x</em><sup><em>m</em>&minus;1</sup> if <em>x</em> is in [0, 1], and 0 otherwise.</li>
</ul>

<p><a id=4_3___pi></a></p>

<h3>4/(3*<em>&pi;</em>)</h3>

<p>Given that the point (<em>x</em>, <em>y</em>) has positive coordinates and lies inside a disk of radius 1 centered at (0, 0), the mean value of <em>x</em> is 4/(3*<em>&pi;</em>). This leads to the following algorithm to sample that probability:</p>

<ol>
<li>Generate two partially-sampled random numbers (PSRNs) in the form of a uniformly chosen point inside a 2-dimensional quarter hypersphere (that is, a quarter of a &quot;filled circle&quot;; see &quot;<a href="https://peteroupc.github.io/exporand.html#Uniform_Distribution_Inside_N_Dimensional_Shapes"><strong>Uniform Distribution Inside N-Dimensional Shapes</strong></a>&quot; in the article &quot;Partially-Sampled Random Numbers&quot;, as well as the examples there).</li>
<li>Let <em>x</em> be one of those PSRNs.  Run <strong>SampleGeometricBag</strong> on that PSRN and return the result (which will be either 0 or 1).</li>
</ol>

<blockquote>
<p><strong>Note:</strong> The mean value 4/(3*<em>&pi;</em>) can be derived as follows.  The relative probability that <em>x</em> is &quot;close&quot; to <em>z</em>, where $0\le <em>z</em> \le 1$, is <em>p</em>(<em>z</em>) = sqrt(1 &minus; <em>z</em>*<em>z</em>).  Now find the integral (&quot;area under the graph&quot;) of <em>z</em>*<em>p</em>(<em>z</em>)/<em>c</em> (where <em>c</em>=<em>&pi;</em>/4 is the integral of <em>p</em>(<em>z</em>) on the interval [0, 1]).  The result is the mean value 4/(3*<em>&pi;</em>).  The following Python code prints this mean value using the SymPy computer algebra library: <code>p=sqrt(1-z*z); c=integrate(p,(z,0,1)); print(integrate(z*p/c,(z,0,1)));</code>.</p>
</blockquote>

<p><a id=1_exp__k__1_exp__k__1></a></p>

<h3>(1 + exp(<em>k</em>)) / (1 + exp(<em>k</em> + 1))</h3>

<p>This algorithm simulates this probability by computing lower and upper bounds of exp(1), which improve as more and more digits are calculated.  These bounds are calculated through an algorithm by Citterio and Pavani (2016)[^26].  Note the use of the methodology in Łatuszyński et al. (2009/2011, algorithm 2)[^27] in this algorithm.  In this algorithm, <em>k</em> must be an integer 0 or greater.</p>

<ol>
<li>If <em>k</em> is 0, run the <strong>algorithm for 2 / (1 + exp(2))</strong> and return the result.  If <em>k</em> is 1, run the <strong>algorithm for (1 + exp(1)) / (1 + exp(2))</strong> and return the result.</li>
<li>Generate a uniform(0, 1) random variate, call it <em>ret</em>.</li>
<li>If <em>k</em> is 3 or greater, return 0 if <em>ret</em> is greater than 38/100, or 1 if <em>ret</em> is less than 36/100.  (This is an early return step.  If <em>ret</em> is implemented as a uniform PSRN, these comparisons should be done via the <strong>URandLessThanReal algorithm</strong>, which is described in my <a href="https://peteroupc.github.io/exporand.html"><strong>article on PSRNs</strong></a>.)</li>
<li>Set <em>d</em> to 2.</li>
<li>Calculate a lower and upper bound of exp(1) (<em>LB</em> and <em>UB</em>, respectively) in the form of rational numbers whose numerator has at most <em>d</em> digits, using the Citterio and Pavani algorithm.  For details, see later.</li>
<li>Set <em>rl</em> to (1+<em>LB</em><sup><em>k</em></sup>) / (1+<em>UB</em><sup><em>k</em> + 1</sup>), and set <em>ru</em> to (1+<em>UB</em><sup><em>k</em></sup>) / (1+<em>LB</em><sup><em>k</em> + 1</sup>); both these numbers should be calculated using rational arithmetic.</li>
<li>If <em>ret</em> is greater than <em>ru</em>, return 0.  If <em>ret</em> is less than <em>rl</em>, return 1.  (If <em>ret</em> is implemented as a uniform PSRN, these comparisons should be done via <strong>URandLessThanReal</strong>.)</li>
<li>Add 1 to <em>d</em> and go to step 5.</li>
</ol>

<p>The following implements the parts of Citterio and Pavani&#39;s algorithm needed to calculate lower and upper bounds for exp(1) in the form of rational numbers.</p>

<p>Define the following operations:</p>

<ul>
<li><strong>Setup:</strong> Set <em>p</em> to the list <code>[0, 1]</code>, set <em>q</em> to the list <code>[1, 0]</code>, set <em>a</em> to the list <code>[0, 0, 2]</code> (two zeros, followed by the integer part for exp(1)), set <em>v</em> to 0, and set <em>av</em> to 0.</li>
<li><strong>Ensure <em>n</em>:</strong> While <em>v</em> is less than or equal to <em>n</em>:

<ol>
<li>(Ensure partial denominator <em>v</em>, starting from 0, is available.) If <em>v</em> + 2 is greater than or equal to the size of <em>a</em>, append 1, <em>av</em>, and 1, in that order, to the list <em>a</em>, then add 2 to <em>av</em>.</li>
<li>(Calculate convergent <em>v</em>, starting from 0.) Append <em>a</em>[<em>n</em>+2] * <em>p</em>[<em>n</em>+1]+<em>p</em>[<em>n</em>] to the list <em>p</em>, and append <em>a</em>[<em>n</em>+2] * <em>q</em>[<em>n</em>+1]+<em>q</em>[<em>n</em>] to the list <em>q</em>. (Positions in lists start at 0.  For example, <em>p</em>[0] means the first item in <em>p</em>; <em>p</em>[1] means the second; and so on.)</li>
<li>Add 1 to <em>v</em>.</li>
</ol></li>
<li><strong>Get the numerator for convergent <em>n</em>:</strong> Ensure <em>n</em>, then return <em>p</em>[<em>n</em>+2].</li>
<li><strong>Get convergent <em>n</em>:</strong> Ensure <em>n</em>, then return <em>p</em>[<em>n</em>+2]/<em>q</em>[<em>n</em>+2].</li>
<li><strong>Get semiconvergent <em>n</em> given <em>d</em>:</strong>

<ol>
<li>Ensure <em>n</em>, then set <em>m</em> to floor(((10<sup><em>d</em></sup>)&minus;1&minus;<em>p</em>[<em>n</em>+1])/<em>p</em>[<em>n</em>+2]).</li>
<li>Return (<em>p</em>[<em>n</em>+2] * <em>m</em> +<em>p</em>[<em>n</em>+1]) / (<em>q</em>[<em>n</em>+2] * <em>m</em> +<em>q</em>[<em>n</em>+1]).</li>
</ol></li>
</ul>

<p>Then the algorithm to calculate lower and upper bounds for exp(1), given <em>d</em>, is as follows:</p>

<ol>
<li>Set <em>i</em> to 0, then run the <strong>setup</strong>.</li>
<li><strong>Get the numerator for convergent <em>i</em></strong>, call it <em>c</em>. If <em>c</em> is less than 10<sup><em>d</em></sup>, add 1 to <em>i</em> and repeat this step.  Otherwise, go to the next step.</li>
<li><strong>Get convergent <em>i</em> &minus; 1</strong> and <strong>get semiconvergent <em>i</em> &minus; 1 given <em>d</em></strong>, call them <em>conv</em> and <em>semi</em>, respectively.</li>
<li>If (<em>i</em> &minus; 1) is odd[^24], return <em>semi</em> as the lower bound and <em>conv</em> as the upper bound.  Otherwise, return <em>conv</em> as the lower bound and <em>semi</em> as the upper bound.</li>
</ol>

<p><a id=Notes></a></p>

<h2>Notes</h2>

<p>[^1]: Nacu, Şerban, and Yuval Peres. &quot;<a href="https://projecteuclid.org/euclid.aoap/1106922322"><strong>Fast simulation of new coins from old</strong></a>&quot;, The Annals of Applied Probability 15, no. 1A (2005): 93-115.</p>

<p>[^2]: In this case, an algorithm to simulate <code>High</code>(<em>&lambda;</em>) is: Flip the input coin <em>n</em> times or until a flip returns 1, whichever comes first, then output the last coin flip result.</p>

<p>[^3]: E. Voronovskaya, &quot;Détermination de la forme asymptotique d&#39;approximation des fonctions par les polynômes de M. Bernstein&quot;, 1932.</p>

<p>[^4]: G.G. Lorentz, &quot;Approximation of functions&quot;, 1966.</p>

<p>[^5]: Mathé, Peter. “Approximation of Hölder Continuous Functions by Bernstein Polynomials.” The American Mathematical Monthly 106, no. 6 (1999): 568–74. <a href="https://doi.org/10.2307/2589469."><strong>https://doi.org/10.2307/2589469.</strong></a></p>

<p>[^6]: Micchelli, Charles. &quot;The saturation class and iterates of the Bernstein polynomials.&quot; Journal of Approximation Theory 8, no. 1 (1973): 1-18.</p>

<p>[^7]: Guan, Zhong. &quot;<a href="https://arxiv.org/abs/0909.0684"><strong>Iterated Bernstein polynomial approximations</strong></a>&quot;, arXiv:0909.0684 (2009).</p>

<p>[^8]: Güntürk, C.S., Li, W., &quot;<a href="https://arxiv.org/abs/2112.09181"><strong>Approximation of functions with one-bit neural networks</strong></a>&quot;, arXiv:2112.09181 [cs.LG], 2021.</p>

<p>[^9]: Güntürk and Li 2021 defines the $C^n$ norm as the maximum absolute value of $f(\lambda)$ and its <em>n</em>-th derivative where $0\le \lambda\le 1$, but the bounds would then be false in general.  One counterexample is $2\lambda(1-\lambda)$, and another is $(\sin(\lambda)+2\lambda(1-\lambda))/2$.</p>

<p>[^10]: More generally, the coefficients can be real numbers, but there are computational issues.  Rational numbers more easily support arbitrary precision than other real numbers, where special measures are required such as so-called constructive/recursive reals.</p>

<p>[^11]: Carvalho, Luiz Max, and Guido A. Moreira. &quot;<a href="https://arxiv.org/abs/2202.06121"><strong>Adaptive truncation of infinite sums: applications to Statistics</strong></a>&quot;, arXiv:2202.06121 (2022).</p>

<p>[^12]: S. Ray, P.S.V. Nataraj, &quot;A Matrix Method for Efficient Computation of Bernstein Coefficients&quot;, Reliable Computing 17(1), 2012.</p>

<p>[^13]: Kawamura, Akitoshi, Norbert Müller, Carsten Rösnick, and Martin Ziegler. &quot;<a href="https://doi.org/10.1016/j.jco.2015.05.001"><strong>Computational benefit of smoothness: Parameterized bit-complexity of numerical operators on analytic functions and Gevrey’s hierarchy</strong></a>.&quot; Journal of Complexity 31, no. 5 (2015): 689-714.</p>

<p>[^14]: M. Gevrey, &quot;Sur la nature analytique des solutions des équations aux dérivées partielles&quot;, 1918.</p>

<p>[^15]: Henderson, S.G., Glynn, P.W., &quot;Nonexistence of a class of variate generation schemes&quot;, <em>Operations Research Letters</em> 31 (2003).</p>

<p>[^16]: For this approximation, if <em>n</em> were infinity, the method would return 1 with probability 1 and so would not approximate <em>&lambda;</em>*<em>c</em>, of course.</p>

<p>[^17]: Goyal, V. and Sigman, K., 2012. On simulating a class of Bernstein polynomials. ACM Transactions on Modeling and Computer Simulation (TOMACS), 22(2), pp.1-5.</p>

<p>[^18]: Flajolet, P., Pelletier, M., Soria, M., &quot;<a href="https://arxiv.org/abs/0906.5560"><strong>On Buffon machines and numbers</strong></a>&quot;, arXiv:0906.5560 [math.PR], 2010.</p>

<p>[^19]: Mendo, Luis. &quot;An asymptotically optimal Bernoulli factory for certain functions that can be expressed as power series.&quot; Stochastic Processes and their Applications 129, no. 11 (2019): 4366-4384.</p>

<p>[^20]: Holtz, O., Nazarov, F., Peres, Y., &quot;New Coins from Old, Smoothly&quot;, <em>Constructive Approximation</em> 33 (2011).</p>

<p>[^21]: Thomas, A.C., Blanchet, J., &quot;<a href="https://arxiv.org/abs/1106.2508v3"><strong>A Practical Implementation of the Bernoulli Factory</strong></a>&quot;, arXiv:1106.2508v3  [stat.AP], 2012.</p>

<p>[^22]: Flajolet, P., Pelletier, M., Soria, M., &quot;<a href="https://arxiv.org/abs/0906.5560"><strong>On Buffon machines and numbers</strong></a>&quot;, arXiv:0906.5560  [math.PR], 2010</p>

<p>[^23]: Mossel, Elchanan, and Yuval Peres. New coins from old: computing with unknown bias. Combinatorica, 25(6), pp.707-724, 2005.</p>

<p>[^24]: &quot;<em>x</em> is odd&quot; means that <em>x</em> is an integer and not divisible by 2.  This is true if <em>x</em> &minus; 2*floor(<em>x</em>/2) equals 1, or if <em>x</em> is an integer and the least significant bit of abs(<em>x</em>) is 1.</p>

<p>[^25]: &quot;<em>x</em> is even&quot; means that <em>x</em> is an integer and divisible by 2.  This is true if <em>x</em> &minus; 2*floor(<em>x</em>/2) equals 0, or if <em>x</em> is an integer and the least significant bit of abs(<em>x</em>) is 0.</p>

<p>[^26]: Citterio, M., Pavani, R., &quot;A Fast Computation of the Best k-Digit Rational Approximation to a Real Number&quot;, Mediterranean Journal of Mathematics 13 (2016).</p>

<p>[^27]: Łatuszyński, K., Kosmidis, I., Papaspiliopoulos, O., Roberts, G.O., &quot;<a href="https://arxiv.org/abs/0907.4018v2"><strong>Simulating events of unknown probabilities via reverse time martingales</strong></a>&quot;, arXiv:0907.4018v2 [stat.CO], 2009/2011.</p>

<p>[^28]: Qian, Weikang, Marc D. Riedel, and Ivo Rosenberg. &quot;Uniform approximation and Bernstein polynomials with coefficients in the unit interval.&quot; European Journal of Combinatorics 32, no. 3 (2011): 448-463.</p>

<p>[^29]: Li, Zhongkai. &quot;Bernstein polynomials and modulus of continuity.&quot; Journal of Approximation Theory 102, no. 1 (2000): 171-174.</p>

<p>[^30]: <em>Summation notation</em>, involving the Greek capital sigma (&Sigma;), is a way to write the sum of one or more terms of similar form. For example, $\sum_{k=0}^n g(k)$ means $g(0)+g(1)+...+g(n)$, and $\sum_{k\ge 0} g(k)$ means $g(0)+g(1)+...$.</p>

<p>[^31]: Skorski, Maciej. &quot;<a href="https://arxiv.org/abs/2012.06270"><strong>Handy formulas for binomial moments</strong></a>&quot;, arXiv:2012.06270 (2020).</p>

<p>[^32]: Keane,  M.  S.,  and  O&#39;Brien,  G.  L., &quot;A Bernoulli factory&quot;, <em>ACM Transactions on Modeling and Computer Simulation</em> 4(2), 1994.</p>

<p>[^33]: von Neumann, J., &quot;Various techniques used in connection with random digits&quot;, 1951.</p>

<p>[^34]: Peres, Y., &quot;<a href="https://projecteuclid.org/euclid.aos/1176348543"><strong>Iterating von Neumann&#39;s procedure for extracting random bits</strong></a>&quot;, Annals of Statistics 1992,20,1, p. 590-597.</p>

<p>[^35]: Knuth, Donald E. and Andrew Chi-Chih Yao. &quot;The complexity of nonuniform random number generation&quot;, in <em>Algorithms and Complexity: New Directions and Recent Results</em>, 1976.</p>

<p>[^36]: Mossel, Elchanan, and Yuval Peres. New coins from old: computing with unknown bias. Combinatorica, 25(6), pp.707-724.</p>

<p>[^37]: S. Pae, &quot;<a href="https://arxiv.org/abs/1602.06058v2"><strong>Binarization Trees and Random Number Generation</strong></a>&quot;, arXiv:1602.06058v2 [cs.DS], 2018.</p>

<p>[^38]: Levy, H., <em>Stochastic dominance</em>, 1998.</p>

<p>[^39]: Henry (<a href="https://math.stackexchange.com/users/6460/henry">https://math.stackexchange.com/users/6460/henry</a>), Proving stochastic dominance for hypergeometric random variables, URL (version: 2021-02-20): <a href="https://math.stackexchange.com/q/4033573"><strong>https://math.stackexchange.com/q/4033573</strong></a> .</p>

<p>[^40]: Gal, S.G., &quot;Calculus of the modulus of continuity for nonconcave functions and applications&quot;, <em>Calcolo</em> 27 (1990)</p>

<p>[^41]: Gal, S.G., 1995. Properties of the modulus of continuity for monotonous convex functions and applications. <em>International Journal of Mathematics and Mathematical Sciences</em> 18(3), pp.443-446.</p>

<p>[^42]: Anastassiou, G.A., Gal, S.G., <em>Approximation Theory: Moduli of Continuity and Global Smoothness Preservation</em>, Birkhäuser, 2012.</p>

<p>[^43]: This formula applies to functions with Lipschitz-continuous derivative (a weaker assumption than having three continuous derivatives), but that derivative&#39;s Lipschitz constant is a lower bound on $M_{1,3}$, so that formula is useful here.</p>

<p>[^44]: Le Gruyer, Erwan. &quot;Minimal Lipschitz extensions to differentiable functions defined on a Hilbert space.&quot; Geometric and Functional Analysis 19, no. 4 (2009): 1101-1118.</p>

<p>[^45]: Herbert-Voss, Ariel, Matthew J. Hirn, and Frederick McCollum. &quot;Computing minimal interpolants in C1, 1 (Rd).&quot; Rev. Mat. Iberoam 33, no. 1 (2017): 29-66.</p>

<p>[^46]: Banderier, C. And Drmota, M., 2015. Formulae and asymptotics for coefficients of algebraic functions. Combinatorics, Probability and Computing, 24(1), pp.1-53.</p>

<p>[^47]: Icard, Thomas F., &quot;Calibrating generative models: The probabilistic Chomsky–Schützenberger hierarchy&quot;, <em>Journal of Mathematical Psychology</em> 95 (2020): 102308.</p>

<p>[^48]: Dughmi, Shaddin, Jason Hartline, Robert D. Kleinberg, and Rad Niazadeh. &quot;Bernoulli Factories and Black-box Reductions in Mechanism Design.&quot; Journal of the ACM (JACM) 68, no. 2 (2021): 1-30.</p>

<p>[^49]: Etessami, K. and Yannakakis, M., &quot;Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations&quot;, <em>Journal of the ACM</em> 56(1), pp.1-66, 2009.</p>

<p>[^50]: Flajolet, P., Pelletier, M., Soria, M., &quot;<a href="https://arxiv.org/abs/0906.5560"><strong>On Buffon machines and numbers</strong></a>&quot;, arXiv:0906.5560  [math.PR], 2010.</p>

<p>[^51]: Esparza, J., Kučera, A. and Mayr, R., 2004, July. Model checking probabilistic pushdown automata. In <em>Proceedings of the 19th Annual IEEE Symposium on Logic in Computer Science</em>, 2004. (pp. 12-21). IEEE.</p>

<p>[^52]: Elder, Murray, Geoffrey Lee, and Andrew Rechnitzer. &quot;Permutations generated by a depth 2 stack and an infinite stack in series are algebraic.&quot; <em>Electronic Journal of Combinatorics</em> 22(1), 2015.</p>

<p>[^53]: Vatan, F., &quot;Distribution functions of probabilistic automata&quot;, in <em>Proceedings of the thirty-third annual ACM symposium on Theory of computing (STOC &#39;01)</em>, pp. 684-693, 2001.</p>

<p>[^54]: Kindler, Guy and D. Romik, &quot;On distributions computable by random walks on graphs,&quot; <em>SIAM Journal on Discrete Mathematics</em> 17 (2004): 624-633.</p>

<p>[^55]: Vatan (2001) claims that a finite-state generator has a continuous <code>CDF</code> (unless it produces a single value with probability 1), but this is not necessarily true if the generator has a state that outputs 0 forever.</p>

<p>[^56]: Adamczewski, B., Cassaigne, J. and Le Gonidec, M., 2020. On the computational complexity of algebraic numbers: the Hartmanis–Stearns problem revisited. Transactions of the American Mathematical Society, 373(5), pp.3085-3115.</p>

<p>[^57]: Cobham, A., &quot;On the Hartmanis-Stearns problem for a class of tag machines&quot;, in <em>IEEE Conference Record of 1968 Ninth Annual Symposium on Switching and Automata Theory</em> 1968.</p>

<p>[^58]: Adamczewski, B., Bugeaud, Y., &quot;On the complexity of algebraic numbers I. Expansions in integer bases&quot;, <em>Annals of Mathematics</em> 165 (2007).</p>

<p>[^59]: Richman, F. (2012). Algebraic functions, calculus style. Communications in Algebra, 40(7), 2671-2683.</p>

<p>[^60]: choose(<em>n</em>, <em>k</em>) = (1*2*3*...*<em>n</em>)/((1*...*<em>k</em>)*(1*...*(<em>n</em>&minus;<em>k</em>))) =  <em>n</em>!/(<em>k</em>! * (<em>n</em> &minus; <em>k</em>)!) is a <em>binomial coefficient</em>, or the number of ways to choose <em>k</em> out of <em>n</em> labeled items.  It can be calculated, for example, by calculating <em>i</em>/(<em>n</em>&minus;<em>i</em>+1) for each integer <em>i</em> in the interval [<em>n</em>&minus;<em>k</em>+1, <em>n</em>], then multiplying the results (Yannis Manolopoulos. 2002. &quot;<a href="https://doi.org/10.1145/820127.820168"><strong>Binomial coefficient computation: recursion or iteration?</strong></a>&quot;, SIGCSE Bull. 34, 4 (December 2002), 65–67).  For every <em>m</em>&gt;0, choose(<em>m</em>, 0) = choose(<em>m</em>, <em>m</em>) = 1 and choose(<em>m</em>, 1) = choose(<em>m</em>, <em>m</em>&minus;1) = <em>m</em>; also, in this document, choose(<em>n</em>, <em>k</em>) is 0 when <em>k</em> is less than 0 or greater than <em>n</em>.</p>

<p>[^61]: <em>n</em>! = 1*2*3*...*<em>n</em> is also known as <em>n</em> factorial; in this document, (0!) = 1.</p>

<p>[^62]: <em>Summation notation</em>, involving the Greek capital sigma (&Sigma;), is a way to write the sum of one or more terms of similar form. For example, $\sum_{k=0}^n g(k)$ means $g(0)+g(1)+...+g(n)$, and $\sum_{k\ge 0} g(k)$ means $g(0)+g(1)+...$.</p>

<p><a id=Appendix></a></p>

<h2>Appendix</h2>

<p>&nbsp;</p>

<p><a id=Proofs_on_Cutting_Off_a_Power_Series></a></p>

<h3>Proofs on Cutting Off a Power Series</h3>

<p><strong>Lemma A1:</strong> Let&mdash; $$f(x)=a_0 x^0 + a_1 x^1 + ...,$$ where the $a_i$ are constants each 0 or greater and sum to a finite value and where $0\le x\le 1$ (the domain is the closed unit interval). Then $f$ is convex and has a maximum at 1.</p>

<p><em>Proof:</em> By inspection, $f(x)$ is a power series and is nonnegative on the positive real line (and thus wherever $0\le x\le 1$).  Each of its terms has a maximum at 1 since&mdash;</p>

<ul>
<li>for $n=0$, $a_0 x^0=a_0$ is a non-negative constant (which trivially reaches its maximum at 1), and</li>
<li>for each $n$ where $a_0 = 0$, $a_0 x^n$ is the constant 0 (which trivially reaches its maximum at 1), and</li>
<li>for each other $n$, $x^n$ is a strictly increasing function and multiplying that by $a_n$ (a positive constant) doesn&#39;t change whether it&#39;s strictly increasing.</li>
</ul>

<p>Since all of these terms have a maximum at 1 on the domain, so does their sum.</p>

<p>The derivative of $f$ is&mdash; $$f&#39;(x) = a_1 x^0 + ... + a_i x^{i-1} + ...,$$ which is still a power series with nonnegative values of $a_n$, so the proof so far applies to $f&#39;$ instead of $f$.  By induction, the proof so far applies to all derivatives of $f$, including its second derivative.</p>

<p>Now, since the second derivative is nonnegative on the positive real line, and thus on its domain, $f$ is convex, which completes the proof. &#x25a1;</p>

<p><strong>Proposition A2:</strong> For a function $f(x)$ as in Lemma A1, let&mdash; $$g_n(x)=a_0 x^0 + ... + a_n x^n,$$ and have the same domain as $f$.  Then for every $n\ge 1$, $g_n(x)$ is within $\epsilon$ of $f(x)$, where $\epsilon = f(1) - g_n(1)$.</p>

<p><em>Proof:</em> $g_n$, consisting of the first $n+1$ terms of $f$, is a power series with nonnegative coefficients, so by Lemma A1, it has a maximum at 1.  The same is true for $f-g_n$, consisting of the remaining terms of $f$.  Since the latter has a maximum at 1, the maximum error is $\epsilon = f(1)-g_n(1)$. &#x25a1;</p>

<p>For a function $f$ described in Lemma A1, $f(1)=a_0 1^0 + a_1 1^1 + ... = a_0 + a_1+...$, and $f$&#39;s error behavior is described at the point 1, the algorithms given in Carvalho and Moreira (2022)[^11] &mdash; which apply to infinite sums &mdash; can be used to &quot;cut off&quot; $f$ at a certain number of terms and do so with a controlled error.</p>

<p><a id=Results_Used_in_Approximate_Bernoulli_Factories></a></p>

<h3>Results Used in Approximate Bernoulli Factories</h3>

<p><strong>Proposition B1</strong>: Let $f(\lambda)$ map the closed unit interval to itself and be continuous and concave.  Then $W_{n,2}$ and $W_{n,3}$ (as defined in &quot;Approximate Bernoulli Factories for Certain Functions&quot;) are nonnegative on the closed unit interval.</p>

<p><em>Proof:</em> For $W_{n,2}$ it&#39;s enough to prove that $B_n(f)\le f$ for every $n\ge 1$.  This is the case because of Jensen&#39;s inequality and because $f$ is concave.</p>

<p>For $W_{n,3}$ it must also be shown that $B_n(B_n(f(\lambda)))$ is nonnegative.  For this, using only the fact that $f$ maps the closed unit interval to itself, $B_n(f)$ will have Bernstein coefficients in that interval (each coefficient is a value of $f$) and so will likewise map the closed unit interval to itself (Qian et al. 2011)[^28].  Thus, by induction, $B_n(B_n(f(\lambda)))$ is nonnegative.  The discussion for $W_{n,2}$ also shows that $(f - B_n(f))$ is nonnegative as well.  Thus, $W_{n,3}$ is nonnegative on the closed unit interval. &#x25a1;</p>

<p><strong>Proposition B2</strong>: Let $f(\lambda)$ map the closed unit interval to itself, be continuous, nowhere decreasing, and subadditive, and equal 0 at 0. Then $W_{n,2}$ is nonnegative on the closed unit interval.</p>

<p><em>Proof:</em> The assumptions on $f$ imply that $B_n(f)\le 2 f$ (Li 2000)[^29], showing that $W_{n,2}$ is nonnegative on the closed unit interval.  &#x25a1;</p>

<blockquote>
<p><strong>Note:</strong> A subadditive function $f$ has the property that $f(a+b) \le f(a)+f(b)$ whenever $a$, $b$, and $a+b$ are in $f$&#39;s domain.</p>
</blockquote>

<p><strong>Proposition B3</strong>: Let $f(\lambda)$ map the closed unit interval to itself and have a Lipschitz continuous derivative with Lipschitz constant $L$.  If $f(\lambda) \ge \frac{L \lambda(1-\lambda)}{2m}$ on $f$&#39;s domain, for some $m\ge 1$, then $W_{n,2}$ is nonnegative there, for every $n\ge m$.</p>

<p><em>Proof</em>: Let $E(\lambda, n) = \frac{L \lambda(1-\lambda)}{2n}$. Lorentz (1966)[^4] showed that with this Lipschitz derivative assumption on $f$, $B_n$ differs from $f(\lambda)$ by no more than $E(\lambda, n)$ for every $n\ge 1$.  By inspection, $E(\lambda, n)$ is biggest when $n=1$ and decreases as $n$ increases. Assuming the worst case that $B_n(\lambda) = f(\lambda) + E(\lambda, m)$, it follows that $W_{n,2}=2 f(\lambda) - B_n(\lambda)\ge 2 f(\lambda) - f(\lambda) - E(\lambda, m) = f(\lambda) - E(\lambda, m)\ge 0$ whenever $f(\lambda)\ge E(\lambda, m)$.  Because $E(\lambda, k+1)\le E(\lambda,k)$ for every $k\ge 1$, the preceding sentence holds true for every $n\ge m$. &#x25a1;</p>

<p>The following results deal with a useful quantity when discussing the error in approximating a function by Bernstein polynomials.  Suppose a coin shows heads with probability $p$, and $n$ independent tosses of the coin are made.  Then the total number of heads $X$ follows a <em>binomial distribution</em>, and the $r$-th central moment of that distribution is as follows: $$T(n, r, p) = \mathbb{E}[(X-\mathbb{E}[X])^r] = \sum_{k=0}^n (k-np)^r{n \choose k}p^k (1-p)^{n-k},$$ where $\mathbb{E}[.]$ is the expected value (&quot;long-run average&quot;).  The following results bound the absolute value of $T$.[^30]</p>

<p><strong>Result B4</strong> (Molteni 2022)[^39]: If $r$ is an even integer such that $0\le r\le 44$, then $|T(n, r, p)| \le \frac{r!}{((r/2)!)8^{r/2}} n^{r/2}$ for every $n\ge 1$.</p>

<p><strong>Proposition B5</strong>: For every integer $n\ge 1$, the following is true: $$|T(n, 3, p)| \le \frac{\sqrt{3}}{18\sqrt{n}} n^{3/2} \le \frac{\sqrt{3}}{18} n^{3/2} \lt (963/10000) n^{3/2}.$$</p>

<p><em>Proof</em>: The critical points of $T(n, 3, p)$ (the points where the maximum might be) are at $p=0$, $p=1$, $p=1/2-\sqrt{3}/6$, and $p=1/2+\sqrt{3}/6$.  The moment equals 0 at the points 0 and 1, so that leaves the last two.  Since $T(n, r, p)$ is antisymmetric whenever $r$ is odd, and is nonnegative whenever $r$ is odd and $0\le p \le 1/2$ (Skorski 2020)[^31], it&#39;s enough to take the critical point $0 \le p=1/2-\sqrt{3}/6 \le 1/2$ to bound $|T(n, 3, p)|$ on either side.  By inspection, the moment at that critical point is decreasing as $n$ increases, starting with $n=1$. &#x25a1;</p>

<p><strong>Corollary B6</strong>:  For every integer $n_0\ge 1$, $|T(n, 3, p)| \le \frac{\sqrt{3}}{18\sqrt{n_0}} n^{3/2} &lt; (963/10000)\frac{1}{\sqrt{n_0}} n^{3/2}$ whenever $n\ge n_0$.</p>

<p><strong>Proposition B7</strong>:  For every integer $n\ge 1$, $|T(n, 5, p)| \le 0.083 n^{5/2}.$  For every integer $n\ge 304$, $|T(n, 5, p)| \le n^2 \le 0.05736 n^{5/2}.$</p>

<p><em>Proof</em>: Evaluating the moment for each $1\le n \le 303$ at its critical point shows that $|T(n,5,p)| &lt; 0.083 n^{5/2}$ for every such $n$.  An upper bound given in sec. 3.1 of Skorski (2020) leads to $|T(n,5,p)| \le n/4+2 {n \choose 2} = n/4+2\frac{n!}{(n-2)!} = n^2 - \frac{3}{4}n \le n^2$ whenever $n\ge 2$, and $n^2/n^{5/2}$ is decreasing as $n$ increases, starting with $n=2$, because its derivative $\frac{-n}{2n^{5/2}}$ is negative whenever $n\ge 2$. Thus it&#39;s enough to take the bound $n^2$ at 304, namely 92188, so that $|T(n,5,p)|\le 304^2 = 92188 &lt; 0.05736/n^{5/2}$ for every $n\ge 304$.  This is still less than $0.083 n^{5/2}$, so that bound stands for the first part.  &#x25a1;</p>

<p><a id=Failures_of_the_Consistency_Requirement></a></p>

<h3>Failures of the Consistency Requirement</h3>

<p>In the academic literature (papers and books), there are many results showing that a polynomial comes within a given error bound of a function <em>f</em>(<em>&lambda;</em>), when <em>f</em> meets certain conditions.  Unfortunately, these error bounds don&#39;t necessarily mean that a sequence of polynomials far from these bounds will obey the consistency requirement, a requirement for simulating <em>f</em> in the Bernoulli factory setting.</p>

<p>Here is one such error bound. Let <em>f</em> have a Lipschitz continuous derivative on the closed unit interval with Lipschitz constant <em>M</em>.  Then the <em>Bernstein polynomial</em> for <em>f</em> of degree <em>n</em> (which is in Bernstein form with coefficients $f(k/n)$ with $0\le k\le n$) is within <em>M</em>*<em>x</em>*(1&minus;<em>x</em>)/(2*<em>n</em>) of <em>f</em> (and thus within <em>M</em>/(8*<em>n</em>) of <em>f</em>) whenever $0\le x\le 1$ (Lorentz 1966)[^4]. Thus, for every <em>n</em>&ge;1:</p>

<ul>
<li><strong>fabove</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) + <em>M</em> / (8*<em>n</em>).</li>
<li><strong>fbelow</strong>(<em>n</em>, <em>k</em>) = <em>f</em>(<em>k</em>/<em>n</em>) &minus; <em>M</em> / (8*<em>n</em>).</li>
</ul>

<p>Where <em>k</em> is an integer and 0 &le; <em>k</em> &le; <em>n</em>.</p>

<p>The example against the consistency requirement involves the function <em>g</em>(<em>&lambda;</em>) = sin(<em>&pi;</em>*<em>&lambda;</em>)/4 + 1/2, which has a Lipschitz continuous derivative.</p>

<p>For <em>g</em>, the coefficients for&mdash;</p>

<ul>
<li>the degree-2 upper polynomial in Bernstein form (<strong>fabove</strong>(5, <em>k</em>)) are [0.6542..., 0.9042..., 0.6542...], and</li>
<li>the degree-4 upper polynomial in Bernstein form (<strong>fabove</strong>(6, <em>k</em>)) are [0.5771..., 0.7538..., 0.8271..., 0.7538..., 0.5771...].</li>
</ul>

<p>The degree-2 polynomial lies above the degree-4 polynomial everywhere in the closed unit interval.  However, to ensure consistency, the degree-2 polynomial, once elevated to degree 4 and rewritten in Bernstein form, must have coefficients that are greater than or equal to those of the degree-4 polynomial.</p>

<ul>
<li>Once elevated to degree 4, the degree-2 polynomial&#39;s coefficients are [0.6542..., 0.7792..., 0.8208..., 0.7792..., 0.6542...].</li>
</ul>

<p>As can be seen, the elevated polynomial&#39;s coefficient 0.8208... is less than the corresponding coefficient 0.8271... for the degree-4 polynomial.</p>

<p><strong>Note on &quot;clamping&quot;.</strong> In addition, for a polynomial-building scheme, &quot;clamping&quot; the values of <strong>fbelow</strong> and <strong>fabove</strong> to fit the closed unit interval won&#39;t necessarily preserve the consistency requirement, even if the original scheme met that requirement.  Here is an example that applies to any scheme.</p>

<p>Let <em>g</em> and <em>h</em> be two polynomials in Bernstein form as follows:</p>

<ul>
<li><em>g</em> has degree 5 and coefficients [10179/10000, 2653/2500, 9387/10000, 5049/5000, 499/500, 9339/10000].</li>
<li><em>h</em> has degree 6 and coefficients [10083/10000, 593/625, 9633/10000, 4513/5000, 4947/5000, 9473/10000, 4519/5000].</li>
</ul>

<p>After elevating <em>g</em>&#39;s degree, <em>g</em>&#39;s coefficients are no less than <em>h</em>&#39;s, as required by the consistency property.</p>

<p>However, by clamping coefficients above 1 to equal 1, so that <em>g</em> is now <em>g&prime;</em> with [1, 1, 9387/10000, 1, 499/500, 9339/10000] and <em>h</em> is now <em>h&prime;</em> with [1, 593/625, 9633/10000, 4513/5000, 4947/5000, 9473/10000, 4519/5000], and elevate <em>g&prime;</em> for coefficients [1, 1, 14387/15000, 19387/20000, 1499/1500, 59239/60000, 9339/10000], some of the coefficients of <em>g&prime;</em> are less than those of <em>h&prime;</em>.  Thus, for this pair of polynomials, clamping the coefficients will destroy the consistency property.</p>

<p><a id=Which_functions_admit_a_Bernoulli_factory></a></p>

<h3>Which functions admit a Bernoulli factory?</h3>

<p>Let <em>f</em>(<em>&lambda;</em>) be a function whose domain is the closed unit interval or a subset of it, and that maps its domain to the closed unit interval.  The domain of <em>f</em> gives the allowable values of <em>&lambda;</em>, which is the input coin&#39;s probability of heads.</p>

<p><em>f</em> admits a Bernoulli factory if and only if <em>f</em> is constant on its domain, or is continuous and <em>polynomially bounded</em> on its domain, as defined later in the section &quot;Proofs for Polynomial-Building Schemes&quot; (Keane and O&#39;Brien 1994)[^32].</p>

<p>If <em>f</em>(<em>&lambda;</em>) meets these sufficient conditions, it admits a Bernoulli factory and is Hölder continuous (see &quot;<a href="#Definitions"><strong>Definitions</strong></a>&quot;):</p>

<ul>
<li><em>f</em>(<em>&lambda;</em>) maps the closed unit interval to [0, 1].</li>
<li><em>f</em>(<em>&lambda;</em>) is continuous.</li>
<li>0 &lt; <em>f</em>(<em>&lambda;</em>) &lt; 1 whenever 0 &lt; <em>&lambda;</em> &lt; 1.</li>
<li><em>f</em>(<em>&lambda;</em>) is algebraic over rational numbers (that is, there is a nonzero polynomial <em>P</em>(<em>x</em>, <em>y</em>) in two variables and whose coefficients are rational numbers, such that <em>P</em>(<em>x</em>, <em>f</em>(<em>x</em>)) = 0 for every <em>x</em> in the domain of <em>f</em>).</li>
</ul>

<p>A <a href="https://mathoverflow.net/a/395018/171320"><strong>proof by Reid Barton</strong></a> begins by showing that <em>f</em> is a <em>semialgebraic function</em>, so that by a known inequality and the other conditions, it meets the definitions of being Hölder continuous and polynomially bounded.</p>

<p><a id=Which_functions_don_t_require_outside_randomness_to_simulate></a></p>

<h3>Which functions don&#39;t require outside randomness to simulate?</h3>

<p>The function <em>f</em>(<em>&lambda;</em>) is <em>strongly simulable</em> if it admits a Bernoulli factory algorithm that uses nothing but the input coin as a source of randomness (Keane and O&#39;Brien 1994)[^32].  See &quot;<a href="https://peteroupc.github.io/bernoulli.html#Randomized_vs_Non_Randomized_Algorithms"><strong>Randomized vs. Non-Randomized Algorithms</strong></a>&quot;.</p>

<p><strong>Strong Simulability Statement.</strong> A function <em>f</em>(<em>&lambda;</em>) is strongly simulable only if&mdash;</p>

<ol>
<li><em>f</em> is constant on its domain, or is continuous and polynomially bounded on its domain, and</li>
<li><em>f</em> maps the closed unit interval or a subset of it to the closed unit interval, and</li>
<li><em>f</em>(0) equals 0 or 1 whenever 0 is in the domain of <em>f</em>, and</li>
<li><em>f</em>(1) equals 0 or 1 whenever 1 is in the domain of <em>f</em>.</li>
</ol>

<p>Keane and O&#39;Brien already showed that <em>f</em> is strongly simulable if conditions 1 and 2 are true and neither 0 nor 1 are included in the domain of <em>f</em>.  Conditions 3 and 4 are required because <em>&lambda;</em> (the probability of heads) can be 0 or 1 so that the input coin returns 0 or 1, respectively, every time.  This is called a &quot;degenerate&quot; coin.  When given just a degenerate coin, no algorithm can produce one value with probability greater than 0, and another value with the opposite probability.  Rather, the algorithm can only produce a constant value with probability 1.  In the Bernoulli factory problem, that constant is either 0 or 1, so a Bernoulli factory algorithm for <em>f</em> must return 1 with probability 1, or 0 with probability 1, when given just a degenerate coin and no outside randomness, resulting in conditions 3 and 4.</p>

<p>To show that <em>f</em> is strongly simulable, it&#39;s enough to show that there is a Bernoulli factory for <em>f</em> that must flip the input coin and get 0 and 1 before it uses any outside randomness.</p>

<p><strong>Proposition 1.</strong> <em>If f(&lambda;) is described in the strong simulability statement and is a polynomial with computable coefficients, it is strongly simulable.</em></p>

<p><em>Proof:</em> If <em>f</em> is the constant 0 or 1, the proof is trivial: simply return 0 or 1, respectively.</p>

<p>Otherwise: Let <em>a</em>[<em>j</em>] be the <em>j</em><sup>th</sup> coefficient of the polynomial in Bernstein form.  Consider the following algorithm, modified from (Goyal and Sigman 2012)[^17].</p>

<ol>
<li>Flip the input coin <em>n</em> times, and let <em>j</em> be the number of times the coin returned 1 this way.</li>
<li>If 0 is in the domain of <em>f</em> and if <em>j</em> is 0, return <em>f</em>(0). (By condition 3, <em>f</em>(0) must be either 0 or 1.)</li>
<li>If 1 is in the domain of <em>f</em> and if <em>j</em> is <em>n</em>, return <em>f</em>(1). (By condition 4, <em>f</em>(1) must be either 0 or 1.)</li>
<li>With probability <em>a</em>[<em>j</em>], return 1.  Otherwise, return 0. (For example, generate a uniformly distributed random variate, greater than 0 and less than 1, then return 1 if that variate is less than <em>a</em>[<em>j</em>], or 0 otherwise.  <em>a</em>[<em>j</em>] is the coefficient <em>j</em> of the polynomial written in Bernstein form), or 0 otherwise.</li>
</ol>

<p>(By the properties of the Bernstein form, <em>a</em>[0] will equal <em>f</em>(0) and <em>a</em>[<em>n</em>] will equal <em>f</em>(1) whenever 0 or 1 is in the domain of <em>f</em>, respectively.)</p>

<p>Step 4 is done by first generating unbiased bits (such as with the von Neumann trick of flipping the input coin twice until the flip returns 0 then 1 or 1 then 0 this way, then taking the result as 0 or 1, respectively (von Neumann 1951)[^33]), then using the algorithm in &quot;<a href="https://peteroupc.github.io/bernoulli.html#Digit_Expansion"><strong>Digit Expansions</strong></a>&quot; to produce the probability <em>a</em>[<em>j</em>].  The algorithm computes <em>a</em>[<em>j</em>] bit by bit and compares the computed value with the generated bits.  Since the coin returned both 0 and 1 in step 1 earlier in the algorithm, we know the coin isn&#39;t degenerate, so that step 4 will finish with probability 1.  Now, since the Bernoulli factory used only the input coin for randomness, this shows that <em>f</em> is strongly simulable. &#x25a1;</p>

<p><strong>Proposition 2.</strong> <em>If f(&lambda;) is described in the strong simulability statement, and if either f is constant on its domain or f meets the additional conditions below, then f is strongly simulable.</em></p>

<ol>
<li><em>If f(0) = 0 or f(1) = 0 or both, then there is a polynomial g(&lambda;) in Bernstein form whose coefficients are computable and in the closed unit interval, such that g(0) = f(0) and g(1) = f(1) whenever 0 or 1, respectively, is in the domain of f, and such that g(&lambda;) &gt; f(&lambda;) for every &lambda; in the domain of f, except at 0 and 1.</em></li>
<li><em>If f(0) = 1 or f(1) = 1 or both, then there is a polynomial h(&lambda;) in Bernstein form whose coefficients are computable and in the closed unit interval, such that h(0) = f(0) and h(1) = f(1) whenever 0 or 1, respectively, is in the domain of f, and such that h(&lambda;) &lt; f(&lambda;) for every &lambda; in the domain of f, except at 0 and 1.</em></li>
</ol>

<p><strong>Lemma 1.</strong> <em>If f(&lambda;) is described in the strong simulability statement and meets the additional condition below, then f is strongly simulable.</em></p>

<ul>
<li><em>There is a polynomial g(&lambda;) in Bernstein form whose coefficients are computable and in the closed unit interval, such that g(0) = f(0) and g(1) = f(1) whenever 0 or 1, respectively, is in the domain of f, and such that g(&lambda;) &gt; f(&lambda;) for every &lambda; in the domain of f, except at 0 and 1.</em></li>
</ul>

<p><em>Proof:</em>  Consider the following algorithm.</p>

<ol>
<li>If <em>f</em> is 0 everywhere in its domain or 1 everywhere in its domain, return 0 or 1, respectively.</li>
<li><p>Otherwise, use the algorithm given in Proposition 1 to simulate <em>g</em>(<em>&lambda;</em>).  If the algorithm returns 0, return 0. By the additional condition in the lemma, 0 will be returned if <em>&lambda;</em> is either 0 or 1.</p>

<p>Now, we know that the input coin&#39;s probability of heads is neither 0 nor 1.</p>

<p>By the conditions in the lemma, both <em>f</em>(<em>&lambda;</em>)&gt;0 and <em>g</em>(<em>&lambda;</em>)&gt;0 whenever 0 &lt; <em>&amp;lambda</em> &lt; 1 and <em>&lambda;</em> is in <em>f</em>&#39;s domain.</p>

<p>Now let <em>h</em>(<em>&lambda;</em>) = <em>f</em>(<em>&lambda;</em>)/<em>g</em>(<em>&lambda;</em>).  By the conditions in the lemma, <em>h</em> will be positive everywhere in that interval.</p></li>
<li><p>Return 1 if <em>h</em> has the following property: <em>h</em>(<em>&lambda;</em>) = 0 whenever 0 &lt; <em>&lambda;</em> &lt; 1 and <em>&lambda;</em> is in <em>f</em>&#39;s domain.</p></li>
<li>Otherwise, we run a Bernoulli factory algorithm for <em>h</em>(<em>&lambda;</em>) that uses the input coin (and possibly outside randomness).  Since <em>h</em> is continuous and polynomially bounded and the input coin&#39;s probability of heads is neither 0 nor 1, <em>h</em> is strongly simulable; we can replace the outside randomness in the algorithm with unbiased random bits via the von Neumann trick.</li>
</ol>

<p>Thus, <em>f</em> admits an algorithm that uses nothing but the input coin as a source of randomness, and so is strongly simulable. &#x25a1;</p>

<p><strong>Lemma 2.</strong> <em>If f(&lambda;) is described in the strong simulability statement and meets the additional conditions below, then f is strongly simulable.</em></p>

<ol>
<li><em>There are two polynomials g(&lambda;) and &omega;(&lambda;) in Bernstein form, such that both polynomials&#39; coefficients are computable and all in the closed unit interval.</em></li>
<li><em>g(0) = &omega;(0) = f(0) = 0 (so that 0 is in the domain of f).</em></li>
<li><em>g(1) = &omega;(1) = f(1) = 1 (so that 1 is in the domain of f).</em></li>
<li><em>For every &lambda; in the domain of f, except at 0 and 1, g(&lambda;) &gt; f(&lambda;).</em></li>
<li><em>For every &lambda; in the domain of f, except at 0 and 1, &omega;(&lambda;) &lt; f(&lambda;).</em></li>
</ol>

<p><em>Proof:</em> First, assume <em>g</em> and <em>&omega;</em> have the same degree.  If not, elevate the degree of the polynomial with lesser degree to have the same degree as the other.</p>

<p>Now, let <em>g</em>[<em>j</em>] and <em>&omega;</em>[<em>j</em>] be the <em>j</em><sup>th</sup> coefficient of the polynomial <em>g</em> or <em>&omega;</em>, respectively, in Bernstein form.  Consider the following algorithm, which is similar to the algorithm in Proposition 1.</p>

<ol>
<li>Flip the input coin <em>n</em> times, and let <em>j</em> be the number of times the coin returned 1 this way.</li>
<li>If 0 is in the domain of <em>f</em> and if <em>j</em> is 0, return <em>g</em>(0) = <em>&omega;</em>(0) = 0.</li>
<li>If 1 is in the domain of <em>f</em> and if <em>j</em> is <em>n</em>, return <em>g</em>(1) = <em>&omega;</em>(1) = 0.</li>
<li>Generate a uniformly distributed random variate, greater than 0 and less than 1, then return 1 if that variate is less than <em>&omega;</em>[<em>j</em>], or return 0 if that variate is greater than <em>g</em>[<em>j</em>].  This step is carried out via the von Neumann method, as in Proposition 1.</li>
</ol>

<p>If the algorithm didn&#39;t return a value, then by now we know that the input coin&#39;s probability of heads is neither 0 nor 1, since step 2 returned a value (either 0 or 1), which can only happen if the input coin didn&#39;t return all zeros or all ones.</p>

<p>Now let <em>r</em>(<em>&lambda;</em>) = (<em>f</em>(<em>&lambda;</em>) &minus; <em>&omega;</em>(<em>&lambda;</em>)) / (<em>g</em>(<em>&lambda;</em>) &minus; <em>&omega;</em>(<em>&lambda;</em>)).  By the conditions in the lemma, <em>h</em>(<em>&lambda;</em>) will be positive wherever 0 &lt; <em>&lambda;</em> &lt; 1 and <em>&lambda;</em> is in the domain of <em>f</em>.</p>

<p>Now, run a Bernoulli factory algorithm for <em>r</em>(<em>&lambda;</em>) that uses the input coin (and possibly outside randomness).  Since <em>r</em> is continuous and polynomially bounded and the input coin&#39;s probability of heads is neither 0 nor 1, <em>r</em> is strongly simulable; we can replace the outside randomness in the algorithm with unbiased random bits via the von Neumann trick.</p>

<p>Thus, <em>f</em> admits an algorithm that uses nothing but the input coin as a source of randomness, and so is strongly simulable. &#x25a1;</p>

<p><em>Proof of Proposition 2:</em>  The following cases can occur:</p>

<ol>
<li>If neither 0 nor 1 are in the domain of <em>f</em>, then <em>f</em> is strongly simulable by the discussion above.</li>
<li>If <em>f</em> is 0 everywhere in its domain or 1 everywhere in its domain: Return 0 or 1, respectively.</li>
<li>If 0 but not 1 is in the domain of <em>f</em>: If <em>f</em>(0) = 0, apply Lemma 1.  If <em>f</em>(0) = 1, apply Lemma 1, but take <em>f</em> = 1 &minus; <em>f</em> and return 1 minus the output of the lemma&#39;s algorithm (this will bring <em>f</em>(0) = 0 and satisfy the lemma.)</li>
<li>If 1 but not 0 is in the domain of <em>f</em>: If <em>f</em>(1) = 0, apply Lemma 1.  If <em>f</em>(1) = 1, apply Lemma 1, but take <em>f</em> = 1 &minus; <em>f</em> and return 1 minus the output of the lemma&#39;s algorithm (this will bring <em>f</em>(1) = 0 and satisfy the lemma.)</li>
<li><em>f</em>(0) = <em>f</em>(1) = 0: Apply Lemma 1.</li>
<li><em>f</em>(0) = <em>f</em>(1) = 1: Apply Lemma 1, but take <em>f</em> = 1 &minus; <em>f</em> and return 1 minus the output of the lemma&#39;s algorithm.</li>
<li><em>f</em>(0) = 0 and <em>f</em>(1) = 1: Apply Lemma 2.</li>
<li><em>f</em>(0) = 1 and <em>f</em>(1) = 0: Apply Lemma 2, but take <em>f</em> = 1 &minus; <em>f</em> and return 1 minus the output of the lemma&#39;s algorithm.</li>
</ol>

<p>&#x25a1;</p>

<p><strong>Proposition 3.</strong> <em>If f(&lambda;) is described in the strong simulability statement and is Lipschitz continuous, then f is strongly simulable.</em></p>

<p><strong>Lemma 3.</strong> <em>If f(&lambda;) is described in the strong simulability statement, is Lipschitz continuous, and is such that f(0) = 0 and f(1) = 0 whenever 0 or 1, respectively, is in the domain of f, then f is strongly simulable.</em></p>

<p><em>Proof:</em> If <em>f</em> is 0 everywhere in its domain or 1 everywhere in its domain: Return 0 or 1, respectively.  Otherwise, let&mdash;</p>

<ul>
<li><em>M</em> be the Lipschitz constant of <em>f</em> (its derivative&#39;s maximum absolute value if <em>f</em> is continuous), or a computable number greater than this.</li>
<li><em>l</em> be either 0 if 0 is in the domain of <em>f</em>, or 1 otherwise, and</li>
<li><em>u</em> be either 0 if 1 is in the domain of <em>f</em>, or 1 otherwise.</li>
</ul>

<p>To build <em>g</em>, take its degree as ceil(<em>M</em>)+1 or greater (so that <em>g</em>&#39;s Lipschitz constant is greater than <em>M</em> and <em>g</em> has ceil(<em>M</em>) + 2 coefficients), then set the first coefficient as <em>l</em>, the last coefficient as <em>u</em>, and the remaining coefficients as 1. (As a result, the polynomial <em>g</em> will have computable coefficients.) Then <em>g</em> will meet the additional condition for Lemma 1 and the result follows from that lemma. &#x25a1;</p>

<p><strong>Lemma 4.</strong> <em>If f(&lambda;) is described in the strong simulability statement, is Lipschitz continuous, and is such that f(0) = 0 and f(1) = 1 (so that 0 and 1 are in the domain of f), then f is strongly simulable.</em></p>

<p><em>Proof:</em> Let <em>M</em> and <em>l</em> be as in Lemma 3.</p>

<p>To build <em>g</em> and <em>&omega;</em>, take their degree as ceil(<em>M</em>)+1 or greater (so that their Lipschitz constant is greater than <em>M</em> and each polynomial has ceil(<em>M</em>) + 2 coefficients), then for each polynomial, set its first coefficient as <em>l</em> and the last coefficient as 1. The remaining coefficients of <em>g</em> are set as 1 and the remaining coefficients of <em>&omega;</em> are set as 0.  (As a result, the polynomial <em>g</em> will have computable coefficients.)  Then <em>g</em> and <em>&omega;</em> will meet the additional conditions for Lemma 2 and the result follows from that lemma. &#x25a1;</p>

<p><em>Proof of Proposition 3:</em> In the proof of proposition 2, replace Lemma 1 and Lemma 2 with Lemma 3 and Lemma 4, respectively. &#x25a1;</p>

<p>It is suspected that the conditions in Proposition 2 are necessary and sufficient for <em>f</em>(<em>&lambda;</em>) to be strongly simulable.</p>

<p><a id=Multiple_Output_Bernoulli_Factory></a></p>

<h3>Multiple-Output Bernoulli Factory</h3>

<p>A related topic is a Bernoulli factory that takes a coin with unknown probability of heads <em>&lambda;</em> and produces one or more samples of the probability <em>f</em>(<em>&lambda;</em>).  This section calls it a <em>multiple-output Bernoulli factory</em>.</p>

<p>Obviously, any single-output Bernoulli factory can produce multiple outputs by running itself multiple times.  But for some functions <em>f</em>, it may be that producing multiple outputs at a time may use fewer input coin flips than producing one output multiple times.</p>

<p>Let <em>a</em> and <em>b</em> be real numbers satisfying 0 &lt; <em>a</em> &lt; <em>b</em> &lt; 1, such as <em>a</em>=1/100, <em>b</em>=99/100.  Define the <em>entropy bound</em> as <em>h</em>(<em>f</em>(<em>&lambda;</em>))/<em>h</em>(<em>&lambda;</em>) where <em>h</em>(<em>x</em>) = &minus;<em>x</em>*ln(<em>x</em>)&minus;(1&minus;<em>x</em>)*ln(1&minus;<em>x</em>) is related to the Shannon entropy function.  The question is:</p>

<p><em>When the probability &lambda; is such that a &le; &lambda; &le; b, is there a multiple-output Bernoulli factory for f(&lambda;) with an expected (&quot;long-run average&quot;) number of input coin flips per sample that is arbitrarily close to the entropy bound?  Call such a Bernoulli factory an <strong>optimal factory</strong>.</em></p>

<p>(See Nacu and Peres (2005, Question 2)[^1].)</p>

<p>So far, the following functions do admit an <em>optimal factory</em>:</p>

<ul>
<li>The functions <em>&lambda;</em> and 1 &minus; <em>&lambda;</em>.</li>
<li>Constants in [0, 1].  As Nacu and Peres (2005)[^1] already showed, any such constant <em>c</em> admits an optimal factory: generate unbiased random bits using Peres&#39;s iterated von Neumann extractor (Peres 1992)[^34], then build a binary tree that generates 1 with probability <em>c</em> and 0 otherwise (Knuth and Yao 1976)[^35].</li>
</ul>

<p>It is easy to see that if an <em>optimal factory</em> exists for <em>f</em>(<em>&lambda;</em>), then one also exists for 1 &minus; <em>f</em>(<em>&lambda;</em>): simply change all ones returned by the <em>f</em>(<em>&lambda;</em>) factory into zeros and vice versa.</p>

<p>Also, as Yuval Peres (Jun. 24, 2021) told me, there is an efficient multiple-output Bernoulli factory for <em>f</em>(<em>&lambda;</em>) = <em>&lambda;</em>/2: the key is to flip the input coin enough times to produce unbiased random bits using his extractor (Peres 1992)[^19], then multiply each unbiased bit with another input coin flip to get a sample from <em>&lambda;</em>/2.  Given that the sample is equal to 0, there are three possibilities that can &quot;be extracted to produce more fair bits&quot;: either the unbiased bit is 0, or the coin flip is 0, or both are 0.</p>

<p>This algorithm, though, doesn&#39;t count as an <em>optimal factory</em>, and Peres described this algorithm only incompletely.  By simulation and trial and error I found an improved version of the algorithm.  It uses two randomness extractors (extractor 1 and extractor 2) that produce unbiased random bits from biased data (which is done using a method given later in this section).  The extractors must be asymptotically optimal (they must approach the entropy limit as closely as desired); one example is the iterated von Neumann construction in Peres (1992)[^34].  The algorithm consists of doing the following in a loop until the desired number of outputs is generated.</p>

<ol>
<li>If the number of outputs generated so far is divisible by 20, do the following:

<ul>
<li>Generate an unbiased random bit (see below).  If that bit is zero, output 0, then repeat this step unless the desired number of outputs has been generated.  If the bit is 1, flip the input coin and output the result.</li>
</ul></li>
<li>Otherwise, do the following:

<ol>
<li>Generate an unbiased random bit (see below), call it <em>fc</em>.  Then flip the input coin and call the result <em>bc</em>.</li>
<li>Output <em>fc</em>*<em>bc</em>.</li>
<li>(The following steps pass &quot;unused&quot; randomness to the extractor in a specific way to ensure correctness.) If <em>fc</em> is 0, and <em>bc</em> is 1, append 0 to extractor 2&#39;s input bits.</li>
<li>If <em>fc</em> and <em>bc</em> are both 0, append 1 then 1 to extractor 2&#39;s input bits.</li>
<li>If <em>fc</em> is 1 and <em>bc</em> is 0, append 1 then 0 to extractor 2&#39;s input bits.</li>
</ol></li>
</ol>

<p>Inspired by Peres&#39;s result with <em>&lambda;</em>/2, the following algorithm is proposed.  It works for every function writable as <em>D</em>(<em>&lambda;</em>)/<em>E</em>(<em>&lambda;</em>), where&mdash;</p>

<ul>
<li>$D$ is the polynomial $D(\lambda)=\sum_{i=0}^k \lambda^i (1-\lambda)^{k-i} d[i]$,</li>
<li>$E$ is the polynomial $E(\lambda)=\sum_{i=0}^k \lambda^i (1-\lambda)^{k-i} e[i]$,</li>
<li>every <em>d</em>[<em>i</em>] is less than or equal to the corresponding <em>e</em>[<em>i</em>], and</li>
<li>each <em>d</em>[<em>i</em>] and each <em>e</em>[<em>i</em>] is a nonnegative integer.</li>
</ul>

<p>The algorithm is a modified version of the &quot;block simulation&quot; in Mossel and Peres (2005, Proposition 2.5)[^36], which also &quot;extracts&quot; residual randomness with the help of six asymptotically optimal randomness extractors.  In the algorithm, let <em>r</em> be an integer such that, for every integer <em>i</em> in [0, <em>k</em>], <em>e</em>[<em>i</em>] &lt; choose(<em>k</em>, <em>i</em>)*choose(2*<em>r</em>, <em>r</em>).</p>

<ol>
<li>Set <em>iter</em> to 0.</li>
<li>Flip the input coin <em>k</em> times.  Then build a bitstring <em>B1</em> consisting of the coin flip results in the order they occurred.  Let <em>i</em> be the number of ones in <em>B1</em>.</li>
<li>Generate 2*<em>r</em> unbiased random bits (see below).  (Rather than flipping the input coin 2*<em>r</em> times, as in the algorithm of Proposition 2.5.)  Then build a bitstring <em>B2</em> consisting of the coin flip results in the order they occurred.</li>
<li>If the number of ones in <em>B2</em> is other than <em>r</em>: Translate <em>B1</em> + <em>B2</em> to an integer under mapping 1, then pass that number to extractor 2<sup>&dagger;</sup>, then add 1 to <em>iter</em>, then go to step 2.</li>
<li>Translate <em>B1</em> + <em>B2</em> to an integer under mapping 2, call the integer <em>&beta;</em>.  If <em>&beta;</em> &lt; <em>d</em>[<em>i</em>], pass <em>&beta;</em> to extractor 3, then pass <em>iter</em> to extractor 6, then output a 1.  Otherwise, if <em>&beta;</em> &lt; <em>e</em>[<em>i</em>], pass <em>&beta;</em> &minus; <em>d</em>[<em>i</em>] to extractor 4, then pass <em>iter</em> to extractor 6, then output a 0.  Otherwise, pass <em>&beta;</em> &minus; <em>e</em>[<em>i</em>] to extractor 5, then add 1 to <em>iter</em>, then go to step 2.</li>
</ol>

<p>The mappings used in this algorithm are as follows:</p>

<ol>
<li>A one-to-one mapping between&mdash;

<ul>
<li>bitstrings of length <em>k</em> + 2*<em>r</em> with fewer or greater than <em>r</em> ones among the last 2*<em>r</em> bits, and</li>
<li>the integers in [0, 2<sup><em>k</em></sup> * (2<sup>2*<em>r</em></sup> &minus; choose(2*<em>r</em>, <em>r</em>))).</li>
</ul></li>
<li>A one-to-one mapping between&mdash;

<ul>
<li>bitstrings of length <em>k</em> + 2*<em>r</em> with exactly <em>i</em> ones among the first <em>k</em> bits and exactly <em>r</em> ones among the remaining bits, and</li>
<li>the integers in [0, choose(<em>k</em>, <em>i</em>)*choose(2*<em>r</em>, <em>r</em>)).</li>
</ul></li>
</ol>

<p>In this algorithm, an unbiased random bit is generated as follows.  Let <em>m</em> be an even integer that is 32 or greater (in general, the greater <em>m</em> is, the more efficient the overall algorithm is in terms of coin flips).</p>

<ol>
<li>Use extractor 1 to extract outputs from floor(<em>n</em>/<em>m</em>)*<em>m</em> inputs, where <em>n</em> is the number of input bits available to that extractor.  Do the same for the remaining extractors.</li>
<li>If extractor 2 has at least one unused output bit, take an output and stop.  Otherwise, repeat this step for the remaining extractors.</li>
<li>Flip the input coin at least <em>m</em> times, append the coin results to extractor 1&#39;s inputs, and go to step 1.</li>
</ol>

<p>Now consider the last paragraph of Proposition 2.5.  If the input coin were flipped in step 2, the probability of&mdash;</p>

<ul>
<li>outputting 1 in the algorithm&#39;s last step would be <em>P1</em> = <em>&lambda;</em><sup><em>r</em></sup>*(1&minus;<em>&lambda;</em>)<sup><em>r</em></sup>*<em>D</em>(<em>&lambda;</em>).</li>
<li>outputting either 0 or 1 in that step would be <em>P01</em> = <em>&lambda;</em><sup><em>r</em></sup>*(1&minus;<em>&lambda;</em>)<sup><em>r</em></sup>*<em>E</em>(<em>&lambda;</em>),</li>
</ul>

<p>so that the algorithm would simulate <em>f</em>(<em>&lambda;</em>) = <em>P1</em> / <em>P01</em>.  Observe that the <em>&lambda;</em><sup><em>r</em></sup>*(1&minus;<em>&lambda;</em>)<sup><em>r</em></sup> cancels out in the division.  Thus, we could replace the input coin with unbiased random bits and still simulate <em>f</em>(<em>&lambda;</em>); the <em>&lambda;</em><sup><em>r</em></sup>*(1&minus;<em>&lambda;</em>)<sup><em>r</em></sup> above would then be (1/2)<sup>2*<em>r</em></sup>.</p>

<p>While this algorithm is coin-flip-efficient, it is not believed to be an optimal factory, at least not without more work.  In particular, a bigger savings of input coin flips could occur if <em>f</em>(<em>&lambda;</em>) maps each value <em>a</em> or greater and <em>b</em> or less to a small range of values, so that the algorithm could, for example, generate a uniform random variate greater than 0 and less than 1 using unbiased random bits and see whether that variate lies outside that range of values &mdash; and thus produce a sample from <em>f</em>(<em>&lambda;</em>) without flipping the input coin again.</p>

<p><small><sup>&dagger;</sup> For example, by translating the number to input bits via Pae&#39;s entropy-preserving binarization (Pae 2018)[^37].  But correctness might depend on how this is done; after all, the number of coin flips per sample must equal or exceed the entropy bound for every <em>&lambda;</em>.</small></p>

<p><a id=Proofs_for_Polynomial_Building_Schemes></a></p>

<h3>Proofs for Polynomial-Building Schemes</h3>

<p>This section shows mathematical proofs for some of the polynomial-building schemes of this page.</p>

<p>In the following results:</p>

<ul>
<li>A <em>strictly bounded factory function</em> means a continuous function on the closed unit interval, with a minimum of greater than 0 and a maximum of less than 1.</li>
<li>A function <em>f</em>(<em>&lambda;</em>) is <em>polynomially bounded</em> if both <em>f</em>(<em>&lambda;</em>) and 1&minus;<em>f</em>(<em>&lambda;</em>) are greater than or equal to min(<em>&lambda;</em><sup><em>n</em></sup>, (1&minus;<em>&lambda;</em>)<sup><em>n</em></sup>) for some integer <em>n</em> (Keane and O&#39;Brien 1994)[^32].</li>
<li>A <em>modulus of continuity</em> of a function <em>f</em> means a nonnegative and nowhere decreasing function <em>&omega;</em> on the closed unit interval, for which <em>&omega;</em>(0) = 0, and for which abs(f(<em>x</em>) &minus; f(<em>y</em>)) &le; <em>&omega;</em>(abs(<em>x</em>&minus;<em>y</em>)) for every <em>x</em> in the closed unit interval and every <em>y</em> in the closed unit interval.  Loosely speaking, a modulus of continuity <em>&omega;</em>(<em>&delta;</em>) is greater than or equal to <em>f</em>&#39;s maximum range in a window of size <em>&delta;</em>.</li>
</ul>

<p><strong>Lemma 1.</strong> <em>Let f(&lambda;) be a continuous and nowhere decreasing function, and let X<sub>k</sub> be a hypergeometric(2*n, k, n) random variable, where n&ge;1 is a constant integer and k is an integer in [0, 2*n] .  Then the expected value of f(X<sub>k</sub>/n) decreases nowhere as k increases.</em></p>

<p><em>Proof.</em> This is equivalent to verifying whether <em>X</em><sub><em>m</em>+1</sub>/<em>n</em> &quot;dominates&quot; <em>X</em><sub><em>m</em></sub>/<em>n</em> (and, obviously by extension, <em>X</em><sub><em>m</em>+1</sub> &quot;dominates&quot; <em>X</em><sub><em>m</em></sub>) in terms of first-degree stochastic dominance (Levy 1998)[^38].   This means that the probability that (<em>X</em><sub><em>m</em>+1</sub> &le; <em>j</em>) is less than or equal to that for <em>X</em><sub><em>m</em></sub> for each <em>j</em> satisfying 0 &le; <em>j</em> &le; <em>n</em>.  A proof of this was given by the user &quot;Henry&quot; of the <em>Mathematics Stack Exchange</em> community[^39]. &#x25a1;</p>

<p>Lemma 6(i) of Nacu and Peres (2005)[^1] can be applied to continuous functions beyond just Lipschitz continuous functions.  This includes the larger class of <em>Hölder continuous</em> functions (see &quot;<a href="#Definitions"><strong>Definitions</strong></a>&quot;).</p>

<p><strong>Lemma 2.</strong> <em>Let f(&lambda;) be a continuous function that maps the closed unit interval to itself, and let X be a hypergeometric(2*n, k, n) random variable.</em></p>

<ol>
<li><em>Let &omega;(x) be a modulus of continuity of f.  If &omega; is continuous and concave, then the expression&mdash;<br>abs(<strong>E</strong>[f(X/n)] &minus; f(k/(2*n))),&nbsp;&nbsp;&nbsp;(1)<br>is less than or equal to&mdash;</em>

<ul>
<li><em>&omega;(sqrt(1/(8*n&minus;4))), for every integer n&ge;1 that&#39;s a power of 2,</em></li>
<li><em>&omega;(sqrt(1/(7*n))), for every integer n&ge;4 that&#39;s a power of 2,</em></li>
<li><em>&omega;(sqrt(1/(2*n))), for every integer n&ge;1 that&#39;s a power of 2, and</em></li>
<li><em>&omega;(sqrt( (k/(2*n)) * (1&minus;k/(2*n)) / (2*n&minus;1) )), for every n&ge;1 that&#39;s a power of 2.</em></li>
</ul></li>
<li><em>If f is Hölder continuous with Hölder constant M and with Hölder exponent &alpha; such that 0 &lt; &alpha; &le; 1, then the expression (1) is less than or equal to&mdash;</em>

<ul>
<li><em>M*(1/(2*n))<sup>&alpha;/2</sup>, for every integer n&ge;1 that&#39;s a power of 2,</em></li>
<li><em>M*(1/(7*n))<sup>&alpha;/2</sup>, for every integer n&ge;4 that&#39;s a power of 2, and</em></li>
<li><em>M*(1/(8*n&minus;4))<sup>&alpha;/2</sup>, for every integer n&ge;1 that&#39;s a power of 2.</em></li>
</ul></li>
<li><em>If f has a Lipschitz continuous derivative with Lipschitz constant M, then the expression (1) is less than or equal to&mdash;</em>

<ul>
<li><em>(M/2)*(1/(7*n)), for every integer n&ge;4 that&#39;s a power of 2, and</em></li>
<li><em>(M/2)*(1/(8*n&minus;4)), for every integer n&ge;1 that&#39;s a power of 2.</em></li>
</ul></li>
<li><em>If f is convex, nowhere decreasing, and greater than or equal to 0, then the expression (1) is less than or equal to <strong>E</strong>[f(Y/n)] for every integer n&ge;1 that&#39;s a power of 2, where Y is a hypergeometric(2*n, n, n) random variable.</em></li>
</ol>

<p><em>Proof.</em></p>

<ol>
<li><em>&omega;</em> is assumed to be nonnegative because absolute values are nonnegative.  To prove the first and second bounds: abs(<strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<em>k</em>/(2 * <em>n</em>))) &le; <strong>E</strong>[abs(<em>f</em>(<em>X</em>/<em>n</em>) &minus; <em>f</em>(<em>k</em>/(2 * <em>n</em>))] &le; <strong>E</strong>[<em>&omega;</em>(abs(<em>X</em>/<em>n</em> &minus; <em>k</em>/(2 * <em>n</em>))] &le; <em>&omega;</em>(<strong>E</strong>[abs(<em>X</em>/<em>n</em> &minus; <em>k</em>/(2 * <em>n</em>))]) (by Jensen&#39;s inequality and because <em>&omega;</em> is concave) &le; <em>&omega;</em>(sqrt(<strong>E</strong>[abs(<em>X</em>/<em>n</em> &minus; <em>k</em>/(2 * <em>n</em>))]<sup>2</sup>)) = <em>&omega;</em>(sqrt(<strong>Var</strong>[<em>X</em>/<em>n</em>])) = <em>&omega;</em>(sqrt((<em>k</em>*(2 * <em>n</em>&minus;<em>k</em>)/(4*(2 * <em>n</em>&minus;1)*<em>n</em><sup>2</sup>)))) &le; <em>&omega;</em>(sqrt((<em>n</em><sup>2</sup>/(4*(2 * <em>n</em>&minus;1)*<em>n</em><sup>2</sup>)))) = <em>&omega;</em>(sqrt((1/(8*<em>n</em>&minus;4)))) = <em>&rho;</em>, and for every <em>n</em>&ge;4 that&#39;s an integer power of 2, <em>&rho;</em> &le; <em>&omega;</em>(sqrt(1/(7*<em>n</em>))).  To prove the third bound: abs(<strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<em>k</em>/(2 * <em>n</em>))) &le; <em>&omega;</em>(sqrt(<strong>Var</strong>[<em>X</em>/<em>n</em>])) &le; <em>&omega;</em>(sqrt(1/(2*n))).  To prove the fourth bound: abs(<strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<em>k</em>/(2 * <em>n</em>))) &le; <em>&omega;</em>(sqrt((<em>n</em><sup>2</sup>/(4*(2 * <em>n</em>&minus;1)*<em>n</em><sup>2</sup>)))) = <em>&omega;</em>(sqrt( (<em>k</em>/(2*<em>n</em>)) * (1&minus;<em>k</em>/(2*<em>n</em>)) / (2*<em>n</em>&minus;1) )).</li>
<li>By the definition of Hölder continuous functions, take <em>&omega;</em>(<em>x</em>) = <em>M</em>*<em>x</em><sup><em>&alpha;</em></sup>.  Because <em>&omega;</em> is a concave modulus of continuity on the closed unit interval, the result follows from part 1.</li>
<li>(Much of this proof builds on Nacu and Peres 2005, Proposition 6(ii)[^1].) The expected value (see note 1) of $X$ is $E[X/n]=k/(2n)$. Since $E[X/n-k/(2n)] = 0$, it follows that $f&#39;(X/n) E(X/n-k/(2n)) = 0$.  Moreover, $|f(x)-f(s)-f&#39;(x)(x-s)|\le (M/2)(x-s)^2$ (see Micchelli 1973, Theorem 3.2)[^6], so&mdash; $$E[|f(X/n)-f(k/(2n))|]=|E[f(X/n)-f(k/(2n))-f&#39;(k/(2n))(X/n-k/(2n))]|$$ $$\le (M/2)(X/n-k/(2n))^2 \le (M/2) Var(X/n).$$  By part 1&#39;s proof, it follows that (<em>M</em>/2)*<strong>Var</strong>[<em>X</em>/<em>n</em>] = (<em>M</em>/2)*(<em>k</em>*(2 * <em>n</em>&minus;<em>k</em>)/(4*(2 * <em>n</em>&minus;1)*<em>n</em><sup>2</sup>)) &le; (<em>M</em>/2)*(<em>n</em><sup>2</sup>/(4*(2 * <em>n</em>&minus;1)*<em>n</em><sup>2</sup>)) = (<em>M</em>/2)*(1/(8*<em>n</em>&minus;4)) = <em>&rho;</em>.  For every integer <em>n</em>&ge;4 that&#39;s a power of 2, <em>&rho;</em> &le;  (<em>M</em>/2)*(1/(7*<em>n</em>)).</li>
<li>Let <em>X</em><sub><em>m</em></sub> be a hypergeometric(2 * <em>n</em>, <em>m</em>, <em>n</em>) random variable.  By Lemma 1 and the assumption that <em>f</em> is nowhere decreasing, <strong>E</strong>[<em>f</em>(<em>X</em><sub><em>k</em></sub>/<em>n</em>)] is nowhere decreasing as <em>k</em> increases, so take <strong>E</strong>[<em>f</em>(<em>X</em><sub><em>n</em></sub>/<em>n</em>)] = <strong>E</strong>[<em>f</em>(<em>Y</em></sub>/<em>n</em>)] as the upper bound.  Then, abs(<strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<em>k</em>/(2 * <em>n</em>))) = abs(<strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<strong>E</strong>[<em>X</em>/<em>n</em>])) = <strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<strong>E</strong>[<em>X</em>/<em>n</em>]) (by Jensen&#39;s inequality, because <em>f</em> is convex and not less than 0) = <strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] &minus; <em>f</em>(<em>k</em>/(2 * <em>n</em>)) &le; <strong>E</strong>[<em>f</em>(<em>X</em>/<em>n</em>)] (because <em>f</em> is not less than 0) &le; <strong>E</strong>[<em>f</em>(<em>Y</em>/<em>n</em>)]. &#x25a1;</li>
</ol>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li><strong>E</strong>[.] means expected value (&quot;long-run average&quot;), and <strong>Var</strong>[.] means variance.  A hypergeometric(2 * <em>n</em>, <em>k</em>, <em>n</em>) random variable is the number of &quot;good&quot; balls out of <em>n</em> balls taken uniformly at random, all at once, from a bag containing 2 * <em>n</em> balls, <em>k</em> of which are &quot;good&quot;.</li>
<li>Parts 1 through 3 exploit a tighter bound on <strong>Var</strong>[<em>X</em>/<em>n</em>] than the bound given in Nacu and Peres (2005, Lemma 6(i) and 6(ii), respectively)[^1].  However, for technical reasons, different bounds are proved for different ranges of integers <em>n</em>.</li>
<li>All continuous functions that map the closed unit interval to itself, including all of them that admit a Bernoulli factory, have a modulus of continuity.  The proof of part 1 remains valid even if <em>&omega;</em>(0) &gt; 0, because the bounds proved remain correct even if <em>&omega;</em> is overestimated.  The following functions have a simple <em>&omega;</em> that satisfies the lemma:

<ol>
<li>If <em>f</em> is strictly increasing and convex, <em>&omega;</em>(<em>x</em>) can equal <em>f</em>(1) &minus; <em>f</em>(1&minus;<em>x</em>) (Gal 1990)[^40]; (Gal 1995)[^41].</li>
<li>If <em>f</em> is strictly decreasing and convex, <em>&omega;</em>(<em>x</em>) can equal <em>f</em>(0) &minus; <em>f</em>(<em>x</em>) (Gal 1990)[^40]; (Gal 1995)[^41].</li>
<li>If <em>f</em> is strictly increasing and concave, <em>&omega;</em>(<em>x</em>) can equal <em>f</em>(<em>x</em>) &minus; <em>f</em>(0) (by symmetry with 2).</li>
<li>If <em>f</em> is strictly decreasing and concave, <em>&omega;</em>(<em>x</em>) can equal <em>f</em>(1&minus;<em>x</em>) &minus; <em>f</em>(1) (by symmetry with 1).</li>
<li>If <em>f</em> is concave and is strictly increasing then strictly decreasing, then <em>&omega;</em>(<em>h</em>) can equal (<em>f</em>(min(<em>h</em>, <em>&sigma;</em>))+(<em>f</em>(1&minus;min(<em>h</em>, 1&minus;<em>&sigma;</em>))&minus;<em>f</em>(1)), where <em>&sigma;</em> is the point where <em>f</em> stops increasing and starts decreasing (Anastassiou and Gal 2012)[^42].</li>
</ol></li>
</ol>
</blockquote>

<p><strong>Theorem 1.</strong> <em>Let $f$ be a strictly bounded factory function, let $n_0\ge 1$ be an integer, and let $\phi(n)$ be a function that takes on a nonnegative value.  Suppose $f$ is such that the expression (1) in Lemma 2 is less than or equal to $\phi(n)$ whenever $n\ge n_0$ is an integer power of 2.  Let&mdash;</em></p>

<p>$$\eta(n)=\sum_{k\ge \log_2(n)} \phi(2^k),$$</p>

<p><em>for every integer n&ge;1 that&#39;s a power of 2.  If the series &eta;(n) converges to a finite value for each such $n$, and if it converges to 0 as $n$ gets large, then the following scheme for f(&lambda;) is valid in the following sense:</em></p>

<p><em>There are polynomials $g_n$ and $h_n$ (where $n\ge 1$ is an integer power of 2) as follows. The $k$-th Bernstein coefficient of $g_n$ and $h_n$ is <strong>fbelow</strong>(n, k) and <strong>fabove</strong>(n, k), respectively (where $0\le k\le n$), where:</em></p>

<p><em>If $n_0 = 1$:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) =</em> $f(k/n)-\eta(n)$.</li>
<li><em><strong>fabove</strong>(n, k) =</em> $f(k/n)+\eta(n)$.</li>
</ul>

<p><em>If $n_0 &gt; 1$:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) =</em> min(<strong>fbelow</strong>($n_0$,0), <strong>fbelow</strong>($n_0$,1), ..., <strong>fbelow</strong>($n_0$,$n_0$)) <em>if</em> $n &lt; n_0$; $f(k/n)-\eta(n)$ <em>otherwise.</em></li>
<li><em><strong>fabove</strong>(n, k) =</em> max(<strong>fabove</strong>($n_0$,0), <strong>fabove</strong>($n_0$,1), ..., <strong>fbelow</strong>($n_0$,$n_0$)) <em>if</em> $n &lt; n_0$; $f(k/n)+\eta(n)$ <em>otherwise.</em></li>
</ul>

<p><em>The polynomials $g_n$ and $h_n$ satisfy:</em></p>

<ol>
<li><em>$g_n \le h_n$.</em></li>
<li><em>$g_n$ and $h_n$ converge to $f$ as $n$ gets large.</em></li>
<li>$(g_{n+1}-g_n)$ <em>and</em> $(h_{n}-h_{n+1})$ <em>are polynomials with nonnegative Bernstein coefficients once they are rewritten to polynomials in Bernstein form of degree exactly $n+1$.</em></li>
</ol>

<p><em>Proof.</em> For simplicity, this proof assumes first that $n_0 = 1$.</p>

<p>For the series <em>&eta;</em>(<em>n</em>) in the theorem, because $\phi(n)$ is nonnegative, each term of the series is nonnegative making the series nonnegative and, by the assumption that the series converges, <em>&eta;</em>(<em>n</em>) is nowhere increasing with increasing <em>n</em>.</p>

<p>Item 1 is trivial.  If $n\ge n_0$, $g_n$ is simply the Bernstein polynomial of $f$ minus a nonnegative value, and $h_n$ is the Bernstein polynomial of $f$ plus that same value, and if $n$ is less than $n_0$, $g_n$ is a constant value not less than the lowest point reachable by the lower polynomials, and $h_n$ is a constant value not less than the highest point reachable by the upper polynomials.</p>

<p>Item 2 is likewise trivial. A well known result is that the Bernstein polynomials of $f$ converge to $f$ as their degree $n$ gets large.  And because the series <em>&eta;</em> (in Theorem 1) sums to a finite value that goes to 0 as $n$ increases, the upper and lower shifts will converge to 0 so that $g_n$ and $h_n$ converge to the degree-$n$ Bernstein polynomials and thus to $f$.</p>

<p>Item 3 is the <em>consistency requirement</em> described earlier in this page. This is ensured as in Proposition 10 of Nacu and Peres (2005)[^1] by bounding, from below, the offset by which to shift the approximating polynomials.  This lower bound is <em>&eta;</em>(<em>n</em>), a solution to the equation 0 = <em>&eta;</em>(<em>n</em>) &minus; <em>&eta;</em>(2 * <em>n</em>) &minus; <em>&phi;</em>(<em>n</em>) (see note below), where <em>&phi;</em>(<em>n</em>) is a function that takes on a nonnegative value.</p>

<p><em>&phi;</em>(<em>n</em>) is, roughly speaking, the minimum distance between one polynomial and the next so that the consistency requirement is met between those two polynomials.  Compare the assumptions on <em>&phi;</em> in Theorem 1 with equations (10) and (11) in Nacu and Peres (2005).</p>

<p>The solution for $\eta(n)$ given in the statement of the theorem is easy to prove by noting that this is a recursive process: we start by calculating the series for <em>n</em> = 2*<em>n</em>, then adding <em>&phi;</em>(<em>n</em>) to it (which will be positive), in effect working backwards and recursively, and we can easily see that we can calculate the series for <em>n</em> = 2*<em>n</em> only if the series converges, hence the assumption of a converging series.</p>

<p>Now to prove the result assuming that $n_0 &gt; 1$.</p>

<p>Then we can take advantage of the observation in Remark B of Nacu and Peres (2005)[^1] that we can start defining the polynomials at any $n$ greater than 0, including $n = n_0$; in that case, the upper and lower polynomials of degree 1 or greater, but less than $n_0$, would be constant functions, so that as polynomials in Bernstein form, the coefficients of each one would be equal. The lower constants are no greater than $g_{n_0}$&#39;s lowest Bernstein coefficient, and the upper constants are no less than $g_{n_0}$&#39;s highest Bernstein coefficients; they meet Item 3 because these lower and upper constants, when elevated to degree $n_0$, have Bernstein coefficients that are still no greater or no less, respectively, than the corresponding degree-$n_0$ polynomial. With the <em>&phi;</em> given in this theorem, the series <em>&eta;</em>(<em>n</em>) in the theorem remains nonnegative.  Moreover, since <em>&eta;</em> is assumed to converge, <em>&eta;</em>(<em>n</em>) still decreases with increasing <em>n</em>. &#x25a1;</p>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>There is only one solution <em>&eta;</em>(<em>n</em>) in the case at hand.  Unlike so-called <a href="https://math.stackexchange.com/questions/3993739"><strong><em>functional equations</em></strong></a> and linear recurrences, with a solution that varies depending on the starting value, there is only one solution in the case at hand, namely the solution that makes the series converge, if it exists at all.  Alternatively, the equation can be expanded to 0 = <em>&eta;</em>(<em>n</em>) &minus; <em>&eta;</em>(4 * <em>n</em>) &minus; <em>&phi;</em>(2*<em>n</em>) &minus; <em>&phi;</em>(<em>n</em>) = <em>&eta;</em>(<em>n</em>) &minus; <em>&eta;</em>(8 * <em>n</em>) &minus; <em>&phi;</em>(4*<em>n</em>) &minus; <em>&phi;</em>(2*<em>n</em>) &minus; <em>&phi;</em>(<em>n</em>) = ...</li>
<li>$\log_2(n)$ is the number $x$ such that $2^x = n$.</li>
</ol>
</blockquote>

<p><strong>Proposition 1A.</strong> <em>If a scheme satisfies Theorem 1, the polynomials $g_n$ and $h_n$ in the scheme can be made to satisfy conditions (i), (iii), and (iv) of Proposition 3 of Nacu and Peres (2005)[^1] as follows:</em></p>

<ul>
<li>$g_n$ = $g_{n-1}$ <em>and</em> $h_n$ = $h_{n-1}$ <em>whenever $n$ is an integer greater than 1 and not a power of 2.</em></li>
<li><em>If <strong>fabove</strong>(n, k) &gt; 1 for a given $n$ and some $k$, the coefficients of $h_n$ (the upper polynomial) are all 1.</em></li>
<li><em>If <strong>fbelow</strong>(n, k) &lt; 0 for a given $n$ and some $k$, the coefficients of $g_n$ (the lower polynomial) are all 0.</em></li>
</ul>

<p><em>Proof:</em> Condition (i) of Proposition 3 says that each coefficient of the polynomials must be 0 or greater and 1 or less.  This is ensured starting with a large enough value of <em>n</em> greater than 0 that&#39;s a power of 2, call it <em>n</em><sub>1</sub>, as shown next.</p>

<p>Let <em>&epsilon;</em> be a positive distance between 0 and the minimum or between 1 and the maximum of <em>f</em>, whichever is smaller.  This <em>&epsilon;</em> exists by the assumption that <em>f</em> is bounded away from 0 and 1. Because the series <em>&eta;</em> (in Theorem 1) sums to a finite value that goes to 0 as $n$ increases, <em>&eta;</em>(<em>n</em>) will eventually stay less than <em>&epsilon;</em>.  And if $n\ge n_0$ is a power of 2 (where $n_0$ is as in Theorem 1), the <code>f(k/n)</code> term is bounded by the minimum and maximum of <em>f</em> by construction. This combined means that the lower and upper polynomials&#39; Bernstein coefficients will eventually be bounded by 0 and 1 for every integer <em>n</em> starting with <em>n</em><sub>1</sub>.</p>

<p>For <em>n</em> less than <em>n</em><sub>1</sub>, condition (i) is ensured by setting the lower or upper polynomial&#39;s coefficient to 0 or 1, respectively, whenever a coefficient of the degree-<em>n</em> polynomial would otherwise be less than 0 or greater than 1, respectively.</p>

<p>Condition (iii) of Proposition 3 is mostly ensured by item 2 of Theorem 1.  The only thing to add is that for $n$ less than <em>n</em><sub>1</sub>, the lower and upper polynomials $g_n$ and $h_n$ can be treated as 0 or 1 without affecting convergence, and that for $n$ other than a power of 2, defining $g_n = g_{n-1}$ and $h_n = h_{n-1}$ maintains condition (iii) by Remark B of Nacu and Peres (2005)[^1].</p>

<p>Condition (iv) of Proposition 3 is mostly ensured by item 3 of Theorem 1.  For <em>n</em>=<em>n</em><sub>1</sub>, condition (iv) is maintained by noting that the degree-<em>n</em><sub>1</sub> polynomial&#39;s coefficients must be bounded by 0 and 1 by condition (i) so they will likewise be bounded by those of the lower and upper polynomials of degree less than <em>n</em><sub>1</sub>, and those polynomials are the constant 0 and the constant 1, respectively, as are their coefficients. Finally, for $n$ other than a power of 2, defining $g_n = g_{n-1}$ and $h_n = h_{n-1}$ maintains condition (iv) by Remark B of Nacu and Peres (2005)[^1].  &#x25a1;</p>

<p><strong>Corollary 1.</strong> <em>Let f(&lambda;) be a strictly bounded factory function. If that function is Hölder continuous with Hölder constant M and Hölder exponent &alpha;, then the following scheme determined by <strong>fbelow</strong> and <strong>fabove</strong> is valid in the sense of Theorem 1:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) = f(k/n) &minus; D(n).</em></li>
<li><em><strong>fabove</strong>(n, k) = f(k/n) + D(n).</em></li>
</ul>

<p><em>Where</em> $D(n)=\frac{M}{((2^{\alpha/2}-1) n^{\alpha/2}}$.</p>

<p><em>Or:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) = min(<strong>fbelow</strong>(4,0), <strong>fbelow</strong>(4,1), ..., <strong>fbelow</strong>(4,4)) if n &lt; 4; otherwise, f(k/n) &minus; &eta;(n).</em></li>
<li><em><strong>fabove</strong>(n, k) = max(<strong>fabove</strong>(4,0), <strong>fabove</strong>(4,1), ..., <strong>fabove</strong>(4,4)) if n &lt; 4; otherwise, f(k/n) + &eta;(n).</em></li>
</ul>

<p><em>Where &eta;(n) = M*(2/7)<sup>&alpha;/2</sup>/((2<sup>&alpha;/2</sup>&minus;1)*n<sup>&alpha;/2</sup>).</em></p>

<p><em>Proof.</em> Because $f$ is Hölder continuous, it admits the modulus of continuity $\omega(x)=Mx^{\alpha}$.  By part 1 of lemma 2:</p>

<ul>
<li>For each integer $n\ge 1$ that&#39;s a power of 2 ($n_0=1$ in Theorem 1), $\phi(n)=\omega(\sqrt{1/(2n)})=M (1/(2n))^{\alpha/2}$ can be taken for each such integer $n$, and thus $\eta(n)=D(n)=\frac{M}{((2^{\alpha/2}-1) n^{\alpha/2}}$ (where $\eta(n)$ is as in Theorem 1).</li>
<li>For each integer $n\ge 4$ that&#39;s a power of 2 ($n_0=4$ in Theorem 1), $\phi(n)=\omega(\sqrt{1/(2n)})=M (1/(7n))^{\alpha/2}$ can be taken for each such integer $n$, and thus $\eta(n)=$ M*(2/7)<sup>&alpha;/2</sup>/((2<sup>&alpha;/2</sup>&minus;1)*n<sup>&alpha;/2</sup>).</li>
</ul>

<p>In both cases $\eta(n)$ is finite and converges to 0 as $n$ increases.</p>

<p>The result then follows from Theorem 1. &#x25a1;</p>

<blockquote>
<p><strong>Note:</strong> For specific values of <em>&alpha;</em>, the equation <em>D</em>(<em>n</em>) = <em>D</em>(2 * <em>n</em>) + <em>&phi;</em>(<em>n</em>) can be solved via linear recurrences; an example for <em>&alpha;</em> = 1/2 is the following code in Python that uses the SymPy computer algebra library: <code>rsolve(Eq(f(n),f(n+1)+z*(1/(2*2**n))**((S(1)/2)/2)),f(n)).subs(n,ln(n,2)).simplify()</code>.  Trying different values of <em>&alpha;</em> suggested the following formula for Hölder continuous functions with <em>&alpha;</em> of 1/<em>j</em> or greater: (<em>M</em>* $\sum_{i=0}^{2j-1} 2^{i/(2j)})/<em>n</em><sup>1/(2*<em>j</em>)</sup> = <em>M</em> / ((2<sup>1/(2*<em>j</em>)</sup>&minus;1)*<em>n</em><sup>1/(2*<em>j</em>)</sup>); and generalizing the latter expression led to the term in the theorem.</p>
</blockquote>

<p><strong>Corollary 2.</strong> <em>Let f(&lambda;) be a strictly bounded factory function.  If that function is Lipschitz continuous with Lipschitz constant M, then the following scheme determined by <strong>fbelow</strong> and <strong>fabove</strong> is valid in the sense of Theorem 1:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) = f(k/n) &minus; M/((sqrt(2)&minus;1)*sqrt(n)).</em></li>
<li><em><strong>fabove</strong>(n, k) = f(k/n) + M/((sqrt(2)&minus;1)*sqrt(n)).</em></li>
</ul>

<p><em>Or:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) = min(<strong>fbelow</strong>(4,0), <strong>fbelow</strong>(4,1), ..., <strong>fbelow</strong>(4,4)) if n &lt; 4; otherwise, f(k/n) &minus; M*sqrt(2/7)/((sqrt(2)&minus;1)*sqrt(n)).</em></li>
<li><em><strong>fabove</strong>(n, k) = max(<strong>fabove</strong>(4,0), <strong>fabove</strong>(4,1), ..., <strong>fabove</strong>(4,4)) if n &lt; 4; otherwise, f(k/n) + M*sqrt(2/7)/((sqrt(2)&minus;1)*sqrt(n)).</em></li>
</ul>

<p><em>Proof.</em> Because Lipschitz continuous functions are Hölder continuous with Hölder constant <em>M</em> and exponent 1, the result follows from Corollary 1. &#x25a1;</p>

<blockquote>
<p><strong>Note:</strong> The first scheme given here is a special case of Theorem 1 that was already found by Nacu and Peres (2005)[^1].</p>
</blockquote>

<p><strong>Corollary 3.</strong> <em>Let f(&lambda;) be a strictly bounded factory function. If that function has a Lipschitz continuous derivative with Lipschitz constant L, then the following scheme determined by <strong>fbelow</strong> and <strong>fabove</strong> is valid in the sense of Theorem 1:</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) = min(<strong>fbelow</strong>(4,0), <strong>fbelow</strong>(4,1), ..., <strong>fbelow</strong>(4,4)) if n &lt; 4; otherwise, f(k/n) &minus; L/(7*n).</em></li>
<li><em><strong>fabove</strong>(n, k) = max(<strong>fabove</strong>(4,0), <strong>fabove</strong>(4,1), ..., <strong>fabove</strong>(4,4)) if n &lt; 4; otherwise, f(k/n) + L/(7*n).</em></li>
</ul>

<p><em>Proof.</em> By part 3 of lemma 2, for each integer $n\ge 4$ that&#39;s a power of 2 ($n_0=4$ in Theorem 1), $\phi(n)=(L/2) (1/(7n))$ can be taken for each such integer $n$, and thus $\eta(n)=L/(7n)$ (where $\eta(n)$ is as in Theorem 1). $\eta(n)$ is finite and converges to 0 as $n$ increases. The result then follows from Theorem 1. &#x25a1;</p>

<blockquote>
<p><strong>Note:</strong> Nacu and Peres (2005)[^1] already proved a looser scheme in the case when $f$ has a second derivative on the closed unit interval that is not greater than a constant (a slightly stronger condition than having a Lipschitz continuous derivative on that domain).</p>
</blockquote>

<p><strong>Theorem 2.</strong> <em>Let f(&lambda;) be a strictly bounded factory function.  If that function is convex and nowhere decreasing, then Theorem 1 remains valid with &phi;(n) = <strong>E</strong>[f(Y/n)] (where Y is a hypergeometric(2*n, n, n) random variable), rather than as given in that theorem.</em></p>

<p><em>Proof.</em>  Follows from Theorem 1 and part 4 of Lemma 2 above. With the <em>&phi;</em> given in this theorem, the series <em>&eta;</em>(<em>n</em>) in Theorem 1 remains nonnegative; also, this theorem adopts Theorem 1&#39;s assumption that the series converges, so that <em>&eta;</em>(<em>n</em>) still decreases with increasing <em>n</em>. &#x25a1;</p>

<p><strong>Proposition 1.</strong></p>

<ol>
<li><em>Let f be as given in Theorem 1 or 2 or Corollary 1 to 3, except that f must be concave and polynomially bounded and may have a minimum of 0. Then the schemes of those results remain valid if <strong>fbelow</strong>(n, k) = f(k/n), rather than as given in those results.</em></li>
<li><em>Let f be as given in Theorem 1 or 2 or Corollary 1 to 3, except that f must be convex and polynomially bounded and may have a maximum of 1.  Then the schemes of those results remain valid if <strong>fabove</strong>(n, k) = f(k/n), rather than as given in those results.</em></li>
<li><p><em>Theorems 1 and 2 and Corollaries 1 to 3 can be extended to all integers n&ge;1, not just those that are powers of 2, by defining&mdash;</em></p>

<ul>
<li><em><strong>fbelow</strong>(n, k) = (k/n)*<strong>fbelow</strong>(n&minus;1, max(0, k&minus;1)) + ((n&minus;k)/n)*<strong>fbelow</strong>(n&minus;1, min(n&minus;1, k)), and</em></li>
<li><em><strong>fabove</strong>(n, k) = (k/n)*<strong>fabove</strong>(n&minus;1, max(0, k&minus;1)) + ((n&minus;k)/n)*<strong>fabove</strong>(n&minus;1, min(n&minus;1, k)),</em></li>
</ul>

<p><em>for every integer n&ge;1 other than a power of 2. Parts 1 and 2 of this proposition still apply to the modified scheme.</em></p></li>
</ol>

<p><em>Proof.</em> Parts 1 and 2 follow from Theorem 1 or 2 or Corollary 1 to 3, as the case may be.  For part 1, the lower polynomials are replaced by the degree-<em>n</em> Bernstein polynomials of <em>f</em>, and they meet the conditions in those theorems by Jensen&#39;s inequality.  For part 2, the upper polynomials are involved instead of the lower polynomials.  Part 3 also follows from Remark B of Nacu and Peres (2005)[^1]. &#x25a1;</p>

<p>The following lemma shows that if a scheme for $f(\lambda)$ shifts polynomials upward and downward, the pre-shifted polynomials are close to $f(\lambda)$ by the amount of the shift.</p>

<p><strong>Lemma 3.</strong> <em>Let $f$ be a strictly bounded factory function. Let $S$ be an infinite set of positive integers.  For each integer $n\ge 1$, let $W_n(\lambda)$ be a function, and let $\epsilon_n(f)$ be a nonnegative constant that depends on $f$ and $n$.  Suppose that there are polynomials $g_n$ and $h_n$ (for each $n$ in $S$) as follows:</em></p>

<ol>
<li><em>$g_n$ and $h_n$ have Bernstein coefficients $W_n(k/n) - \epsilon_n(f)$ and $W_n(k/n) + \epsilon_n(f)$, respectively ($0\le k\le n$).</em></li>
<li><em>$g_n \le h_n$.</em></li>
<li><em>$g_n$ and $h_n$ converge to $f$ as $n$ gets large.</em></li>
<li>$(g_{m}-g_n)$ <em>and</em> $(h_{n}-h_{m})$ <em>are polynomials with nonnegative Bernstein coefficients once they are rewritten to polynomials in Bernstein form of degree exactly $m$, where $m$ is the smallest number greater than $n$ in $S$.</em></li>
</ol>

<p><em>Then for each $n$ in $S$, $|f(\lambda) - B_n(W_n(\lambda))| \le \epsilon_n(f)$ whenever $0\le \lambda\le 1$, where $B_n(W_n(\lambda))$ is the Bernstein polynomial of degree $n$ of the function $W_n(\lambda)$.</em></p>

<p><em>Proof:</em> $W_n(k/n)$ is the $k$-th Bernstein coefficient of $B_n(W_n(\lambda))$, which is $g_n$ and $h_n$ before they are shifted downward and upward, respectively, by $\epsilon_n(f)$.  Moreover, property 4 in the lemma corresponds to condition (iv) of Nacu and Peres (2005)[^1], which implies that, for every $m&gt;n$, $g_{n}(\lambda)\le g_{m}(\lambda)\le f(\lambda)$ (the lower polynomials &quot;increase&quot;) and $h_{n}(\lambda)\ge h_{m}(\lambda)\ge f(\lambda)$ (the upper polynomials &quot;decrease&quot;) for every $n\ge 1$ (Nacu and Peres 2005, Remark A)[^1].</p>

<p>Then if $B_n(W_n(\lambda)) &lt; f(\lambda)$ for some $\lambda$ in the closed unit interval, shifting the left-hand side upward by $\epsilon_n(f)$ (a nonnegative constant) means that $h_n = B_n(W_n(\lambda))+\epsilon_n(f) \ge f(\lambda)$, and rearranging this expression leads to $f(\lambda) - B_n(W_n(\lambda)) \le \epsilon_n(f)$.</p>

<p>Likewise, if $B_n(W_n(\lambda)) &gt; f(\lambda)$ for some $\lambda$ in the closed unit interval, shifting the left-hand side downward by $\epsilon_n(f)$ means that $g_n = B_n(W_n(\lambda))-\epsilon_n(f) \le f(\lambda)$, and rearranging this expression leads to $B_n(W_n(\lambda)) - f(\lambda) \le \epsilon_n(f)$.</p>

<p>This combined means that $|f(x) - B_n(W_n(\lambda))| \le \epsilon_n(f)$ whenever $0\le \lambda\le 1$.  &#x25a1;</p>

<p><strong>Corollary 4</strong>.  <em>If $f(\lambda)$ satisfies a scheme given in Theorem 1 with $n_0\ge 1$, then $B_n(f(\lambda))$ comes within $\eta(n)$ of $f$ for every integer $n\ge n_0$ that&#39;s a power of 2; that is, $|B_n(f(\lambda))| \le \eta(n)$ for every such $n$.</em></p>

<p><a id=A_Conjecture_on_Polynomial_Approximation></a></p>

<h4>A Conjecture on Polynomial Approximation</h4>

<p>The following conjecture suggests there may be a way to easily adapt other approximating polynomials, besides the ordinary Bernstein polynomials, to the Bernoulli factory problem.</p>

<p><strong>Conjecture.</strong></p>

<p>Let $r\ge 1$, and let $f$ be a strictly bounded factory function whose $r$-th derivative is continuous.  Let $M$ be the maximum absolute value of $f$ and its derivatives up to the $r$-th derivative. Let $W_{2^0}(\lambda), W_{2^1}(\lambda), ..., W_{2^n}(\lambda),...$ be functions on the closed unit interval that converge uniformly to $f$ (that is, for every tolerance level, all $W_{2^i}$ after some value $i$ are within that tolerance level of $f$ at all points on the closed unit interval).</p>

<p>For each integer $n\ge1$ that&#39;s a power of 2, suppose that there is $D&gt;0$ such that&mdash; $$|f(\lambda)-B_n(W_n(\lambda))| \le DM/n^{r/2},$$ whenever $0\le \lambda\le 1$, where $B_n(W_n(\lambda))$ is the degree-$n$ Bernstein polynomial of $W_n(\lambda)$.</p>

<p>Then there is $C_0\ge D$ such that for every $C\ge C_0$, there are polynomials $g_n$ and $h_n$ (for each $n\ge 1$) as follows:</p>

<ol>
<li>$g_n$ and $h_n$ have Bernstein coefficients $W_n(k/n) - CM/n^{r/2}$ and $W_n(k/n) + CM/n^{r/2}$, respectively ($0\le k\le n$), if $n$ is a power of 2, and $g_n=g_{n-1}$ and $h_n=h_{n-1}$ otherwise.</li>
<li>$g_n$ and $h_n$ converge to $f$ as $n$ gets large.</li>
<li>$(g_{n+1}-g_{n})$ and $(h_{n}-h_{n+1})$ are polynomials with nonnegative Bernstein coefficients once they are rewritten to polynomials in Bernstein form of degree exactly $n+1$.</li>
</ol>

<p>Equivalently (see also Nacu and Peres 2005), there is $C_1&gt;0$ such that, for each integer $n\ge 1$ that&#39;s a power of 2&mdash; $$\left|\left(\sum_{i=0}^k \left(W_n\left(\frac{i}{n}\right)\right) {n\choose i}{n\choose {k-i}}/{2n \choose k}\right)-W_{2n}\left(\frac{k}{2n}\right)\right|\le \frac{C_1 M}{n^{r/2}},$$ whenever $0\le k\le 2n$, so that $C=\frac{C_1}{1-\sqrt{2/2^{r+1}}}$.</p>

<p>It is further conjectured that the same value of $C_0$ (or $C_1$) suffices when $f$ has a Lipschitz continuous $(r-1)$-th derivative and $M$ is the maximum absolute value of $f$ and the Lipschitz constants of $f$ and its derivatives up to the $(r-1)$-th derivative.</p>

<blockquote>
<p><strong>Note:</strong> By Lemma 3, $B_n(W_n(f(\lambda)))$ would be close to $f(\lambda)$ by at most $C_0 M/n^{r/2}$.  Properties 2 and 3 above correspond to (iii) and (iv) in Nacu and Peres (2005, Proposition 3)[^1].</p>
</blockquote>

<hr>

<p>The following lower bounds on $C_0$ can be shown.  In the table:</p>

<ul>
<li>$M_{0,r}$ is the maximum absolute value of $f(\lambda)$ and its $r$-th derivative (Güntürk and Li 2021)[^8].</li>
<li>$M_{1,r}$ is the maximum absolute value of $f(\lambda)$ and its derivatives up to the $r$-th derivative.  Thus, $M_{1,r}\ge M_{0,r}$.</li>
<li>The bounds are valid only if $n$ is a power-of-two integer and, unless otherwise specified, only if $n\ge 1$.</li>
</ul>

<p>&nbsp;</p>

<table><thead>
<tr>
<th>If $r$ is...</th>
<th>And...</th>
<th>And $W_n$ is...</th>
<th>Then $C_0$ must be greater than:</th>
<th>And $C_0$ is conjectured to be:</th>
<th>Because of this counterexample:</th>
</tr>
</thead><tbody>
<tr>
<td>3</td>
<td>$M=M_{0,3}$</td>
<td>$2 f - B_n(f)$*</td>
<td>2.62</td>
<td>2.63</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>3</td>
<td>$M=M_{0,3}$, $n\ge 4$</td>
<td>$2 f - B_n(f)$</td>
<td>0.66</td>
<td>0.67</td>
<td>$2\lambda(1-\lambda)$</td>
</tr>
<tr>
<td>4</td>
<td>$M=M_{0,4}$</td>
<td>$2 f - B_n(f)$</td>
<td>3.58</td>
<td>3.59</td>
<td>$3 \lambda^{2} \cdot \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>4</td>
<td>$M=M_{0,4}$, $n\ge 4$</td>
<td>$2 f - B_n(f)$</td>
<td>3.52</td>
<td>3.53</td>
<td>$\lambda^{2} \cdot \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>3</td>
<td>$M=M_{1,3}$</td>
<td>$2 f - B_n(f)$</td>
<td>0.29</td>
<td>$\frac{3}{16-4 \sqrt{2}}$ &lt; 0.29005.**</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>3</td>
<td>$M=M_{1,3}$, $n\ge 4$</td>
<td>$2 f - B_n(f)$</td>
<td>0.08</td>
<td>0.09</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>4</td>
<td>$M=M_{1,4}$</td>
<td>$2 f - B_n(f)$</td>
<td>0.24</td>
<td>0.25</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>4</td>
<td>$M=M_{1,4}$, $n\ge 4$</td>
<td>$2 f - B_n(f)$</td>
<td>0.14</td>
<td>0.15</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>5</td>
<td>$M=M_{1,5}$</td>
<td>$B_n(B_n(f))+3(f-B_n(f))$***</td>
<td>0.26</td>
<td>0.27</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>5</td>
<td>$M=M_{1,5}$, $n\ge 4$</td>
<td>$B_n(B_n(f))+3(f-B_n(f))$</td>
<td>0.10</td>
<td>0.11</td>
<td>$3 \lambda^{2} \cdot \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>6</td>
<td>$M=M_{1,6}$</td>
<td>$B_n(B_n(f))+3(f-B_n(f))$</td>
<td>0.24</td>
<td>0.25</td>
<td>$2 \lambda \left(1 - \lambda\right)$</td>
</tr>
<tr>
<td>6</td>
<td>$M=M_{1,6}$, $n\ge 4$</td>
<td>$B_n(B_n(f))+3(f-B_n(f))$</td>
<td>0.22</td>
<td>0.23</td>
<td>$3 \lambda^{2} \cdot \left(1 - \lambda\right)$</td>
</tr>
</tbody></table>

<p>* Corresponds to the iterated Bernstein polynomial of order 2 (Güntürk and Li 2021)[^8].</p>

<p>*** Corresponds to the iterated Bernstein polynomial of order 3 (Güntürk and Li 2021)[^8].</p>

<p>** The following is evidence for the conjectured bound, at least if $f(0)=f(1)$.</p>

<p>The Bernstein polynomials $B_n(f)$ satisfy the partition-of-unity property: $\sum_{k=0}^n (c+f(k/n)) p_{n,k} = c+\sum_{k=0}^n f(k/n) p_{n,k}$ for every $c$, where $\sum_{k=0}^n p_{n,k}=1$.  So do the hypergeometric probabilities $\sigma_{n,k,i} = {n\choose i}{n\choose {k-i}}/{2n \choose k}$.  Thus, rewrite $\sum_{k=0}^n (c+W_n(i/n)) \sigma_{n,k,i}$ = $\sum_{k=0}^n (2 (c+f) - B_n(c+f)) \sigma_{n,k,i}$ = $\sum_{k=0}^n (2c + 2f - c - B_n(f)) \sigma_{n,k,i}$ = $\sum_{k=0}^n (c + 2f - B_n(f)) \sigma_{n,k,i}$ = $c + \sum_{k=0}^n W_n(i/n) \sigma_{n,k,i}$.  And rewrite $c + \sum_{k=0}^n W_n(i/n) \sigma_{n,k,i} - (c+W_{2n}(k/(2n))) = \sum_{k=0}^n W_n(i/n) \sigma_{n,k,i} - W_{2n}(k/(2n))$. Thus, when $W_n = 2f-B_n(f)$, $f(0)$ can equal 0 without loss of generality.</p>

<p>Suppose $f(0) = f(1) = 0$, then $W_1(f)$ will equal 0.  Let $X$ be a hypergeometric(2,k,1) random variable (for $k$ equal to 0, 1, or 2). Then $E[f(X/1)]$ will likewise equal 0.  Thus, since $W_n(f)(0)=f(0)$ and $W_n(f)(1)=f(1)$ for every $n$, and since $W_2(f)(1/2) = 2f(1/2) - B_2(f)(1/2)$ (and thus considers only the values of $f$ at 0, 1/2, and 1), $|E[f(X/1)] - f(k/2)|$ will take its maximum at $k=1$, namely $|0 - (2f(1/2) - B_2(f)(1/2))| = |- 3f(1/2)/2|$. That right-hand side is the minimum shift needed for the consistency requirement to hold; call it $z$, and let $y$ be $M_{1,3}$. To get the minimum $C_0$ that works, solve $C_0 y/1^{3/2} - C_0 y/2^{3/2}$ = z for $C_0$.  The solution is&mdash; $$C_0=\frac{z}{y} \frac{4}{4-\sqrt{2}} = \frac{|0-f(1/2)|}{y} \frac{12}{2\cdot (4-\sqrt{2})}.$$</p>

<p>The solution shows that if $y = M_{1,3}$ can come arbitrarily close to 0, then no value for $C_0$ will work.  Which is why the goal is now to find a tight upper bound on the least possible value of $M_{1,3}$ for $r=3$ such that:</p>

<ol>
<li>$f(\lambda)$ has a continuous third derivative.</li>
<li>$f(0)=f(1)=0$ and $0 &lt; f(1/2) &lt; 1$.</li>
</ol>

<p>Take the function $g(\lambda)=2\lambda(1-\lambda)$, which satisfies (1), (2), and $g(0)=g(1)=0$ and $g(1/2)=1/2$, and has an $M_{1,3}$ of 4.  Given that $\frac{|0-g(1/2)|}{y}=\frac{|0-1/2|}{y}=1/8$, the goal is now to see whether any function $f$ satisfying (1) and (2) has $\max(0, f(1/2)) &lt; M_{1,3} &lt; 8\cdot |0-f(1/2)|=8 f(1/2)$.</p>

<p>To aid in this goal, there is a formula to find the least possible Lipschitz constant for $f$&#39;s first derivative (see &quot;Definitions&quot;)[^43], given a finite set of points (0, 1/2, and 1 in the case at hand) and the values of $f$ and $f&#39;$ at those points (Le Gruyer 2009)[^44]; see also (Herbert-Voss et al. 2017)[^45]. Denote $L(.,.,.)$ as this least possible Lipschitz constant.  Then according to that formula&mdash; $$L([0, 1/2, 1], [0, t, 0], [z_1, z_2, z_3]) = \max(2 \sqrt{\left|{z_{1} - z_{2}}\right|^{2} + \left|{- 4 t + z_{1} + z_{2}}\right|^{2}} + 2 \left|{- 4 t + z_{1} + z_{2}}\right|,$$ $$\sqrt{\left|{z_{1} - z_{3}}\right|^{2} + \left|{z_{1} + z_{3}}\right|^{2}} + \left|{z_{1} + z_{3}}\right|,$$ $$2 \sqrt{\left|{z_{2} - z_{3}}\right|^{2} + \left|{4 t + z_{2} + z_{3}}\right|^{2}} + 2 \left|{4 t + z_{2} + z_{3}}\right|)$$ (where $t$ is greater than 0 and less than 1), so only $f&#39;$ values in the interval $[-8f(1/2), 8f(1/2)]$ have to be checked.</p>

<p>Let $H = 8\cdot |\beta-f(1/2)|$. In this case, only values of $f$ in the closed unit interval have to be checked and only $f&#39;$ values in $[-H, H]$ have to be checked.</p>

<p>Assuming that no $M_{1,3}$ less than $8\cdot |0-f(1/2)|$ is found, then&mdash; $$C_0=\frac{|0-f(1/2)|}{y} \frac{12}{2\cdot (4-\sqrt{2})}=\frac{|0-f(1/2)|}{H} \frac{12}{2\cdot (4-\sqrt{2})}=(1/8) \frac{12}{2\cdot (4-\sqrt{2})}$$ $$=3/(16-4\sqrt{2}),$$ which is the conjectured lower bound for $C_0$.</p>

<p><a id=Example_of_Polynomial_Building_Scheme></a></p>

<h3>Example of Polynomial-Building Scheme</h3>

<p>The following example uses the results above to build a polynomial-building scheme for a factory function.</p>

<p>Let <em>f</em>(<em>&lambda;</em>) = 0 if <em>&lambda;</em> is 0, and (ln(<em>&lambda;</em>/exp(3)))<sup>&minus;2</sup> otherwise. (This function is not Hölder continuous; its slope is exponentially steep at the point 0.)  Then the following scheme is valid in the sense of Theorem 1, subject to that theorem&#39;s bounding note:</p>

<ul>
<li><em>&eta;</em>(<em>k</em>) = &Phi;(1, 2, (ln(<em>k</em>)+ln(7)+6)/ln(2))*4/ln(2)<sup>2</sup>.</li>
<li><strong>fbelow</strong>(n, k) = f(<em>k</em>/<em>n</em>).</li>
<li><strong>fabove</strong>(n, k) = max(<strong>fabove</strong>(4,0), <strong>fabove</strong>(4,1), ..., <strong>fabove</strong>(4,4)) if n &lt; 4; otherwise, f(<em>k</em>/<em>n</em>) +  <em>&eta;</em>(<em>n</em>).</li>
</ul>

<p>Where &Phi;(.) is a function called the <em>Lerch transcendent</em>.</p>

<p>The first step is to find a concave modulus of continuity of <em>f</em> (called <em>&omega;</em>(<em>h</em>)).  Because <em>f</em> is strictly increasing and concave, and because <em>f</em>(0) = 0, we can take <em>&omega;</em>(<em>h</em>) = <em>f</em>(<em>h</em>).</p>

<p>Now, by plugging sqrt(1/(7*<em>n</em>)) into <em>&omega;</em>, we get the following for Theorem 2 (assuming <em>n</em>&ge;0):</p>

<ul>
<li><em>&phi;</em>(<em>n</em>) = 1/(ln(sqrt(7/<em>n</em>)/7)&minus;3)<sup>2</sup>.</li>
</ul>

<p>Now, by applying Theorem 1, we compute <em>&eta;</em>(<em>k</em>) by substituting <em>n</em> with 2<sup><em>n</em></sup>, summing over [<em>k</em>, &infin;), and substituting <em>k</em> with ln(<em>k</em>)/ln(2).  <em>&eta;</em> converges, resulting in:</p>

<ul>
<li><em>&eta;</em>(<em>k</em>) = &Phi;(1, 2, (ln(<em>k</em>)+ln(7)+6)/ln(2))*4/ln(2)<sup>2</sup>,</li>
</ul>

<p>where &Phi;(.) is the Lerch transcendent.  This <em>&eta;</em> matches the <em>&eta;</em> given in the scheme above.  That scheme then follows from Theorems 1 and 2, as well as from part 1 of Proposition 1 because <em>f</em> is concave.</p>

<p>the following code in Python that uses the SymPy computer algebra library is an example of finding the parameters for this polynomial-building scheme.</p>

<pre>px=Piecewise((0,Eq(x,0)),((ln(x/exp(3))**-2),True))
# omega is modulus of continuity.  Since
# px is strictly increasing, concave, and px(0)=0,
# we can take omega as px
omega=px
omega=piecewise_fold(omega.rewrite(Piecewise)).simplify()
# compute omega
phi=omega.subs(x,sqrt(1/(7*n)))
pprint(phi)
# compute eta
eta=summation(phi.subs(n,2**n),(n,k,oo)).simplify()
eta=eta.subs(k,log(k,2)) # Replace k with ln(k)/ln(2)
pprint(eta)
for i in range(20):
  # Calculate upper bounds for eta at certain points.
  try:
    print(&quot;eta(2^%d) ~= %s&quot; % (i,ceiling(eta.subs(k,2**i)*10000000).n()/10000000))
  except:
    print(&quot;eta(2^%d) ~= [FAILED]&quot; % (i))
</pre>

<p><a id=Pushdown_Automata_and_Algebraic_Functions></a></p>

<h3>Pushdown Automata and Algebraic Functions</h3>

<p>This section has mathematical proofs showing which kinds of algebraic functions (functions that can be a solution of a nonzero polynomial equation) can be simulated with a pushdown automaton (a state machine with a stack).</p>

<p>The following summarizes what can be established about these algebraic functions:</p>

<ul>
<li>sqrt(<em>&lambda;</em>) can be simulated.</li>
<li>Every rational function with rational coefficients that maps the open interval (0, 1) to itself can be simulated.</li>
<li>If <em>f</em>(<em>&lambda;</em>) can be simulated, so can any Bernstein-form polynomial in the variable <em>f</em>(<em>&lambda;</em>) with coefficients that can be simulated.</li>
<li>If <em>f</em>(<em>&lambda;</em>) and <em>g</em>(<em>&lambda;</em>) can be simulated, so can <em>f</em>(<em>&lambda;</em>)*<em>g</em>(<em>&lambda;</em>), <em>f</em>(<em>g</em>(<em>&lambda;</em>)), and <em>g</em>(<em>f</em>(<em>&lambda;</em>)).</li>
<li>If a full-domain pushdown automaton (defined later) can generate words of a given length with a given probability (a <em>probability distribution</em> of word lengths), then the probability generating function for that distribution can be simulated, as well as for that distribution conditioned on a finite set or periodic infinite set of word lengths (for example, all odd word lengths only).</li>
<li>If a stochastic context-free grammar (defined later) can generate a probability distribution of word lengths, and terminates with probability 1, then the probability generating function for that distribution can be simulated.</li>
<li>Every quadratic irrational number between 0 and 1 can be simulated.</li>
</ul>

<p>It is not yet known whether the following functions can be simulated:</p>

<ul>
<li><em>&lambda;</em><sup>1/<em>p</em></sup> for prime numbers <em>p</em> greater than 2. The answer may be no; Banderier and Drmota (2015)[^46] proved results that show, among other things, that $\lambda^{1/p}$, where $p$ is not a power of 2, is not a possible solution to $P(\lambda) = 0$, where $P(\lambda)$ is a polynomial whose coefficients are non-negative real numbers.</li>
<li>min(<em>&lambda;</em>, 1&minus;<em>&lambda;</em>).</li>
</ul>

<hr>

<p>The following definitions are used in this section:</p>

<ol>
<li><p>A <em>pushdown automaton</em> has a finite set of <em>states</em> and a finite set of <em>stack symbols</em>, one of which is called EMPTY, and takes a coin that shows heads with an unknown probability. It starts at a given state and its stack starts with EMPTY.  On each iteration:</p>

<ul>
<li>The automaton flips the coin.</li>
<li>Based on the coin flip (HEADS or TAILS), the current state, and the top stack symbol, it moves to a new state (or keeps it unchanged) and replaces the top stack symbol with zero, one or two symbols.  Thus, there are three kinds of <em>transition rules</em>:

<ul>
<li>(<em>state</em>, <em>flip</em>, <em>symbol</em>) &rarr; (<em>state2</em>, {<em>symbol2</em>}): move to <em>state2</em>, replace top stack symbol with same or different one.</li>
<li>(<em>state</em>, <em>flip</em>, <em>symbol</em>) &rarr; (<em>state2</em>, {<em>symbol2</em>, <em>new</em>}): move to <em>state2</em>, replace top stack symbol with <em>symbol2</em>, then <em>push</em> a new symbol (<em>new</em>) onto the stack.</li>
<li>(<em>state</em>, <em>flip</em>, <em>symbol</em>) &rarr; (<em>state2</em>, {}): move to <em>state2</em>, <em>pop</em> the top symbol from the stack.</li>
</ul></li>
</ul>

<p>When the stack is empty, the machine stops, and returns either 0 or 1 depending on the state it ends up at.  (Because each left-hand side has no more than one possible transition, the automaton is <em>deterministic</em>.)</p></li>
<li><p>A <em>full-domain pushdown automaton</em> means a pushdown automaton that terminates with probability 1 given a coin with probability of heads <em>&lambda;</em>, for every <em>&lambda;</em> greater than 0 and less than 1.</p></li>
<li><strong>PDA</strong> is the class of functions <em>f</em>(<em>&lambda;</em>) that satisfy 0&lt;<em>f</em>(<em>&lambda;</em>)&lt;1 whenever 0&lt;<em>&lambda;</em>&lt;1, and can be simulated by a full-domain pushdown automaton.  <strong>PDA</strong> also includes the constant functions 0 and 1.</li>
<li><strong>ALGRAT</strong> is the class of functions that satisfy 0&lt;<em>f</em>(<em>&lambda;</em>)&lt;1 whenever 0&lt;<em>&lambda;</em>&lt;1, are continuous, and are algebraic over the rational numbers (they satisfy a nonzero polynomial system whose coefficients are rational numbers). <strong>ALGRAT</strong> also includes the constant functions 0 and 1.</li>
<li>A <em>probability generating function</em> has the form <em>p</em><sub>0</sub>*<em>&lambda;</em><sup>0</sup> + <em>p</em><sub>1</sub>*<em>&lambda;</em><sup>1</sup> + ..., where <em>p</em><sub><em>i</em></sub> (a <em>coefficient</em>) is the probability of getting <em>i</em>.</li>
</ol>

<blockquote>
<p><strong>Notes:</strong></p>

<ol>
<li>Mossel and Peres (2005)[^23] defined pushdown automata to start with a non-empty stack of <em>arbitrary</em> size, and to allow each rule to replace the top symbol with an <em>arbitrary</em> number of symbols.  Both cases can be reduced to the definition in this section.</li>
<li>Pushdown automata, as defined here, are very similar to so-called <em>probabilistic right-linear indexed grammars</em> (Icard 2020)[^47] and can be translated to those grammars as well as to <em>probabilistic pushdown systems</em> (Etessami and Yannakakis 2009)[^47], as long as those grammars and systems use only transition probabilities that are rational numbers.</li>
</ol>
</blockquote>

<p><strong>Proposition 0</strong> (Mossel and Peres 2005[^23], Theorem 1.2): <em>A full-domain pushdown automaton can simulate a function that maps (0, 1) to itself only if the function is in class <strong>ALGRAT</strong>.</em></p>

<p>It is not known whether <strong>ALGRAT</strong> and <strong>PDA</strong> are equal, but the following can be established about <strong>PDA</strong>:</p>

<p><strong>Lemma 1A:</strong> <em>Let g(&lambda;) be a function in the class <strong>PDA</strong>, and suppose a pushdown automaton F has two rules of the form (<code>state</code>, HEADS, <code>stacksymbol</code>) &rarr; RHS1 and (<code>state</code>, TAILS, <code>stacksymbol</code>) &rarr; RHS2, where <code>state</code> and <code>stacksymbol</code> are a specific state/symbol pair among the left-hand sides of F&#39;s rules.  Then there is a pushdown automaton that transitions to RHS1 with probability g(&lambda;) and to RHS2 with probability 1&minus;g(&lambda;) instead.</em></p>

<p><em>Proof:</em> If RHS1 and RHS2 are the same, then the conclusion holds and nothing has to be done.  Thus assume RHS1 and RHS2 are different.</p>

<p>Let <em>G</em> be the full-domain pushdown automaton for <em>g</em>. Assume that machines <em>F</em> and <em>G</em> stop when they pop EMPTY from the stack. If this is not the case, transform both machines by renaming the symbol EMPTY to EMPTY&prime;&prime;, adding a new symbol EMPTY&prime;&prime; and new starting state X0, and adding rules (X0, <em>flip</em>, EMPTY) &rarr; (<em>start</em>, {EMPTY&prime;&prime;}) and rule (<em>state</em>, <em>flip</em>, EMPTY) &rarr; (<em>state</em>, {}) for all states other than X0, where <em>start</em> is the starting state of <em>F</em> or <em>G</em>, as the case may be.</p>

<p>Now, rename each state of <em>G</em> as necessary so that the sets of states of <em>F</em> and of <em>G</em> are disjoint.  Then, add to <em>F</em> a new stack symbol EMPTY&prime; (or a name not found in the stack symbols of G, as the case may be).  Then, for the following two pairs of rules in <em>F</em>, namely&mdash;</p>

<p>(<em>state</em>, HEADS, <em>stacksymbol</em>) &rarr; (<em>state2heads</em>, <em>stackheads</em>), and<br>
(<em>state</em>, TAILS, <em>stacksymbol</em>) &rarr; (<em>state2tails</em>, <em>stacktails</em>),</p>

<p>add two new states <em>state</em><sub>0</sub> and <em>state</em><sub>1</sub> that correspond to <em>state</em> and have names different from all other states, and replace that rule with the following rules:</p>

<p>(<em>state</em>, HEADS, <em>stacksymbol</em>) &rarr; (<em>gstart</em>, {<em>stacksymbol</em>, EMPTY&prime;}),<br>
(<em>state</em>, TAILS, <em>stacksymbol</em>) &rarr; (<em>gstart</em>, {<em>stacksymbol</em>, EMPTY&prime;}),<br>
(<em>state</em><sub>0</sub>, HEADS, <em>stacksymbol</em>) &rarr; (<em>state2heads</em>, <em>stackheads</em>),<br>
(<em>state</em><sub>0</sub>, TAILS, <em>stacksymbol</em>) &rarr; (<em>state2heads</em>, <em>stackheads</em>),<br>
(<em>state</em><sub>1</sub>, HEADS, <em>stacksymbol</em>) &rarr; (<em>state2tails</em>, <em>stacktails</em>), and<br>
(<em>state</em><sub>1</sub>, TAILS, <em>stacksymbol</em>) &rarr; (<em>state2tails</em>, <em>stacktails</em>),<br></p>

<p>where <em>gstart</em> is the starting state for <em>G</em>, and copy the rules of the automaton for <em>G</em> onto <em>F</em>, but with the following modifications:</p>

<ul>
<li>Replace the symbol EMPTY in <em>G</em> with EMPTY&prime;.</li>
<li>Replace each state in <em>G</em> with a name distinct from all other states in <em>F</em>.</li>
<li>Replace each rule in <em>G</em> of the form (<em>state</em>, <em>flip</em>, EMPTY&prime;) &rarr; (<em>state2</em>, {}), where <em>state2</em> is a final state of <em>G</em> associated with output 1, with the rule (<em>state</em>, <em>flip</em>, EMPTY&prime;) &rarr; ( <em>state</em><sub>1</sub>, {}).</li>
<li>Replace each rule in <em>G</em> of the form (<em>state</em>, <em>flip</em>, EMPTY&prime;) &rarr; (<em>state2</em>, {}), where <em>state2</em> is a final state of <em>G</em> associated with output 0, with the rule (<em>state</em>, <em>flip</em>, EMPTY&prime;) &rarr; ( <em>state</em><sub>0</sub>, {}).</li>
</ul>

<p>Then, the final states of the new machine are the same as those for the original machine <em>F</em>. &#x25a1;</p>

<p><strong>Lemma 1B:</strong>  <em>There are pushdown automata that simulate the probabilities 0 and 1.</em></p>

<p><em>Proof:</em> The probability 0 automaton has the rules (START, HEADS, EMPTY) &rarr; (START, {}) and (START, TAILS, EMPTY) &rarr; (START, {}), and its only state START is associated with output 0. The probability 1 automaton is the same, except START is associated with output 1.  Both automata obviously terminate with probability 1. &#x25a1;</p>

<p>Because of Lemma 1A, it&#39;s possible to label each left-hand side of a pushdown automaton&#39;s rules with not just HEADS or TAILS, but also a rational number or another function in <strong>PDA</strong>, as long as for each state/symbol pair, the probabilities for that pair sum to 1.  For example, rules like the following are now allowed:</p>

<p>(START, 1/2, EMPTY) &rarr; ..., (START, sqrt(<em>&lambda;</em>)/2, EMPTY) &rarr; ..., (START, (1 &minus; sqrt(<em>&lambda;</em>))/2, EMPTY) &rarr; ....</p>

<p><strong>Proposition 1A:</strong> <em>If f(&lambda;) is in the class <strong>PDA</strong>, then so is every polynomial written as&mdash;</em></p>

<p>$${n\choose 0}f(\lambda)^0 (1-f(\lambda))^{n-0} a[0] + {n\choose 1}f(\lambda)^1 (1-f(\lambda))^{n-1} a[1] + ... + {n\choose n}f(\lambda)^n (1-f(\lambda))^{n-n} a[n],$$</p>

<p><em>where n is the polynomial&#39;s degree and a[0], a[1], ..., a[n] are functions in the class <strong>PDA</strong>.</em></p>

<p><em>Proof Sketch</em>: This corresponds to a two-stage pushdown automaton that follows the algorithm of Goyal and Sigman (2012)[^17]: The first stage counts the number of &quot;heads&quot; shown when flipping the f(&lambda;) coin, and the second stage flips another coin that has success probability <em>a</em>[<em>i</em>], where <em>i</em> is the number of &quot;heads&quot;. The automaton&#39;s transitions take advantage of Lemma 1A.  &#x25a1;</p>

<p><strong>Proposition 1:</strong> <em>If f(&lambda;) and g(&lambda;) are functions in the class <strong>PDA</strong>, then so is their product, namely f(&lambda;)*g(&lambda;).</em></p>

<p><em>Proof:</em> Special case of Proposition 1A with <em>n</em>=1, <em>f</em>(<em>&lambda;</em>)=<em>f</em>(<em>&lambda;</em>), <em>a</em>[0]=0 (using Lemma 1B), and <em>a</em>[1]=<em>g</em>(<em>&lambda;</em>).  &#x25a1;</p>

<p><strong>Corollary 1A:</strong> <em>If f(&lambda;), g(&lambda;), and h(&lambda;) are functions in the class <strong>PDA</strong>, then so is f(&lambda;)*g(&lambda;) + (1&minus;f(&lambda;))*h(&lambda;).</em></p>

<p><em>Proof:</em> Special case of Proposition 1A with <em>n</em>=1, <em>f</em>(<em>&lambda;</em>)=<em>f</em>(<em>&lambda;</em>), <em>a</em>[0]=<em>h</em>(<em>&lambda;</em>), and <em>a</em>[1]=<em>g</em>(<em>&lambda;</em>).  &#x25a1;</p>

<p><strong>Proposition 2:</strong> <em>If f(&lambda;) and g(&lambda;) are functions in the class <strong>PDA</strong>, then so is their composition, namely f(g(&lambda;)) or f&#x2218;g(&lambda;).</em></p>

<p><em>Proof:</em> Let <em>F</em> be the full-domain pushdown automaton for <em>f</em>. For each state/symbol pair among the left-hand sides of <em>F</em>&#39;s rules, apply Lemma 1A to the automaton <em>F</em>, using the function <em>g</em>.  Then the new machine <em>F</em> terminates with probability 1 because the original <em>F</em> and the original automaton for <em>g</em> do for every <em>&lambda;</em> greater than 0 and less than 1, and because the automaton for <em>g</em> never outputs the same value with probability 0 or 1 for any <em>&lambda;</em> greater than 0 or less than 1.  Moreover, <em>f</em> is in class <strong>PDA</strong> by Theorem 1.2 of (Mossel and Peres 2005)[^23] because the machine is a full-domain pushdown automaton.  &#x25a1;</p>

<p><strong>Proposition 3:</strong> <em>Every rational function with rational coefficients that maps (0, 1) to itself is in class <strong>PDA</strong>.</em></p>

<p><em>Proof:</em> These functions can be simulated by a finite-state machine (Mossel and Peres 2005)[^23].  This corresponds to a full-domain pushdown automaton that has no stack symbols other than EMPTY, never pushes symbols onto the stack, and pops the only symbol EMPTY from the stack whenever it transitions to a final state of the finite-state machine. &#x25a1;</p>

<blockquote>
<p><strong>Note:</strong> An unbounded stack size is necessary for a pushdown automaton to simulate functions that a finite-state machine can&#39;t.  With a bounded stack size, there is a finite-state machine where each state not only holds the pushdown automaton&#39;s original state, but also encodes the contents of the stack (which is possible because the stack&#39;s size is bounded); each operation that would push, pop, or change the top symbol transitions to a state with the appropriate encoding of the stack instead.</p>
</blockquote>

<p><strong>Proposition 4:</strong> <em>If a full-domain pushdown automaton can generate words with the same letter such that the length of each word follows a probability distribution, then that distribution&#39;s probability generating function is in class <strong>PDA</strong>.</em></p>

<p><em>Proof:</em> Let <em>F</em> be a full-domain pushdown automaton.  Add one state FAILURE, then augment <em>F</em> with a special &quot;letter-generating&quot; operation as follows.  Add the following rule that pops all symbols from the stack:</p>

<p>(FAILURE, <em>flip</em>, <em>stacksymbol</em>) &rarr; (FAILURE, {}),</p>

<p>and for each rule of the following form that transitions to a letter-generating operation (where S and T are arbitrary states):</p>

<p>(S, <em>flip</em>, <em>stacksymbol</em>) &rarr; (T, <em>newstack</em>),</p>

<p>add another state S&prime; (with a name that differs from all other states) and replace that rule with the following rules:</p>

<p>(S, <em>flip</em>, <em>stacksymbol</em>) &rarr; (S&prime;, {<em>stacksymbol</em>}),<br/>
(S&prime;, HEADS, <em>stacksymbol</em>) &rarr; (T, <em>newstack</em>), and<br/>
(S&prime;, TAILS, <em>stacksymbol</em>) &rarr; (FAILURE, {}).</p>

<p>Then if the stack is empty upon reaching the FAILURE state, the result is 0, and if the stack is empty upon reaching any other state, the result is 1.  By Dughmi et al. (2021)[^48], the machine now simulates the distribution&#39;s probability generating function.  Moreover, the function is in class <strong>PDA</strong> by Theorem 1.2 of Mossel and Peres (2005)[^23] because the machine is a full-domain pushdown automaton.  &#x25a1;</p>

<p>Define a <em>stochastic context-free grammar</em> as follows.  The grammar consists of a finite set of <em>nonterminals</em> and a finite set of <em>letters</em>, and rewrites one nonterminal (the starting nonterminal) into a word.  The grammar has three kinds of rules (in generalized Chomsky Normal Form (Etessami and Yannakakis 2009)[^49]):</p>

<ul>
<li><em>X</em> &rarr; <em>a</em> (rewrite <em>X</em> to the letter <em>a</em>).</li>
<li><em>X</em> &rarr;<sub><em>p</em></sub> (<em>a</em>, <em>Y</em>) (with rational probability <em>p</em>, rewrite <em>X</em> to the letter <em>a</em> followed by the nonterminal <em>Y</em>).  For the same left-hand side, all the <em>p</em> must sum to 1.</li>
<li><em>X</em> &rarr; (<em>Y</em>, <em>Z</em>) (rewrite <em>X</em> to the nonterminals <em>Y</em> and <em>Z</em> in that order).</li>
</ul>

<p>Instead of a letter (such as <em>a</em>), a rule can use <em>&epsilon;</em> (the empty string). (The grammar is <em>context-free</em> because the left-hand side has only a single nonterminal, so that no context from the word is needed to parse it.)</p>

<p><strong>Proposition 5:</strong> <em>Every stochastic context-free grammar can be transformed into a pushdown automaton.  If the automaton is a full-domain pushdown automaton and the grammar has a one-letter alphabet, the automaton can generate words such that the length of each word follows the same distribution as the grammar, and that distribution&#39;s probability generating function is in class <strong>PDA</strong>.</em></p>

<p><em>Proof Sketch:</em> In the equivalent pushdown automaton:</p>

<ul>
<li><em>X</em> &rarr; <em>a</em> becomes the two rules&mdash;<br>(START, HEADS, <em>X</em>) &rarr; (<em>letter</em>, {}), and<br>(START, TAILS, <em>X</em>) &rarr; (<em>letter</em>, {}).<br>Here, <em>letter</em> is either START or a unique state in <em>F</em> that &quot;detours&quot; to a letter-generating operation for <em>a</em> and sets the state back to START when finished (see Proposition 4).  If <em>a</em> is <em>&epsilon;</em>, <em>letter</em> is START and no letter-generating operation is done.</li>
<li><em>X</em> &rarr;<sub><em>p</em><sub><em>i</em></sub></sub> (<em>a</em><sub><em>i</em></sub>, <em>Y</em><sub><em>i</em></sub>) (all rules with the same nonterminal <em>X</em>) are rewritten to enough rules to transition to a letter-generating operation for <em>a</em><sub><em>i</em></sub>, and swap the top stack symbol with <em>Y</em><sub><em>i</em></sub>, with probability <em>p</em><sub><em>i</em></sub>, which is possible with just a finite-state machine (see Proposition 4) because all the probabilities are rational numbers (Mossel and Peres 2005)[^23].  If <em>a</em><sub><em>i</em></sub> is <em>&epsilon;</em>, no letter-generating operation is done.</li>
<li><em>X</em> &rarr; (<em>Y</em>, <em>Z</em>) becomes the two rules&mdash;<br>(START, HEADS, <em>X</em>) &rarr; (START, {<em>Z</em>, <em>Y</em>}), and<br>(START, TAILS, <em>X</em>) &rarr; (START, {<em>Z</em>, <em>Y</em>}).</li>
</ul>

<p>Here, <em>X</em> is the stack symbol EMPTY if <em>X</em> is the grammar&#39;s starting nonterminal. Now, assuming the automaton is full-domain, the rest of the result follows easily.   For a single-letter alphabet, the grammar corresponds to a system of polynomial equations, one for each rule in the grammar, as follows:</p>

<ul>
<li><em>X</em> &rarr; <em>a</em> becomes <em>X</em> = 1 if <em>a</em> is the empty string (<em>&epsilon;</em>), or <em>X</em> =  <em>&lambda;</em> otherwise.</li>
<li>For each nonterminal <em>X</em>, all <em>n</em> rules of the form <em>X</em> &rarr;<sub><em>p</em><sub><em>i</em></sub></sub> (<em>a</em><sub><em>i</em></sub>, <em>Y</em><sub><em>i</em></sub>) become the equation <em>X</em> = <em>p</em><sub>1</sub>*<em>&lambda;</em><sub>1</sub>*<em>Y</em><sub>1</sub> + <em>p</em><sub>2</sub>*<em>&lambda;</em><sub>2</sub>*<em>Y</em><sub>2</sub> + ... + <em>p</em><sub><em>n</em></sub>*<em>&lambda;</em><sub><em>n</em></sub>*<em>Y</em><sub><em>n</em></sub>, where <em>&lambda;</em><sub><em>i</em></sub> is either 1 if <em>a</em><sub><em>i</em></sub> is <em>&epsilon;</em>, or <em>&lambda;</em> otherwise.</li>
<li><em>X</em> &rarr; (<em>Y</em>, <em>Z</em>) becomes <em>X</em> = <em>Y</em>*<em>Z</em>.</li>
</ul>

<p>Solving this system for the grammar&#39;s starting nonterminal, and applying Proposition 4, leads to the <em>probability generating function</em> for the grammar&#39;s word distribution.  (See also Flajolet et al. 2010[^22], Icard 2020[^38].) &#x25a1;</p>

<blockquote>
<p><strong>Example:</strong> The stochastic context-free grammar&mdash;<br><em>X</em> &rarr;<sub>1/2</sub> (<em>a</em>, <em>X1</em>),<br><em>X1</em> &rarr; (<em>X</em>, <em>X2</em>),<br><em>X2</em> &rarr; (<em>X</em>, <em>X</em>),<br><em>X</em> &rarr;<sub>1/2</sub> (<em>a</em>, <em>X3</em>),<br><em>X3</em> &rarr; <em>&epsilon;</em>,<br>which encodes ternary trees (Flajolet et al. 2010)[^50], corresponds to the equation <em>X</em> = (1/2) * <em>&lambda;</em>*<em>X</em>*<em>X</em>*<em>X</em> + (1/2)*<em>&lambda;</em>*1, and solving this equation for <em>X</em> leads to the probability generating function for such trees, which is a complicated expression.</p>

<p><strong>Notes:</strong></p>

<ol>
<li><p>A stochastic context-free grammar in which all the probabilities are 1/2 is called a <em>binary stochastic grammar</em> (Flajolet et al. 2010)[^22].  If every probability is a multiple of 1/<em>n</em>, then the grammar can be called an &quot;<em>n</em>-ary stochastic grammar&quot;.  It is even possible for a nonterminal to have two rules of probability <em>&lambda;</em> and (1&minus; <em>&lambda;</em>), which are used when the input coin returns 1 (HEADS) or 0 (TAILS), respectively.</p></li>
<li><p>If a pushdown automaton simulates the function <em>f</em>(<em>&lambda;</em>), then <em>f</em> corresponds to a special system of equations, built as follows (Mossel and Peres 2005)[^23]; see also Esparza et al. (2004)[^51].  For each state of the automaton (call the state <em>en</em>), include the following equations in the system based on the automaton&#39;s transition rules:</p>

<ul>
<li>(<em>st</em>, <em>p</em>, <em>sy</em>) &rarr; (<em>s2</em>, {}) becomes either <em>&alpha;</em><sub><em>st</em>,<em>sy</em>,<em>en</em></sub> = <em>p</em> if <em>s2</em> is <em>en</em>, or <em>&alpha;</em><sub><em>st</em>,<em>sy</em>,<em>en</em></sub> = 0 otherwise.</li>
<li>(<em>st</em>, <em>p</em>, <em>sy</em>) &rarr; (<em>s2</em>, {<em>sy1</em>}) becomes <em>&alpha;</em><sub><em>st</em>,<em>sy</em>,<em>en</em></sub> = <em>p</em> * <em>&alpha;</em><sub><em>s2</em>,<em>sy1</em>,<em>en</em></sub>.</li>
<li>(<em>st</em>, <em>p</em>, <em>sy</em>) &rarr; (<em>s2</em>, {<em>sy1</em>, <em>sy2</em>}) becomes <em>&alpha;</em><sub><em>st</em>,<em>sy</em>,<em>en</em></sub> = <em>p</em>*<em>&alpha;</em><sub><em>s2</em>,<em>sy2</em>,<em>&sigma;[1]</em></sub>*<em>&alpha;</em><sub><em>&sigma;[1]</em>,<em>sy1</em>,<em>en</em></sub> + ... + <em>p</em>*<em>&alpha;</em><sub><em>s2</em>,<em>sy2</em>,<em>&sigma;[n]</em></sub>*<em>&alpha;</em><sub><em>&sigma;[n]</em>,<em>sy1</em>,<em>en</em></sub>, where <em>&sigma;[i]</em> is one of the machine&#39;s <em>n</em> states.</li>
</ul>

<p>(Here, <em>p</em> is the probability of using the given transition rule; the special value HEADS becomes <em>&lambda;</em>, and the special value TAILS becomes 1&minus;<em>&lambda;</em>.)  Now, each time multiple equations have the same left-hand side, combine them into one equation with the same left-hand side, but with the sum of their right-hand sides.  Then, for every variable of the form <em>&alpha;</em><sub><em>a</em>,<em>b</em>,<em>c</em></sub> not yet present in the system, include the equation <em>&alpha;</em><sub><em>a</em>,<em>b</em>,<em>c</em></sub> = 0.  Then, for each final state <em>fs</em> that returns 1, solve the system for the variable <em>&alpha;</em><sub>START,EMPTY,<em>fs</em></sub> (where START is the automaton&#39;s starting state) to get a solution (a function) that maps (0, 1) to itself. (Each solve can produce multiple solutions, but only one of them will map (0, 1) to itself assuming every <em>p</em> is either HEADS or TAILS.) Finally, add all the solutions to get <em>f</em>(<em>&lambda;</em>).</p></li>
<li><p>Assume there is a pushdown automaton (<em>F</em>) that follows Definition 1 except it uses a set of <em>N</em> input letters (and not simply HEADS or TAILS), accepts an input word if the stack is empty, and rejects the word if the machine reaches a configuration without a transition rule.  Then a pushdown automaton in the full sense of Definition 1 (<em>G</em>) can be built.  In essence:</p>

<ol>
<li>Add a new FAILURE state, which when reached, pops all symbols from the stack.</li>
<li>For each pair (<em>state</em>, <em>stacksymbol</em>) for <em>F</em>, add a set of rules that generate one of the input letters (each letter <em>i</em> generated with probability <em>f</em><sub> <em>i</em></sub>(<em>&lambda;</em>), which must be a function in <strong>PDA</strong>), then use the generated letter to perform the transition stated in the corresponding rule for <em>F</em>.  If there is no such transition, transition to the FAILURE state instead.</li>
<li>When the stack is empty, output 0 if <em>G</em> is in the FAILURE state, or 1 otherwise.</li>
</ol>

<p>Then <em>G</em> returns 1 with the same probability as <em>F</em> accepts an input word with letters randomly generated as in the second step.  Also, one of the <em>N</em> letters can be a so-called &quot;end-of-string&quot; symbol, so that a pushdown automaton can be built that accepts &quot;empty strings&quot;; an example is Elder et al. (2015)[^52].</p></li>
</ol>
</blockquote>

<p><strong>Proposition 6:</strong> <em>If a full-domain pushdown automaton can generate a distribution of words with the same letter, there is a full-domain pushdown automaton that can generate a distribution of such words conditioned on&mdash;</em></p>

<ol>
<li><em>a finite set of word lengths, or</em></li>
<li><em>a periodic infinite set of word lengths.</em></li>
</ol>

<p>One example of a finite set of word lengths is {1, 3, 5, 6}, where only words of length 1, 3, 5, or 6 are allowed.  A <em>periodic infinite set</em> is defined by a finite set of integers such as {1}, as well as an integer modulus such as 2, so that in this example, all integers congruent to 1 modulo 2 (that is, all odd integers) are allowed word lengths and belong to the set.</p>

<p><em>Proof Sketch:</em></p>

<ol>
<li><p>As in Lemma 1A, assume that the automaton stops when it pops EMPTY from the stack.  Let <em>S</em> be the finite set (for example, {1, 3, 5, 6}), and let <em>M</em> be the maximum value in the finite set.  For each integer <em>i</em> in [0, <em>M</em>], make a copy of the automaton and append the integer <em>i</em> to the name of each of its states.  Combine the copies into a new automaton <em>F</em>, and let its start state be the start state for copy 0.  Now, whenever <em>F</em> generates a letter, instead of transitioning to the next state after the letter-generating operation (see Proposition 4), transition to the corresponding state for the next copy (for example, if the operation would transition to copy 2&#39;s version of &quot;XYZ&quot;, namely &quot;2_XYZ&quot;, transition to &quot;3_XYZ&quot; instead), or if the last copy is reached, transition to the last copy&#39;s FAILURE state.  If <em>F</em> would transition to a failure state corresponding to a copy not in <em>S</em> (for example, &quot;0_FAILURE&quot;, &quot;2_FAILURE&quot;, &quot;3_FAILURE&quot; in this example), first all symbols other than EMPTY are popped from the stack and then <em>F</em> transitions to its start state (this is a so-called &quot;rejection&quot; operation).  Now, all the final states (except FAILURE states) for the copies corresponding to the values in <em>S</em> (for example, copies 1, 3, 5, 6 in the example) are treated as returning 1, and all other states are treated as returning 0.</p></li>
<li><p>Follow (1), except as follows: (A) <em>M</em> is equal to the integer modulus minus 1.  (B) For the last copy of the automaton, instead of transitioning to the next state after the letter-generating operation (see Proposition 4), transition to the corresponding state for copy 0 of the automaton.  &#x25a1;</p></li>
</ol>

<p><strong>Proposition 7:</strong> <em>Every constant function equal to a quadratic irrational number between 0 and 1 is in class <strong>PDA</strong>.</em></p>

<p>A <em>continued fraction</em> is one way to write a real number.  For purposes of the following proof, every real number greater than 0 and less than 1 has the following <em>continued fraction expansion</em>: 0 + 1 / (<em>a</em>[1] + 1 / (<em>a</em>[2] + 1 / (<em>a</em>[3] + ... ))), where each <em>a</em>[<em>i</em>], a <em>partial denominator</em>, is an integer greater than 0.  A <em>quadratic irrational number</em> is an irrational number that can be written as (<em>b</em>+sqrt(<em>c</em>))/<em>d</em>, where <em>b</em>, <em>c</em>, and <em>d</em> are rational numbers.</p>

<p><em>Proof:</em>  By Lagrange&#39;s continued fraction theorem, every quadratic irrational number has a continued fraction expansion that is eventually periodic; the expansion can be described using a finite number of partial denominators, the last &quot;few&quot; of which repeat forever.  The following example describes a periodic continued fraction expansion: [0; 1, 2, (5, 4, 3)], which is the same as [0; 1, 2, 5, 4, 3, 5, 4, 3, 5, 4, 3, ...].  In this example, the partial denominators are the numbers after the semicolon; the size of the period (<code>(5, 4, 3)</code>) is 3; and the size of the non-period (<code>1, 2</code>) is 2.</p>

<p>Given a periodic expansion, and with the aid of an algorithm for simulating <a href="https://peteroupc.github.io/bernoulli.html#Continued_Fractions"><strong>continued fractions</strong></a>, a recursive Markov chain for the expansion (Etessami and Yannakakis 2009)[^49] can be described as follows.  The chain&#39;s components are all built on the following template.  The template component has one entry E, one inner node N, one box, and two exits X0 and X1.  The box has one <em>call port</em> as well as two <em>return ports</em> B0 and B1.</p>

<ul>
<li>From E: Go to N with probability <em>x</em>, or to the box&#39;s call port with probability 1 &minus; <em>x</em>.</li>
<li>From N: Go to X1 with probability <em>y</em>, or to X0 with probability 1 &minus; <em>y</em>.</li>
<li>From B0: Go to E with probability 1.</li>
<li>From B1: Go to X0 with probability 1.</li>
</ul>

<p>Let <em>p</em> be the period size, and let <em>n</em> be the non-period size.  Now the recursive Markov chain to be built has <em>n</em>+<em>p</em> components:</p>

<ul>
<li>For each <em>i</em> in [1, <em>n</em>+1], there is a component labeled <em>i</em>.  It is the same as the template component, except <em>x</em> = <em>a</em>[<em>i</em>]/(1 + <em>a</em>[<em>i</em>]), and <em>y</em> = 1/<em>a</em>[<em>i</em>].  The component&#39;s single box goes to the component labeled <em>i</em>+1, <em>except</em> that for component <em>n</em>+<em>p</em>, the component&#39;s single box goes to the component labeled <em>n</em>+1.</li>
</ul>

<p>According to Etessami and Yannakakis (2009)[^49], the recursive Markov chain can be translated to a pushdown automaton of the kind used in this section. Now all that&#39;s left is to argue that the recursive Markov chain terminates with probability 1.  For every component in the chain, it goes from its entry to its box with probability 1/2 or less (because each partial numerator must be 1 or greater).  Thus, the component recurses with no greater probability than not, and there are otherwise no probability-1 loops in each component, so the overall chain terminates with probability 1. &#x25a1;</p>

<p><strong>Lemma 1:</strong> <em>The square root function sqrt(&lambda;) is in class <strong>PDA</strong>.</em></p>

<p><em>Proof:</em> See Mossel and Peres (2005)[^23]. &#x25a1;</p>

<p><strong>Corollary 1:</strong> <em>The function f(&lambda;) = &lambda;<sup>m/(2<sup>n</sup>)</sup>, where n &ge; 1 is an integer and where m &ge; 1 is an integer, is in class <strong>PDA</strong>.</em></p>

<p><em>Proof:</em> Start with the case <em>m</em>=1.  If <em>n</em> is 1, write <em>f</em> as sqrt(<em>&lambda;</em>); if <em>n</em> is 2, write <em>f</em> as sqrt&#x2218;sqrt(<em>&lambda;</em>); and for general <em>n</em>, write <em>f</em> as sqrt&#x2218;sqrt&#x2218;...&#x2218;sqrt(<em>&lambda;</em>), with <em>n</em> instances of sqrt.  Because this is a composition and sqrt can be simulated by a full-domain pushdown automaton, so can <em>f</em>.</p>

<p>For general <em>m</em> and <em>n</em>, write <em>f</em> as (sqrt&#x2218;sqrt&#x2218;...&#x2218;sqrt(<em>&lambda;</em>))<sup><em>m</em></sup>, with <em>n</em> instances of sqrt.  This involves doing <em>m</em> multiplications of sqrt&#x2218;sqrt&#x2218;...&#x2218;sqrt, and because this is an integer power of a function that can be simulated by a full-domain pushdown automaton, so can <em>f</em>.</p>

<p>Moreover, <em>f</em> is in class <strong>PDA</strong> by Theorem 1.2 of (Mossel and Peres 2005)[^23] because the machine is a full-domain pushdown automaton. &#x25a1;</p>

<p><a id=Finite_State_and_Pushdown_Generators></a></p>

<h4>Finite-State and Pushdown Generators</h4>

<p>Another interesting class of machines (called <em>pushdown generators</em> here) are similar to pushdown automata (see above), with the following exceptions:</p>

<ol>
<li>Each transition rule can also, optionally, output a base-<em>N</em> digit in its right-hand side.  An example is: (<em>state</em>, <em>flip</em>, <em>sy</em>) &rarr; (<em>digit</em>, <em>state2</em>, {<em>sy2</em>}).</li>
<li>The machine must output infinitely many digits if allowed to run forever.</li>
<li>Rules that would pop the last symbol from the stack are not allowed.</li>
</ol>

<p>The &quot;output&quot; of the machine is now a real number <em>X</em> in the form of the base-<em>N</em> digit expansion <code>0.dddddd...</code>, where <code>dddddd...</code> are the digits produced by the machine from left to right.  In the rest of this section:</p>

<ul>
<li><code>CDF(z)</code> is the cumulative distribution function of <em>X</em>, or the probability that <em>X</em> is <em>z</em> or less.</li>
<li><code>PDF(z)</code> is the probability density function of <em>X</em>, or the derivative of <code>CDF(z)</code>, or the relative probability of choosing a number &quot;close&quot; to <em>z</em> at random.</li>
</ul>

<p>A <em>finite-state generator</em> (Knuth and Yao 1976)[^35] is the special case where the probability of heads is 1/2, each digit is either 0 or 1, rules can&#39;t push stack symbols, and only one stack symbol is used.  Then if <code>PDF(z)</code> has infinitely many &quot;slope&quot; functions on the open interval (0, 1), it must be a polynomial with rational coefficients and not equal 0 at any irrational point on (0, 1) (Vatan 2001)[^53], (Kindler and Romik 2004)[^54], and it can be shown that the expected value (mean or &quot;long-run average&quot;) of <em>X</em> must be a rational number. [^55]</p>

<p><strong>Proposition 8.</strong> <em>Suppose a finite-state generator can generate a probability distribution that takes on finitely many values.  Then:</em></p>

<ol>
<li><em>Each value occurs with a rational probability.</em></li>
<li><em>Each value is either rational or transcendental.</em></li>
</ol>

<p>A real number is <em>transcendental</em> if it can&#39;t be a root of a nonzero polynomial with integer coefficients.  Thus, part 2 means, for example, that irrational, non-transcendental numbers such as 1/sqrt(2) and the golden ratio minus 1 can&#39;t be generated exactly.</p>

<p>Proving this proposition involves the following lemma, which shows that a finite-state generator is related to a machine with a one-way read-only input and a one-way write-only output:</p>

<p><strong>Lemma 2.</strong> <em>A finite-state generator can fit the model of a one-way transducer-like k-machine (as defined in Adamczewski et al. (2020)[^56] section 5.3), for some k equal to 2 or greater.</em></p>

<p><em>Proof Sketch:</em> There are two cases.</p>

<p>Case 1: If every transition rule of the generator outputs a digit, then <em>k</em> is the number of unique inputs among the generator&#39;s transition rules (usually, there are two unique inputs, namely HEADS and TAILS), and the model of a finite-state generator is modified as follows:</p>

<ol>
<li>A <em>configuration</em> of the finite-state generator consists of its current state together with either the last coin flip result or, if the coin wasn&#39;t flipped yet, the empty string.</li>
<li>The <em>output function</em> takes a configuration described above and returns a digit.  If the coin wasn&#39;t flipped yet, the function returns an arbitrary digit (which is not used in proposition 4.6 of the Adamczewski paper).</li>
</ol>

<p>Case 2: If at least one transition rule does not output a digit, then the finite-state generator can be transformed to a machine where HEADS/TAILS is replaced with 50% probabilities, then transformed to an equivalent machine whose rules always output one or more digits, as claimed in Lemma 5.2 of Vatan (2001)[^53].  In case the resulting generator has rules that output more than one digit, additional states and rules can be added so that the generator&#39;s rules output only one digit as desired.  Now at this point the generator&#39;s probabilities will be rational numbers. Now transform the generator from probabilities to inputs of size <em>k</em>, where <em>k</em> is the product of those probabilities, by adding additional rules as desired.  &#x25a1;</p>

<p><em>Proof of Proposition 8:</em> Let <em>n</em> be an integer greater than 0. Take a finite-state generator that starts at state START and branches to one of <em>n</em> finite-state generators (sub-generators) with some probability, which must be rational because the overall generator is a finite-state machine (Icard 2020, Proposition 13)[^47].  The branching process outputs no digit, and part 3 of the proposition follows from Corollary 9 of Icard (2020)[^47].  The <em>n</em> sub-generators are special; each of them generates the binary expansion of a single real number in the closed unit interval with probability 1.</p>

<p>To prove part 2 of the proposition, translate an arbitrary finite-state generator to a machine described in Lemma 2.  Once that is done, all that must be shown is that there are two different non-empty sequences of coin flips that end up at the same configuration. This is easy using the pigeonhole principle, since the finite-state generator has a finite number of configurations. Thus, by propositions 5.11, 4.6, and AB of Adamczewski et al. (2020)[^56], the generator can generate a real number&#39;s binary expansion only if that number is rational or transcendental (see also Cobham (1968)[^57]; Adamczewski and Bugeaud (2007)[^58]).  &#x25a1;</p>

<p><strong>Proposition 9.</strong> <em>If the distribution function generated by a finite-state generator is continuous and algebraic on the open interval (0, 1), then that function is a piecewise polynomial function on that interval.</em></p>

<p>The proof follows from combining Kindler and Romik (2004, Theorem 2)[^54] and Knuth and Yao (1976)[^35] with Richman (2012)[^59], who proved that a continuous algebraic function on an open interval is piecewise analytic (&quot;analytic&quot; means writable as $c_0 x^0+ ... +c_i x^i + ...$ where $c_i$ are real numbers).</p>

<p><a id=License></a></p>

<h2>License</h2>

<p>Any copyright to this page is released to the Public Domain.  In case this is not possible, this page is also licensed under <a href="https://creativecommons.org/publicdomain/zero/1.0/"><strong>Creative Commons Zero</strong></a>.</p>
</div><nav id="navigation"><ul>
<li><a href="/">Back to start site.</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io">This site's repository (source code)</a>
<li><a href="https://github.com/peteroupc/peteroupc.github.io/issues">Post an issue or comment</a></ul>

<div class="noprint">
<p>
<a href="//twitter.com/intent/tweet">Share via Twitter</a>, <a href="//www.facebook.com/sharer/sharer.php" id="sharer">Share via Facebook</a>
</p>
</div>
</nav><script>
 document.getElementById("sharer").href="//www.facebook.com/sharer/sharer.php?u="+
    encodeURIComponent(document.location.href)
</script>
</body></html>
